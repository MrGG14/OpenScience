{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from grobid_client.grobid_client import GrobidClient\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "import os\n",
    "from stop_words import get_stop_words\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN\n",
    "import numpy as np\n",
    "# Now let's do topic modeling using LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "# si no hace bien los imports de utilsdescomenta esta linea\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils import remove_files, get_abstract, cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de documentos: 19\n"
     ]
    }
   ],
   "source": [
    "# Directorio donde se encuentran los archivos XML\n",
    "xml_dir = os.path.join(parent_dir, \"output\")\n",
    "# papers_dir = os.path.join(parent_dir, \"papers\")\n",
    "\n",
    "# remove_files(xml_dir)\n",
    "\n",
    "# client = GrobidClient(config_path=\"./config.json\")\n",
    "# client.process(\"processFulltextDocument\", papers_dir, output=xml_dir, consolidate_citations=True, tei_coordinates=True, n=20)\n",
    "\n",
    "# Lista para almacenar los resúmenes\n",
    "papers = {}\n",
    "\n",
    "# Procesar cada archivo XML en el directorio\n",
    "for file in os.listdir(xml_dir):\n",
    "    if file.endswith(\".xml\"):  # Verificar que el archivo sea XML\n",
    "        file_path = os.path.join(xml_dir, file)\n",
    "        file_name = os.path.basename(file_path)[:-15]\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        #verificamos si el paper esta en nuestro diccionario y si no lo esta creamos una entrada para el\n",
    "        if file_name not in papers:\n",
    "            papers[file_name] = {}\n",
    "\n",
    "        #get abstract\n",
    "        abstract = get_abstract(root)\n",
    "        papers[file_name]['abstract'] = abstract\n",
    "\n",
    "        #get authors\n",
    "        ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "        autores = root.findall('.//tei:author', ns)\n",
    "\n",
    "        # Obtener los nombres completos de los autores\n",
    "        nombres_completos = []\n",
    "        for autor in autores:\n",
    "            # Encontrar el elemento forename dentro de author\n",
    "            forename_elem = autor.find('.//tei:forename', ns)\n",
    "            # Encontrar el elemento surname dentro de author\n",
    "            surname_elem = autor.find('.//tei:surname', ns)\n",
    "            \n",
    "            # Verificar si se encontraron los elementos forename y surname\n",
    "            try:\n",
    "                # Obtener el texto de los elementos forename y surname\n",
    "                nombre = forename_elem.text\n",
    "                apellido = surname_elem.text\n",
    "                nombre_completo = f\"{nombre} {apellido}\"\n",
    "            except:\n",
    "                # Si no se encontraron los elementos, asignar NA a nombre y apellido\n",
    "                # print('Missing name and/or surname. Cant get full name.')\n",
    "                pass\n",
    "            nombres_completos.append(nombre_completo)\n",
    "        papers[file_name]['authors'] = nombres_completos\n",
    "\n",
    "\n",
    "print(f'Numero de documentos: {len(papers)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'11621ijccsa02': {'abstract': 'With recent advances in technology, internet has drastically changed the computing world from the concept of parallel computing to distributed computing to grid computing and now to cloud computing. The evolution of cloud computing over the past few years is potentially one of the major advances in the history of computing. Unfortunately, many banks are still hesitant to adopt cloud technology. New credit unions wanting to achieve greater business agility, cloud technology enables organizations to respond instantly to changing market conditions, leveraging data and applied analytics to achieve customer experience and operational productivity benefits. As a result, cloud computing comes in to provide a solution to such challenges making banking a reliable and trustworthy service. This paper aims at cloud computing strategy, impact in banking and financial institutions and discusses the significant reliance of cloud computing.',\n",
       "  'authors': ['Prudhvi Parne',\n",
       "   'S Tiwari',\n",
       "   'S Bharadwaj',\n",
       "   'S Joshi',\n",
       "   'F Kaya',\n",
       "   'F Kaya',\n",
       "   'M Berg',\n",
       "   'R Wieringa',\n",
       "   'M Makkes',\n",
       "   'P Rieger',\n",
       "   'H Gewald',\n",
       "   'B Schumacher',\n",
       "   'R Parry',\n",
       "   'R Bisson',\n",
       "   'Nir Kshetri',\n",
       "   'Marcel Decker',\n",
       "   'Bernd Grobauer',\n",
       "   'Tobias Walloschek',\n",
       "   'Elmar Stocker',\n",
       "   'A Ali',\n",
       "   'Alice Villar',\n",
       "   'Nawaz Khan',\n",
       "   'A Ali',\n",
       "   'Alice Villar',\n",
       "   'Nawaz Khan',\n",
       "   'Strahil Sokolov',\n",
       "   'Stefan Vlaev',\n",
       "   'Mihail Vukadinoff',\n",
       "   'Asen Zahariev',\n",
       "   'Teodor Iliev',\n",
       "   'Ivaylo Stoyanov',\n",
       "   'Ivaylo Stoyanov',\n",
       "   'Lizhe Wang',\n",
       "   'Gregor Von Laszewski',\n",
       "   'Andrew Younge',\n",
       "   'Xi He',\n",
       "   'Marcel Kunze',\n",
       "   'Jie Tao',\n",
       "   'Cheng Fu',\n",
       "   'L Antonopoulos',\n",
       "   'L Antonopoulos',\n",
       "   'Michael Armbrust',\n",
       "   'Armando Fox',\n",
       "   'Rean Griffith',\n",
       "   'Anthony Joseph',\n",
       "   'Randy Katz',\n",
       "   'Andy Konwinski',\n",
       "   'Gunho Lee',\n",
       "   'David Patterson',\n",
       "   'Ariel Rabkin',\n",
       "   'Ion Stoica',\n",
       "   'Matei Zaharia',\n",
       "   'Yashpalsinh Jadeja',\n",
       "   'Kirit Modi',\n",
       "   'Dan Marinescu',\n",
       "   'Dan Marinescu',\n",
       "   'O Musa',\n",
       "   'O Musa',\n",
       "   'O Musa',\n",
       "   'Medara Rambabu',\n",
       "   'Swati Gupta',\n",
       "   'Ravi Singh']},\n",
       " '1709.01907': {'abstract': 'Reliable uncertainty estimation for time series prediction is critical in many fields, including physics, biology, and manufacturing. At Uber, probabilistic time series forecasting is used for robust prediction of number of trips during special events, driver incentive allocation, as well as real-time anomaly detection across millions of metrics. Classical time series models are often used in conjunction with a probabilistic formulation for uncertainty estimation. However, such models are hard to tune, scale, and add exogenous variables to. Motivated by the recent resurgence of Long Short Term Memory networks, we propose a novel end-to-end Bayesian deep model that provides time series prediction along with uncertainty estimation. We provide detailed experiments of the proposed solution on completed trips data, and successfully apply it to large-scale time series anomaly detection at Uber.',\n",
       "  'authors': ['Lingxue Zhu',\n",
       "   'Nikolay Laptev',\n",
       "   'Nikolay Laptev',\n",
       "   'Nikolay Laptev',\n",
       "   'John Horne',\n",
       "   'Wolfram Manzenreiter',\n",
       "   'R Hyndman',\n",
       "   'Y Khandakar',\n",
       "   'Sepp Hochreiter',\n",
       "   'Jürgen Schmidhuber',\n",
       "   'Mohammad Assaad',\n",
       "   'Romuald Boné',\n",
       "   'Hubert Cardot',\n",
       "   'O Ogunmolu',\n",
       "   'X Gu',\n",
       "   'S Jiang',\n",
       "   'N Gans',\n",
       "   'N Laptev',\n",
       "   'J Yosinski',\n",
       "   'E Li',\n",
       "   'S Smyl',\n",
       "   'A Kendall',\n",
       "   'Y Gal',\n",
       "   'I Goodfellow',\n",
       "   'J Shlens',\n",
       "   'C Szegedy',\n",
       "   'C Szegedy',\n",
       "   '-S Wei',\n",
       "   'T Opitz',\n",
       "   'R Dietz',\n",
       "   'T Casavant',\n",
       "   'T Scheetz',\n",
       "   'T Braun',\n",
       "   'M Andersland',\n",
       "   'Y Gal',\n",
       "   'J Hron',\n",
       "   'A Kendall',\n",
       "   'Y Gal',\n",
       "   'Z Ghahramani',\n",
       "   'J Paisley',\n",
       "   'D Blei',\n",
       "   'M Jordan',\n",
       "   'D Kingma',\n",
       "   'M Welling',\n",
       "   'J Hernández-Lobato',\n",
       "   'R Adams',\n",
       "   'C Blundell',\n",
       "   'J Cornebise',\n",
       "   'K Kavukcuoglu',\n",
       "   'D Wierstra',\n",
       "   'M Fortunato',\n",
       "   'C Blundell',\n",
       "   'O Vinyals',\n",
       "   'J Hernández-Lobato',\n",
       "   'Y Li',\n",
       "   'M Rowland',\n",
       "   'D Hernández-Lobato',\n",
       "   'T Bui',\n",
       "   'R Turner',\n",
       "   'Y Li',\n",
       "   'Y Gal',\n",
       "   'Y Gal',\n",
       "   'Y Gal',\n",
       "   'Z Ghahramani',\n",
       "   'N Srivastava',\n",
       "   'E Mansimov',\n",
       "   'R Salakhudinov']},\n",
       " '1802.05799': {'abstract': \"Training modern deep learning models requires large amounts of computation, often provided by GPUs. Scaling computation from one GPU to many can enable much faster training and research progress but entails two complications. First, the training library must support inter-GPU communication. Depending on the particular methods employed, this communication may entail anywhere from negligible to significant overhead. Second, the user must modify his or her training code to take advantage of inter-GPU communication. Depending on the training library's API, the modification required may be either significant or minimal.\",\n",
       "  'authors': ['Alexander Sergeev',\n",
       "   'Mike Balso',\n",
       "   'Martín Abadi',\n",
       "   'Ashish Agarwal',\n",
       "   'Paul Barham',\n",
       "   'Eugene Brevdo',\n",
       "   'Zhifeng Chen',\n",
       "   'Craig Citro',\n",
       "   'Greg Corrado',\n",
       "   'Andy Davis',\n",
       "   'Jeffrey Dean',\n",
       "   'Matthieu Devin',\n",
       "   'Sanjay Ghemawat',\n",
       "   'Ian Goodfellow',\n",
       "   'Andrew Harp',\n",
       "   'Geoffrey Irving',\n",
       "   'Michael Isard',\n",
       "   'Yangqing Jia',\n",
       "   'Rafal Jozefowicz',\n",
       "   'Lukasz Kaiser',\n",
       "   'Manjunath Kudlur',\n",
       "   'Josh Levenberg',\n",
       "   'Dan Mane',\n",
       "   'Rajat Monga',\n",
       "   'Sherry Moore',\n",
       "   'Derek Murray',\n",
       "   'Chris Olah',\n",
       "   'Mike Schuster',\n",
       "   'Jonathon Shlens',\n",
       "   'Benoit Steiner',\n",
       "   'Ilya Sutskever',\n",
       "   'Kunal Talwar',\n",
       "   'Paul Tucker',\n",
       "   'Vincent Vanhoucke',\n",
       "   'Vijay Vasudevan',\n",
       "   'Fernanda Viegas',\n",
       "   'Oriol Vinyals',\n",
       "   'Pete Warden',\n",
       "   'Martin Wattenberg',\n",
       "   'Martin Wicke',\n",
       "   'Yuan Yu',\n",
       "   'Xiaoqiang Zheng',\n",
       "   'François Chollet',\n",
       "   'Jeremy Hermann',\n",
       "   'Mike Del',\n",
       "   'Mike Del',\n",
       "   'Martín Abadi',\n",
       "   'Paul Barham',\n",
       "   'Jianmin Chen',\n",
       "   'Zhifeng Chen',\n",
       "   'Andy Davis',\n",
       "   'Jeffrey Dean',\n",
       "   'Matthieu Devin',\n",
       "   'Sanjay Ghemawat',\n",
       "   'Geoffrey Irving',\n",
       "   'Michael Isard',\n",
       "   'Manjunath Kudlur',\n",
       "   'Josh Levenberg',\n",
       "   'Rajat Monga',\n",
       "   'Sherry Moore',\n",
       "   'Derek Murray',\n",
       "   'Benoit Steiner',\n",
       "   'Paul Tucker',\n",
       "   'Vijay Vasudevan',\n",
       "   'Pete Warden',\n",
       "   'Martin Wicke',\n",
       "   'Yuan Yu',\n",
       "   'Xiaoqiang Zheng',\n",
       "   'Xiaoqiang Zheng',\n",
       "   'Priya Goyal',\n",
       "   'Piotr Dollár',\n",
       "   'Ross Girshick',\n",
       "   'Pieter Noordhuis',\n",
       "   'Lukasz Wesolowski',\n",
       "   'Aapo Kyrola',\n",
       "   'Andrew Tulloch',\n",
       "   'Ryan Mcgrady',\n",
       "   'Andrew Gibiansky',\n",
       "   'Pitch Patarasuk',\n",
       "   'Xin Yuan',\n",
       "   'Xin Yuan',\n",
       "   'Edgar Gabriel',\n",
       "   'Graham Fagg',\n",
       "   'George Bosilca',\n",
       "   'Thara Angskun',\n",
       "   'Jack Dongarra',\n",
       "   'Jeffrey Squyres',\n",
       "   'Vishal Sahay',\n",
       "   'Prabhanjan Kambadur',\n",
       "   'Brian Barrett',\n",
       "   'Andrew Lumsdaine',\n",
       "   'Ralph Castain',\n",
       "   'David Daniel',\n",
       "   'Richard Graham',\n",
       "   'Timothy Woodall',\n",
       "   'Andrew Gibiansky',\n",
       "   'Joel Hestness',\n",
       "   'Joel Hestness',\n",
       "   'Alexander Sergeev',\n",
       "   'Chromium Authors',\n",
       "   'Alexander Sergeev',\n",
       "   'Alexander Sergeev',\n",
       "   'Alexander Sergeev',\n",
       "   'Chi-Yen Chen',\n",
       "   'Wei-Yun Ma',\n",
       "   'Wei-Yun Ma']},\n",
       " '2007.03051': {'abstract': 'Deep learning (DL) can achieve impressive results across a wide variety of tasks, but this often comes at the cost of training models for extensive periods on specialized hardware accelerators. This energy-intensive workload has seen immense growth in recent years. Machine learning (ML) may become a significant contributor to climate change if this exponential trend continues. If practitioners are aware of their energy and carbon footprint, then they may actively take steps to reduce it whenever possible. In this work, we present carbontracker, a tool for tracking and predicting the energy and carbon footprint of training DL models. We propose that energy and carbon footprint of model development and training is reported alongside performance metrics using tools like carbontracker. We hope this will promote responsible computing in ML and encourage research into energy-efficient deep neural networks. 1  ',\n",
       "  'authors': ['Lasse Wolff Anthony',\n",
       "   'Benjamin Kanding',\n",
       "   'Raghavendra Selvan',\n",
       "   'Lasse Wolff',\n",
       "   'D Amodei',\n",
       "   'D Hernandez',\n",
       "   'D Hernandez',\n",
       "   'D Hernandez',\n",
       "   'Iii Armato',\n",
       "   'S Mclennan',\n",
       "   'G Mcnitt-Gray',\n",
       "   'M Meyer',\n",
       "   'C Yankelevitz',\n",
       "   'D Aberle',\n",
       "   'D Henschke',\n",
       "   'C Hoffman',\n",
       "   'E Kazerooni',\n",
       "   'E Macmahon',\n",
       "   'E Macmahon',\n",
       "   'R Ascierto',\n",
       "   'R Ascierto',\n",
       "   'V Avelar',\n",
       "   'D Azevedo',\n",
       "   'A French',\n",
       "   'A Badia',\n",
       "   'B Piot',\n",
       "   'S Kapturowski',\n",
       "   'P Sprechmann',\n",
       "   'A Vitvitskyi',\n",
       "   'D Guo',\n",
       "   'C Blundell',\n",
       "   'J Bergstra',\n",
       "   'Y Bengio',\n",
       "   'T Brown',\n",
       "   'B Mann',\n",
       "   'N Ryder',\n",
       "   'M Subbiah',\n",
       "   'J Kaplan',\n",
       "   'P Dhariwal',\n",
       "   'A Neelakantan',\n",
       "   'P Shyam',\n",
       "   'G Sastry',\n",
       "   'A Askell',\n",
       "   'S Agarwal',\n",
       "   'A Herbert-Voss',\n",
       "   'G Krueger',\n",
       "   'T Henighan',\n",
       "   'R Child',\n",
       "   'A Ramesh',\n",
       "   'D Ziegler',\n",
       "   'J Wu',\n",
       "   'C Winter',\n",
       "   'C Hesse',\n",
       "   'M Chen',\n",
       "   'E Sigler',\n",
       "   'M Litwin',\n",
       "   'S Gray',\n",
       "   'B Chess',\n",
       "   'J Clark',\n",
       "   'C Berner',\n",
       "   'S Mccandlish',\n",
       "   'A Radford',\n",
       "   'I Sutskever',\n",
       "   'D Amodei',\n",
       "   'T Bruckner',\n",
       "   'Y Bashmakov',\n",
       "   'H Mulugetta',\n",
       "   'A Chum',\n",
       "   'Howard David',\n",
       "   'Eugene Gorbatov',\n",
       "   'Ulf Hanebutte',\n",
       "   'Rahul Khanna',\n",
       "   'Christian Le',\n",
       "   'Adam Holloway',\n",
       "   'Adam Holloway',\n",
       "   'M Gorkovenko',\n",
       "   'A Dholakia',\n",
       "   'Peter Henderson',\n",
       "   'Jieru Hu',\n",
       "   'Mona Diab',\n",
       "   'Joelle Pineau',\n",
       "   'Joelle Pineau',\n",
       "   'S Jaeger',\n",
       "   'S Candemir',\n",
       "   'S Antani',\n",
       "   'Y.-X Wáng',\n",
       "   'P.-X Lu',\n",
       "   'G Thoma',\n",
       "   'D Kingma',\n",
       "   'J Ba',\n",
       "   'J Ba',\n",
       "   'A Lacoste',\n",
       "   'A Luccioni',\n",
       "   'V Schmidt',\n",
       "   'T Dandres',\n",
       "   'Da Li',\n",
       "   'Xinbo Chen',\n",
       "   'Michela Becchi',\n",
       "   'Ziliang Zong',\n",
       "   'L Li',\n",
       "   'K Jamieson',\n",
       "   'G Desalvo',\n",
       "   'A Rostamizadeh',\n",
       "   'A Talwalkar',\n",
       "   'A Talwalkar',\n",
       "   'A Paszke',\n",
       "   'S Gross',\n",
       "   'F Massa',\n",
       "   'A Lerer',\n",
       "   'J Bradbury',\n",
       "   'G Chanan',\n",
       "   'T Killeen',\n",
       "   'Z Lin',\n",
       "   'N Gimelshein',\n",
       "   'L Antiga',\n",
       "   'A Desmaison',\n",
       "   'A Köpf',\n",
       "   'E Yang',\n",
       "   'Z Devito',\n",
       "   'M Raison',\n",
       "   'A Tejani',\n",
       "   'S Chilamkurthy',\n",
       "   'B Steiner',\n",
       "   'L Fang',\n",
       "   'J Bai',\n",
       "   'S Chintala',\n",
       "   'S Chintala',\n",
       "   'Olaf Ronneberger',\n",
       "   'Philipp Fischer',\n",
       "   'Thomas Brox',\n",
       "   'R Selvan',\n",
       "   'E Dam',\n",
       "   'N Detlefsen',\n",
       "   'S Rischel',\n",
       "   'K Sheng',\n",
       "   'M Nielsen',\n",
       "   'A Pai',\n",
       "   'J Snoek',\n",
       "   'H Larochelle',\n",
       "   'R Adams',\n",
       "   'J Staal',\n",
       "   'M Abramoff',\n",
       "   'M Niemeijer',\n",
       "   'M Viergever',\n",
       "   'B Van Ginneken',\n",
       "   'Dimitrios Stamoulis',\n",
       "   'Ermao Cai',\n",
       "   'Da-Cheng Juan',\n",
       "   'Diana Marculescu',\n",
       "   'Emma Strubell',\n",
       "   'Ananya Ganesh',\n",
       "   'Andrew Mccallum',\n",
       "   'Zhenheng Tang',\n",
       "   'Yuxin Wang',\n",
       "   'Qiang Wang',\n",
       "   'Xiaowen Chu',\n",
       "   'Oriol Vinyals',\n",
       "   'Igor Babuschkin',\n",
       "   'Wojciech Czarnecki',\n",
       "   'Michaël Mathieu',\n",
       "   'Andrew Dudzik',\n",
       "   'Junyoung Chung',\n",
       "   'David Choi',\n",
       "   'Richard Powell',\n",
       "   'Timo Ewalds',\n",
       "   'Petko Georgiev',\n",
       "   'Junhyuk Oh',\n",
       "   'Dan Horgan',\n",
       "   'Manuel Kroiss',\n",
       "   'Ivo Danihelka',\n",
       "   'Aja Huang',\n",
       "   'Laurent Sifre',\n",
       "   'Trevor Cai',\n",
       "   'John Agapiou',\n",
       "   'Max Jaderberg',\n",
       "   'Alexander Vezhnevets',\n",
       "   'Rémi Leblond',\n",
       "   'Tobias Pohlen',\n",
       "   'Valentin Dalibard',\n",
       "   'David Budden',\n",
       "   'Yury Sulsky',\n",
       "   'James Molloy',\n",
       "   'Tom Paine',\n",
       "   'Caglar Gulcehre',\n",
       "   'Ziyu Wang',\n",
       "   'Tobias Pfaff',\n",
       "   'Yuhuai Wu',\n",
       "   'Roman Ring',\n",
       "   'Dani Yogatama',\n",
       "   'Dario Wünsch',\n",
       "   'Katrina Mckinney',\n",
       "   'Oliver Smith',\n",
       "   'Tom Schaul',\n",
       "   'Timothy Lillicrap',\n",
       "   'Koray Kavukcuoglu',\n",
       "   'Demis Hassabis',\n",
       "   'Chris Apps',\n",
       "   'David Silver',\n",
       "   'M Wilcox',\n",
       "   'S Schuermans',\n",
       "   'C Voskoglou',\n",
       "   'A Sobolevski',\n",
       "   'Tien-Ju Yang',\n",
       "   'Yu-Hsin Chen',\n",
       "   'Vivienne Sze',\n",
       "   'Jumie Yuventi',\n",
       "   'Roshan Mehdizadeh']},\n",
       " '208': {'abstract': 'Artificial Intelligence (AI), sometimes called machine intelligence, is the integration of the human mind into a machine. This is a machine that is smarter than the intelligence of human nature. From Secrets to self-driving cars, the artificial intelligence is rapidly evolving. Artificial intelligence generally has two main ideas. First, it mimics the human brain in how mental processes work, and second, it helps to visualize these processes through machine learning. Fake Financial Advice is for chat machines only. Artificial intelligence has dealt with many sectors, including the banking sector. The main objective of this study was to understand the effects of artificial intelligence in modern banks. This study focuses on the perception of artificial intelligence in the banking sector, how it has transformed banks dramatically, and its impact on human resources. Knowing that human beings have a faulty tendency but the world is changing, and creativity, a lack of knowledge is needed for automation. Much of the work and craftsmanship that has ever been done by humans has now been improved by the use of advanced technology.',\n",
       "  'authors': ['Rashmi Bh',\n",
       "   'Mr Ram',\n",
       "   'Kumar Kuncha',\n",
       "   'Mr Manu',\n",
       "   'Vasudevan Unni',\n",
       "   'Philcy Antony',\n",
       "   'Priya Vinod',\n",
       "   'Niklas Bussmann',\n",
       "   'Paolo Giudici',\n",
       "   'Dimitri Marinelli',\n",
       "   'Jochen Papenbrock',\n",
       "   'C Catalini',\n",
       "   'R Foster',\n",
       "   'R Foster',\n",
       "   'Anna Costello',\n",
       "   'Andrea Down',\n",
       "   'Mihir Mehta',\n",
       "   'Jon Frost',\n",
       "   'Leonardo Gambacorta',\n",
       "   'Yi Huang',\n",
       "   'Hyun Shin',\n",
       "   'Pablo Zbinden',\n",
       "   'Andreas Fuster',\n",
       "   'Paul Goldsmith-Pinkham',\n",
       "   'Tarun Ramadorai',\n",
       "   'Ansgar Walther',\n",
       "   'Guy Messick',\n",
       "   'S Jewandah',\n",
       "   '© Icas',\n",
       "   'Kumba Sennaar',\n",
       "   'Lok Lam',\n",
       "   'Shun-Wen Hsiao',\n",
       "   'E Ludwig',\n",
       "   'Alison Lui',\n",
       "   'George Lamb',\n",
       "   'Manu Unni',\n",
       "   'M Swapna',\n",
       "   'M Meinert',\n",
       "   'Robin Nunn',\n",
       "   'María Óskarsdóttir',\n",
       "   'Cristián Bravo',\n",
       "   'Carlos Sarraute',\n",
       "   'Jan Vanthienen',\n",
       "   'Bart Baesens',\n",
       "   'G Sarvady',\n",
       "   'G Satell',\n",
       "   'T Dhanabalan',\n",
       "   'A Sathish',\n",
       "   'M Unni',\n",
       "   'S Rudresh']},\n",
       " '269 An Insight into Cloud Computing Paradigm and Services': {'abstract': 'Cloud computing is a computing model which provides the computing services over the internet on pay-as-you-go basis. You can take the services on rent instead of building your own ITwhich results in cost saving. In this paper, we have discussed the differences between traditional IT infrastructure and cloud computing. We have also discussed the benefits of cloud, risks associated with cloud computing and the cloud models.',\n",
       "  'authors': ['Heena Girdher',\n",
       "   'Jagjit Kaur',\n",
       "   'Jagjit Kaur',\n",
       "   'E Onur',\n",
       "   'E Sfakianakis',\n",
       "   'C Papagianni',\n",
       "   'G Karagiannis',\n",
       "   'T Kontos',\n",
       "   'I Niemegeers',\n",
       "   'I Chochliouros',\n",
       "   'S De Groot',\n",
       "   'P Sjodin',\n",
       "   'M Hidell',\n",
       "   'T Cinkler',\n",
       "   'M Maliosz',\n",
       "   'D Kaklamani',\n",
       "   'J Carapinha',\n",
       "   'M Belesioti',\n",
       "   'E Fytros',\n",
       "   'Rajkumar Buyya',\n",
       "   'G Gruman',\n",
       "   'Rajkumar Buyya',\n",
       "   'Chee Yeo',\n",
       "   'Srikumar Venugopal',\n",
       "   'Christian Baun',\n",
       "   'Marcel Kunze',\n",
       "   'Jens Nimis',\n",
       "   'Stefan Tai',\n",
       "   'Sumalatha Adabala',\n",
       "   'Vineet Chadha',\n",
       "   'Puneet Chawla',\n",
       "   'Renato Figueiredo',\n",
       "   'José Fortes',\n",
       "   'Ivan Krsul',\n",
       "   'Andrea Matsunaga',\n",
       "   'Mauricio Tsugawa',\n",
       "   'Jian Zhang',\n",
       "   'Ming Zhao',\n",
       "   'Liping Zhu',\n",
       "   'Xiaomin Zhu',\n",
       "   'Paul Barham',\n",
       "   'Boris Dragovic',\n",
       "   'Keir Fraser',\n",
       "   'Steven Hand',\n",
       "   'Tim Harris',\n",
       "   'Alex Ho',\n",
       "   'Rolf Neugebauer',\n",
       "   'Ian Pratt',\n",
       "   'Andrew Warfield']},\n",
       " '6114nsa03': {'abstract': \"Cloud computing has formed the conceptual and infrastructural basis for tomorrow's computing. The global computing infrastructure is rapidly moving towards cloud based architecture. While it is important to take advantages of could based computing by means of deploying it in diversified sectors, the security aspects in a cloud based computing environment remains at the core of interest. Cloud based services and service providers are being evolved which has resulted in a new business trend based on cloud technology. With the introduction of numerous cloud based services and geographically dispersed cloud service providers, sensitive information of different entities are normally stored in remote servers and locations with the possibilities of being exposed to unwanted parties in situations where the cloud servers storing those information are compromised. If security is not robust and consistent, the flexibility and advantages that cloud computing has to offer will have little credibility. This paper presents a review on the cloud computing concepts as well as security issues inherent within the context of cloud computing and cloud infrastructure.\",\n",
       "  'authors': ['Monjur Ahmed',\n",
       "   'Mohammad Ashraf Hossain',\n",
       "   'Imad Abbadi',\n",
       "   'Andrew Martin',\n",
       "   'A Agarwal',\n",
       "   'A Agarwal',\n",
       "   'Junaid Arshad',\n",
       "   'Paul Townend',\n",
       "   'Jie Xu',\n",
       "   'A Atayero',\n",
       "   'O Feyisetan',\n",
       "   'Anthony Bisong',\n",
       "   'Syed M. Rahman',\n",
       "   'Rajkumar Buyya',\n",
       "   'Chee Yeo',\n",
       "   'Srikumar Venugopal',\n",
       "   'James Broberg',\n",
       "   'Ivona Brandic',\n",
       "   'Valentina Casola',\n",
       "   'Antonio Cuomo',\n",
       "   'Massimiliano Rak',\n",
       "   'Umberto Villano',\n",
       "   'Antonio Celesti',\n",
       "   'Maria Fazio',\n",
       "   'Massimo Villari',\n",
       "   'Antonio Puliafito',\n",
       "   'Jianhua Che',\n",
       "   'Yamin Duan',\n",
       "   'Tao Zhang',\n",
       "   'Jie Fan',\n",
       "   'Deyan Chen',\n",
       "   'Hong Zhao',\n",
       "   'Wanchun Dou',\n",
       "   'Qi Chen',\n",
       "   'Jinjun Chen',\n",
       "   'Robert Dukaric',\n",
       "   'Matjaz Juric',\n",
       "   'A Emam',\n",
       "   'Niroshinie Fernando',\n",
       "   'Seng Loke',\n",
       "   'Wenny Rahayu',\n",
       "   'Nelson Gonzalez',\n",
       "   'Charles Miers',\n",
       "   'Fernando Redigolo',\n",
       "   'Tereza Carvalho',\n",
       "   'Marcos Simplicio',\n",
       "   'Mats Naslund',\n",
       "   'Makan Pourzandi',\n",
       "   'Kevin Hamlen',\n",
       "   'Murat Kantarcioglu',\n",
       "   'Latifur Khan',\n",
       "   'Bhavani Thuraisingham',\n",
       "   'Jinguang Han',\n",
       "   'Willy Susilo',\n",
       "   'Yi Mu',\n",
       "   'Keiko Hashizume',\n",
       "   'David Rosado',\n",
       "   'Eduardo Fernández-Medina',\n",
       "   'Eduardo Fernandez',\n",
       "   'Noriswadi Ismail',\n",
       "   'Andrew Joint',\n",
       "   'Edwin Baker',\n",
       "   'Andrew Joint',\n",
       "   'Edwin Baker',\n",
       "   'Edward Eccles',\n",
       "   'K Jorissen',\n",
       "   'F Vila',\n",
       "   'J Rehr',\n",
       "   'Md. Khorshed',\n",
       "   'A Ali',\n",
       "   'Saleh Wasimi',\n",
       "   'Hyungjoo Kim',\n",
       "   'Hyunsoo Chung',\n",
       "   'Jungho Kang',\n",
       "   'Won Kim',\n",
       "   'Nancy King',\n",
       "   'V Raja',\n",
       "   'Ashish Kumar',\n",
       "   'S Kuyoro',\n",
       "   'F Ibikunle',\n",
       "   'O Awodele',\n",
       "   'K Lee',\n",
       "   'Sean Marston',\n",
       "   'Zhi Li',\n",
       "   'Subhajyoti Bandyopadhyay',\n",
       "   'Juheng Zhang',\n",
       "   'Anand Ghalsasi',\n",
       "   'Stephen Mason',\n",
       "   'Esther George',\n",
       "   'M Mircea',\n",
       "   'R Mosher',\n",
       "   'F Ogigau-Neamtiu',\n",
       "   'M Okuhara',\n",
       "   'T Shiozaki',\n",
       "   'T Suzuki',\n",
       "   'Dana Petcu',\n",
       "   'Georgiana Macariu',\n",
       "   'Silviu Panica',\n",
       "   'Ciprian Crăciun',\n",
       "   'R Petre',\n",
       "   'S Qaisar',\n",
       "   'K Khawaja',\n",
       "   'Sahoo Rashmi',\n",
       "   'G Mehfuz',\n",
       "   'G Mehfuz',\n",
       "   'Patrick Ryan',\n",
       "   'Sarah Falvey',\n",
       "   'S Sharma',\n",
       "   'U Mittal',\n",
       "   'Swarnpreet Singh',\n",
       "   'T Jangwal',\n",
       "   'Dan Svantesson',\n",
       "   'Roger Clarke',\n",
       "   'K Suresh',\n",
       "   'K Prasad',\n",
       "   'David Teneyuca',\n",
       "   'C Westphall',\n",
       "   'C Westphall',\n",
       "   'F Koch',\n",
       "   'C Rolim',\n",
       "   'K Vieira',\n",
       "   'A Schulter',\n",
       "   'S Chaves',\n",
       "   'J Werner',\n",
       "   'R Mendes',\n",
       "   'R Brinhosa',\n",
       "   'G Geronimo',\n",
       "   'R Freitas',\n",
       "   'Ali Yassin',\n",
       "   'Hai Jin',\n",
       "   'Ayad Ibrahim',\n",
       "   'Weizhong Qiang',\n",
       "   'Deqing Zou',\n",
       "   'A Youssef',\n",
       "   'Dimitrios Zissis',\n",
       "   'Dimitrios Lekkas']},\n",
       " '9-12': {'abstract': \"Artificial Intelligence is making a machine behave in a way that would be intelligent if a human were so behaving. Many industrial sectors have adopted artificial intelligence for the development and customer satisfaction. Banking sectors were lagging back in adopting this intelligence as this sector requires dependence of human workforce to think and decide. Banks are now implementing Artificial intelligence for managing risk, financial analytics and portfolio/investment management. It helps to make decision based on the various data the banks have supplied to the Artificial intelligence device. This is useful for the banks internally and for the clients externally. Most of the work will become digitalized in future. It will decrease the work load and it will save various processing cost. There is lot of advantages with the adaption of Artificial intelligence. At the same time, Technology may replace human intelligence where machine may overtake human being. They will also face some problem regarding unemployment; responsibility of the transaction, decision taken by the artificial intelligence won't be fully trusted, and many more. This paper discusses about the various opportunities and challenges a banking sectors may face due to artificial intelligence.\",\n",
       "  'authors': ['Ms Mani',\n",
       "   'Fatima Raju',\n",
       "   'Madhura Ayachit',\n",
       "   'Munnish Sabharwal']},\n",
       " '907-Article Text-2692-1-10-20230720': {'abstract': 'As artificial intelligence (AI) technology becomes increasingly integrated into healthcare, it is crucial for clinicians to possess a comprehensive understanding of its capabilities, limitations, and ethical implications. This literature review explores the reasons why clinicians need to be better informed about artificial intelligence, emphasizes the potential benefits of artificial intelligence in healthcare, raises awareness regarding the risks and unintended consequences associated with its use, discusses the development of machine learning and artificial intelligence in healthcare, and underscores the need for ethical guidelines and regulation to harness the potential of artificial intelligence in a responsible manner.',\n",
       "  'authors': ['Ahmad Fawzy',\n",
       "   'Danastri Cantya Nirmala',\n",
       "   'Denaya Khansa',\n",
       "   'Yudhistira Wardhana',\n",
       "   'Danastri Nirmala',\n",
       "   'Pavel Hamet',\n",
       "   'Johanne Tremblay',\n",
       "   'Johanne Tremblay',\n",
       "   'P Esmaeilzadeh',\n",
       "   'P Esmaeilzadeh',\n",
       "   'F Jiang',\n",
       "   'Y Jiang',\n",
       "   'H Zhi',\n",
       "   'Y Dong',\n",
       "   'H Li',\n",
       "   'S Ma',\n",
       "   'Y Wang',\n",
       "   'Q Dong',\n",
       "   'H Shen',\n",
       "   'Y Wang',\n",
       "   'Yasser Abdullah',\n",
       "   'Joel Schuman',\n",
       "   'Ridwan Shabsigh',\n",
       "   'Arthur Caplan',\n",
       "   'Lama Al-Aswad',\n",
       "   'Omar Ali',\n",
       "   'Wiem Abdelbaki',\n",
       "   'Anup Shrestha',\n",
       "   'Ersin Elbasi',\n",
       "   'Mohammad Alryalat',\n",
       "   'Yogesh Dwivedi',\n",
       "   'Chris Giordano',\n",
       "   'Meghan Brennan',\n",
       "   'Basma Mohamed',\n",
       "   'Parisa Rashidi',\n",
       "   'François Modave',\n",
       "   'Patrick Tighe',\n",
       "   'Patrick Tighe',\n",
       "   'Patrick Tighe',\n",
       "   'S Alotaibi',\n",
       "   'Rahatara Ferdousi',\n",
       "   'M Hossain',\n",
       "   'Abdulmotaleb El Saddik',\n",
       "   'Thomas Davenport',\n",
       "   'Ravi Kalakota',\n",
       "   'Mei Chen',\n",
       "   'Michel Decary',\n",
       "   'Michel Decary',\n",
       "   'P Romanski',\n",
       "   'P Bortoletto',\n",
       "   'S Pfeifer',\n",
       "   'Robert Challen',\n",
       "   'Joshua Denny',\n",
       "   'Martin Pitt',\n",
       "   'Luke Gompels',\n",
       "   'Tom Edwards',\n",
       "   'Krasimira Tsaneva-Atanasova',\n",
       "   'Mohammad Ennab',\n",
       "   'Hamid Mcheick',\n",
       "   'Alison Darcy',\n",
       "   'Alan Louie',\n",
       "   'Laura Roberts',\n",
       "   'Naveed Afzal',\n",
       "   'Sunghwan Sohn',\n",
       "   'Sara Abram',\n",
       "   'Christopher Scott',\n",
       "   'Rajeev Chaudhry',\n",
       "   'Hongfang Liu',\n",
       "   'Iftikhar Kullo',\n",
       "   'Adelaide Arruda-Olson',\n",
       "   'Adelaide Arruda-Olson',\n",
       "   'Jessica Vamathevan',\n",
       "   'Dominic Clark',\n",
       "   'Paul Czodrowski',\n",
       "   'Ian Dunham',\n",
       "   'Edgardo Ferran',\n",
       "   'George Lee',\n",
       "   'Bin Li',\n",
       "   'Anant Madabhushi',\n",
       "   'Parantu Shah',\n",
       "   'Michaela Spitzer',\n",
       "   'Shanrong Zhao',\n",
       "   'Xxxiv Grewal',\n",
       "   'J Tessier-Cloutier',\n",
       "   'B Jones',\n",
       "   'M Gakkhar',\n",
       "   'S Ma',\n",
       "   'Y Moore',\n",
       "   'Y Moore',\n",
       "   'Y Moore',\n",
       "   'Y Moore',\n",
       "   'I Wasito',\n",
       "   'N Hanifah',\n",
       "   'R Mufidah',\n",
       "   'R Mufidah',\n",
       "   'R Jansson',\n",
       "   'K Hufthammer',\n",
       "   'J Krohn',\n",
       "   'Xxxviii Shademan',\n",
       "   'A Decker',\n",
       "   'R Opfermann',\n",
       "   'J Leonard',\n",
       "   'S Krieger',\n",
       "   'A Kim',\n",
       "   'Pc ; Soguero-Ruiz',\n",
       "   'C Hindberg',\n",
       "   'K Mora-Jiménez',\n",
       "   'I Rojo-Álvarez',\n",
       "   'J Skrøvseth',\n",
       "   'S Godtliebsen',\n",
       "   'S Godtliebsen',\n",
       "   'Valentina Bellini',\n",
       "   'Marina Valente',\n",
       "   'Antonio Gaddi',\n",
       "   'Paolo Pelosi',\n",
       "   'Elena Bignami',\n",
       "   'Elena Bignami',\n",
       "   'L Lehmann',\n",
       "   'L Lehmann',\n",
       "   'A Kiseleva',\n",
       "   'D Kotzinos',\n",
       "   'P De Hert',\n",
       "   'P De Hert',\n",
       "   'P Bagave',\n",
       "   'M Westberg',\n",
       "   'R Dobbe',\n",
       "   'M Janssen',\n",
       "   'A Ding',\n",
       "   'A Ding',\n",
       "   'I Bartoletti',\n",
       "   'Onur Asan',\n",
       "   'Alparslan Bayrak',\n",
       "   'Avishek Choudhury',\n",
       "   'L Bates',\n",
       "   'D Landman',\n",
       "   'A Levine',\n",
       "   'D Mcafee',\n",
       "   'A Nichols',\n",
       "   'B Pawlson',\n",
       "   'G Blumenthal',\n",
       "   'G Blumenthal',\n",
       "   'Jianxing He',\n",
       "   'Sally Baxter',\n",
       "   'Jie Xu',\n",
       "   'Jiming Xu',\n",
       "   'Xingtao Zhou',\n",
       "   'Kang Zhang',\n",
       "   'Kang Zhang',\n",
       "   'D Bates',\n",
       "   'A Auerbach',\n",
       "   'P Schulam',\n",
       "   'A Wright',\n",
       "   'S Saria',\n",
       "   'J Liii. Yin',\n",
       "   'K Ngiam',\n",
       "   'H Teo',\n",
       "   'L Dunn',\n",
       "   'A Tong',\n",
       "   'H Kocaballi',\n",
       "   'A Chen',\n",
       "   'J Bashir',\n",
       "   'R Surian',\n",
       "   'R Surian',\n",
       "   'R Surian',\n",
       "   'S Nundy',\n",
       "   'K Patel',\n",
       "   'R Baron']},\n",
       " 'CBIR USING FEATURES DERIVED BY DEEP LEARNING': {'abstract': 'In a Content Based Image Retrieval (CBIR) System, the task is to retrieve similar images from a large database given a query image. The usual procedure is to extract some useful features from the query image, and retrieve images which have similar set of features. For this purpose, a suitable similarity measure is chosen, and images with high similarity scores are retrieved. Naturally the choice of these features play a very important role in the success of this system, and high level features are required to reduce the \"semantic gap\". In this paper, we propose to use features derived from pre-trained network models from a deeplearning convolution network trained for a large image classification problem. This approach appears to produce vastly superior results for a variety of databases, and it outperforms many contemporary CBIR systems. We analyse the retrieval time of the method, and also propose a pre-clustering of the database based on the above-mentioned features which yields comparable results in a much shorter time in most of the cases.',\n",
       "  'authors': ['Subhadip Maji',\n",
       "   'Smarajit Bose',\n",
       "   'Smarajit Bose',\n",
       "   'Smarajit Bose',\n",
       "   'Khawaja Ahmed',\n",
       "   'Syed Naqvi',\n",
       "   'Amjad Rehman',\n",
       "   'Tanzila Saba',\n",
       "   'Khawaja Ahmed',\n",
       "   'Shahida Ummesafi',\n",
       "   'Amjad Iqbal',\n",
       "   'S Aksoy',\n",
       "   'R Haralick',\n",
       "   'Rehan Ashraf',\n",
       "   'Mudassar Ahmed',\n",
       "   'Sohail Jabbar',\n",
       "   'Shehzad Khalid',\n",
       "   'Awais Ahmad',\n",
       "   'Sadia Din',\n",
       "   'Gwangil Jeon',\n",
       "   'Rehan Ashraf',\n",
       "   'Khalid Bashir',\n",
       "   'Aun Irtaza',\n",
       "   'Muhammad Mahmood',\n",
       "   'Mohamed Uvaze',\n",
       "   'Ahamed Ayoobkhan',\n",
       "   'C Eswaran',\n",
       "   'Kannan Ramakrishnan',\n",
       "   'Smarajit Bose',\n",
       "   'Amita Pal',\n",
       "   'Disha Chakrabarti',\n",
       "   'Taranga Mukherjee',\n",
       "   'Alfredo Canziani',\n",
       "   'Adam Paszke',\n",
       "   'Eugenio Culurciello',\n",
       "   'François Chollet',\n",
       "   'Francois Chollet',\n",
       "   'Kishan Maladkar',\n",
       "   'Jia Deng',\n",
       "   'Wei Dong',\n",
       "   'Richard Socher',\n",
       "   'Li-Jia Li',\n",
       "   'Li-Jia Li',\n",
       "   'Li-Jia Li',\n",
       "   'Kaiming He',\n",
       "   'Gao Huang',\n",
       "   'Jing Huang',\n",
       "   'Safia Jabeen',\n",
       "   'Zahid Mehmood',\n",
       "   'Toqeer Mahmood',\n",
       "   'Tanzila Saba',\n",
       "   'Amjad Rehman',\n",
       "   'Muhammad Mahmood',\n",
       "   'Suman Khokhar',\n",
       "   'Satya Verma',\n",
       "   'Harald Kosch',\n",
       "   'Harald Kosch',\n",
       "   'R Fergus',\n",
       "   'P Perona',\n",
       "   'Yogen Mr',\n",
       "   'Yogen Mr',\n",
       "   'Yogen Mr',\n",
       "   'J Sushant',\n",
       "   'J Sushant',\n",
       "   'B Manjunath',\n",
       "   'J-R Ohm',\n",
       "   'V Vasudevan',\n",
       "   'A Yamada',\n",
       "   'Pedro Marcelino',\n",
       "   'Zahid Mehmood',\n",
       "   'Toqeer Mahmood',\n",
       "   'Muhammad Javid',\n",
       "   'Carlton Niblack',\n",
       "   'Ron Barber',\n",
       "   'Will Equitz',\n",
       "   'Myron Flickner',\n",
       "   'Eduardo Glasman',\n",
       "   'Dragutin Petkovic',\n",
       "   'Peter Yanker',\n",
       "   'Christos Faloutsos',\n",
       "   'Gabriel Taubin',\n",
       "   'A Obulesu',\n",
       "   'V Vijay Kumar',\n",
       "   'L Sumalatha',\n",
       "   'T Ojala',\n",
       "   'M Aittola',\n",
       "   'E Matinmikko',\n",
       "   'Michael Ortega-Binderberger',\n",
       "   'Jing Peng',\n",
       "   'Bir Bhanu',\n",
       "   'Shan Qing',\n",
       "   'Soumya Rana',\n",
       "   'Maitreyee Dey',\n",
       "   'Patrick Siarry',\n",
       "   'Abdolreza Rashno',\n",
       "   'Saeed Sadri',\n",
       "   'Mark Sandler',\n",
       "   'Amna Sarwar',\n",
       "   'Zahid Mehmood',\n",
       "   'Tanzila Saba',\n",
       "   'Khurram Qazi',\n",
       "   'Ahmed Adnan',\n",
       "   'Habibullah Jamal',\n",
       "   'G Satya Kumar',\n",
       "   'P Krishna Mohan',\n",
       "   'Seong-O Shim',\n",
       "   'Tae-Sun Choi',\n",
       "   'Uzma Sharif',\n",
       "   'Zahid Mehmood',\n",
       "   'Toqeer Mahmood',\n",
       "   'Muhammad Javid',\n",
       "   'Amjad Rehman',\n",
       "   'Tanzila Saba',\n",
       "   'Jonathon Shlens',\n",
       "   'Karen Simonyan',\n",
       "   'Andrew Zisserman',\n",
       "   'Christian Szegedy',\n",
       "   'Sergey Ioffe',\n",
       "   'Vincent Vanhoucke',\n",
       "   'Alexander Alemi',\n",
       "   'Lisa Torrey',\n",
       "   'Jude Shavlik',\n",
       "   'Jian Xu',\n",
       "   'Muhammad Yousuf',\n",
       "   'Zahid Mehmood',\n",
       "   'Hafiz Habib',\n",
       "   'Toqeer Mahmood',\n",
       "   'Tanzila Saba',\n",
       "   'Amjad Rehman',\n",
       "   'Muhammad Rashid',\n",
       "   'Matthew Zeiler',\n",
       "   'Graham Taylor',\n",
       "   'Rob Fergus',\n",
       "   'Wengang Zhou',\n",
       "   'Houqiang Li',\n",
       "   'Qi Tian',\n",
       "   'Barret Zoph']},\n",
       " 'hir-22-351': {'abstract': 'Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator formally to specify all of the knowledge needed by the computer. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in relation to deep learning.',\n",
       "  'authors': ['Kwang Kim',\n",
       "   'Ian Goodfellow',\n",
       "   'Yoshua Bengio',\n",
       "   'Aaron Courville',\n",
       "   'K Murphy',\n",
       "   'C Bishop',\n",
       "   'Le Cun',\n",
       "   'Y Jackel',\n",
       "   'L Boser',\n",
       "   'B Denker',\n",
       "   'J Graf',\n",
       "   'H Guyon',\n",
       "   'H Guyon',\n",
       "   'David Rumelhart',\n",
       "   'Geoffrey Hinton',\n",
       "   'Ronald Williams',\n",
       "   'A Ng',\n",
       "   'A Ng',\n",
       "   'Geoffrey Hinton',\n",
       "   'Peter Dayan',\n",
       "   'Brendan Frey',\n",
       "   'Radford Neal',\n",
       "   'Z Ghahramani',\n",
       "   'G Hinton',\n",
       "   'Sam Roweis',\n",
       "   'Zoubin Ghahramani']},\n",
       " 'How good are deep models in understanding generated images': {'abstract': 'My goal in this paper is twofold: to study how well deep models can understand the images generated by DALL-E 2 and Midjourney, and to quantitatively evaluate these generative models. Two sets of generated images are collected for object recognition and visual question answering (VQA) tasks. On object recognition, the best model, out of 10 state-of-the-art object recognition models, achieves about 60% and 80% top-1 and top-5 accuracy, respectively. These numbers are much lower than the best accuracy on the ImageNet dataset (91% and 99%). On VQA, the OFA model scores 77.3% on answering 241 binary questions across 50 images. This model scores 94.7% on the binary VQA-v2 dataset. Humans are able to recognize the generated images and answer questions on them easily. We conclude that a) deep models struggle to understand the generated content, and may do better after fine-tuning, and b) there is a large distribution shift between the generated images and the real photographs. The distribution shift appears to be category-dependent. Data is available at: link.',\n",
       "  'authors': ['Ali Borji',\n",
       "   'Stanislaw Antol',\n",
       "   'Aishwarya Agrawal',\n",
       "   'Jiasen Lu',\n",
       "   'Margaret Mitchell',\n",
       "   'Dhruv Batra',\n",
       "   'C Zitnick',\n",
       "   'Devi Parikh',\n",
       "   'Ali Borji',\n",
       "   'Ali Borji',\n",
       "   'Yunhao Ge',\n",
       "   'Jiashu Xu',\n",
       "   'Brian Zhao',\n",
       "   'Laurent Itti',\n",
       "   'Vibhav Vineet',\n",
       "   'Ian Goodfellow',\n",
       "   'Jean Pouget-Abadie',\n",
       "   'Mehdi Mirza',\n",
       "   'Bing Xu',\n",
       "   'David Warde-Farley',\n",
       "   'Sherjil Ozair',\n",
       "   'Aaron Courville',\n",
       "   'Yoshua Bengio',\n",
       "   'Kaiming He',\n",
       "   'Xiangyu Zhang',\n",
       "   'Shaoqing Ren',\n",
       "   'Jian Sun',\n",
       "   'Martin Heusel',\n",
       "   'Hubert Ramsauer',\n",
       "   'Thomas Unterthiner',\n",
       "   'Bernhard Nessler',\n",
       "   'Sepp Hochreiter',\n",
       "   'Gao Huang',\n",
       "   'Zhuang Liu',\n",
       "   'Laurens Van Der Maaten',\n",
       "   'Kilian Weinberger',\n",
       "   'Alex Krizhevsky',\n",
       "   'Ilya Sutskever',\n",
       "   'Geoffrey Hinton',\n",
       "   'Dhruv Mahajan',\n",
       "   'Ross Girshick',\n",
       "   'Vignesh Ramanathan',\n",
       "   'Kaiming He',\n",
       "   'Manohar Paluri',\n",
       "   'Yixuan Li',\n",
       "   'Ashwin Bharambe',\n",
       "   'Laurens Van Der Maaten',\n",
       "   'Gary Marcus',\n",
       "   'Ernest Davis',\n",
       "   'Scott Aaronson',\n",
       "   'Aditya Ramesh',\n",
       "   'Prafulla Dhariwal',\n",
       "   'Alex Nichol',\n",
       "   'Casey Chu',\n",
       "   'Mark Chen',\n",
       "   'S Mehdi',\n",
       "   'Olivier Sajjadi',\n",
       "   'Mario Bachem',\n",
       "   'Olivier Lucic',\n",
       "   'Sylvain Bousquet',\n",
       "   'Sylvain Bousquet',\n",
       "   'Mark Sandler',\n",
       "   'Andrew Howard',\n",
       "   'Menglong Zhu',\n",
       "   'Andrey Zhmoginov',\n",
       "   'Liang-Chieh Chen',\n",
       "   'Christian Szegedy',\n",
       "   'Christian Szegedy',\n",
       "   'Christian Szegedy',\n",
       "   'Pierre Sermanet',\n",
       "   'Scott Reed',\n",
       "   'Dragomir Anguelov',\n",
       "   'Dumitru Erhan',\n",
       "   'Vincent Vanhoucke',\n",
       "   'Andrew Rabinovich',\n",
       "   'Christian Szegedy',\n",
       "   'Vincent Vanhoucke',\n",
       "   'Sergey Ioffe',\n",
       "   'Jon Shlens',\n",
       "   'Zbigniew Wojna',\n",
       "   'Hugo Touvron',\n",
       "   'Alexandre Sablayrolles',\n",
       "   'Matthijs Douze',\n",
       "   'Matthieu Cord',\n",
       "   'Herve Jegou',\n",
       "   'Peng Wang',\n",
       "   'An Yang',\n",
       "   'Rui Men',\n",
       "   'Junyang Lin',\n",
       "   'Shuai Bai',\n",
       "   'Zhikang Li',\n",
       "   'Jianxin Ma',\n",
       "   'Chang Zhou',\n",
       "   'Jingren Zhou',\n",
       "   'Hongxia Yang',\n",
       "   'Saining Xie',\n",
       "   'Ross Girshick',\n",
       "   'Piotr Dollar',\n",
       "   'Zhuowen Tu',\n",
       "   'Kaiming He']},\n",
       " 'IJISRT23AUG773': {'abstract': 'Cloud computing has revolutionized the way businesses process, store and manage data. With the explosion of big data, cloud computing has become an integral tool in providing scalable and cost-effective solutions for handling large volumes of data. In this paper, we will explore the role of cloud computing in big data and its impact on businesses. We will start by defining cloud computing and its relation to big data. We will also examine the benefits of using cloud computing for big data processing and storage, as well as the challenges that come with implementing cloud computing for big data. Additionally, we will discuss the different cloud computing architecture models used in big data processing and storage and compare them to traditional on-premise computing architecture. We will analyze the advantages and disadvantages of each cloud computing architecture model for big data. Finally, we will explore the common applications of cloud computing for big data and the trends and future developments in cloud computing for big data, including the potential challenges and opportunities for using cloud computing for big data in the future.',\n",
       "  'authors': ['Farhan Aslam',\n",
       "   'B Berisha',\n",
       "   'E Mëziu',\n",
       "   'I Shabani',\n",
       "   'Christos Stergiou',\n",
       "   'Kostas Psannis',\n",
       "   'Brij Gupta',\n",
       "   'Yutaka Ishibashi',\n",
       "   'Ibrahim Hashem',\n",
       "   'Ibrar Yaqoob',\n",
       "   'Nor Anuar',\n",
       "   'Salimah Mokhtar',\n",
       "   'Abdullah Gani',\n",
       "   'Samee Ullah Khan',\n",
       "   'Aisling O’driscoll',\n",
       "   'Jurate Daugelaite',\n",
       "   'Roy Sleator',\n",
       "   'Madhusmita Das',\n",
       "   'Rasmita Dash',\n",
       "   'Mehdi Bahrami',\n",
       "   'Mukesh Singhal',\n",
       "   'Bo Tang',\n",
       "   'Zhen Chen',\n",
       "   'Gerald Hefferman',\n",
       "   'Tao Wei',\n",
       "   'Haibo He',\n",
       "   'Qing Yang',\n",
       "   'B Purcell',\n",
       "   'M Mayilvaganan',\n",
       "   'M Sabitha',\n",
       "   'Gunasekaran Manogaran',\n",
       "   'Chandu Thota',\n",
       "   'M Kumar',\n",
       "   'Changqing Ji',\n",
       "   'Yu Li',\n",
       "   'Wenming Qiu',\n",
       "   'Uchechukwu Awada',\n",
       "   'Keqiu Li',\n",
       "   'Alfred Zimmermann',\n",
       "   'Michael Pretz',\n",
       "   'Gertrud Zimmermann',\n",
       "   'Donald Firesmith',\n",
       "   'Ilia Petrov',\n",
       "   'Chaowei Yang',\n",
       "   'Qunying Huang',\n",
       "   'Zhenlong Li',\n",
       "   'Kai Liu',\n",
       "   'Fei Hu',\n",
       "   'Rainer Schmidt',\n",
       "   'Michael Mohring',\n",
       "   'Anna Kobusińska',\n",
       "   'Carson Leung',\n",
       "   'Ching-Hsien Hsu',\n",
       "   'Raghavendra S.',\n",
       "   'Victor Chang',\n",
       "   'Mohamed Elhoseny',\n",
       "   'Ahmed Abdelaziz',\n",
       "   'Ahmed Salama',\n",
       "   'A Riad',\n",
       "   'Khan Muhammad',\n",
       "   'Arun Sangaiah',\n",
       "   'Divyakant Agrawal',\n",
       "   'Sudipto Das',\n",
       "   'Amr El Abbadi',\n",
       "   'Xuanjun Chen',\n",
       "   'N Metawa',\n",
       "   'B Gupta',\n",
       "   'D Agrawal',\n",
       "   'Richard Branch',\n",
       "   'Heather Tjeerdsma',\n",
       "   'Cody Wilson',\n",
       "   'Richard Hurley',\n",
       "   'Sabine Mcconnell',\n",
       "   'Farhan Aslam']},\n",
       " 'IMAGEBERT CROSS-MODAL PRE-TRAINING WITH': {'abstract': 'In this paper, we introduce a new vision-language pre-trained model -ImageBERT -for image-text joint embedding. Our model is a Transformer[1]-based model, which takes different modalities as input and models the relationship between them. The model is pre-trained on four tasks simultaneously: Masked Language Modeling (MLM), Masked Object Classification (MOC), Masked Region Feature Regression (MRFR), and Image Text Matching (ITM). To further enhance the pre-training quality, we have collected a Large-scale weAk-supervised Image-Text (LAIT) dataset from Web. We first pre-train the model on this dataset, then conduct a second stage pre-training on Conceptual Captions[2] and SBU Captions ',\n",
       "  'authors': ['Di Qi',\n",
       "   'Lin Su',\n",
       "   'Jia Song',\n",
       "   'Edward Cui',\n",
       "   'Taroon Bharti',\n",
       "   'Arun Sacheti',\n",
       "   'Ashish Vaswani',\n",
       "   'Noam Shazeer',\n",
       "   'Niki Parmar',\n",
       "   'Jakob Uszkoreit',\n",
       "   'Llion Jones',\n",
       "   'Aidan Gomez',\n",
       "   'Lukasz Kaiser',\n",
       "   'Illia Polosukhin',\n",
       "   'Piyush Sharma',\n",
       "   'Nan Ding',\n",
       "   'Sebastian Goodman',\n",
       "   'Radu Soricut',\n",
       "   'Vicente Ordonez',\n",
       "   'Girish Kulkarni',\n",
       "   'Tamara Berg',\n",
       "   'Andrej Karpathy',\n",
       "   'Li Fei-Fei',\n",
       "   'Xinlei Chen',\n",
       "   'Hao Fang',\n",
       "   'Tsung-Yi Lin',\n",
       "   'Ramakrishna Vedantam',\n",
       "   'Saurabh Gupta',\n",
       "   'Piotr Dollar',\n",
       "   'C Zitnick',\n",
       "   'Peter Young',\n",
       "   'Alice Lai',\n",
       "   'Micah Hodosh',\n",
       "   'Julia Hockenmaier',\n",
       "   'Aishwarya Agrawal',\n",
       "   'Jiasen Lu',\n",
       "   'Stanislaw Antol',\n",
       "   'Margaret Mitchell',\n",
       "   'C Zitnick',\n",
       "   'Dhruv Batra',\n",
       "   'Devi Parikh',\n",
       "   'Rowan Zellers',\n",
       "   'Yonatan Bisk',\n",
       "   'Ali Farhadi',\n",
       "   'Yejin Choi',\n",
       "   'Peter Anderson',\n",
       "   'Xiaodong He',\n",
       "   'Chris Buehler',\n",
       "   'Damien Teney',\n",
       "   'Mark Johnson',\n",
       "   'Stephen Gould',\n",
       "   'Lei Zhang',\n",
       "   'Jacob Devlin',\n",
       "   'Ming-Wei Chang',\n",
       "   'Kenton Lee',\n",
       "   'Kristina Toutanova',\n",
       "   'Kaiming He',\n",
       "   'Xiangyu Zhang',\n",
       "   'Shaoqing Ren',\n",
       "   'Jian Sun',\n",
       "   'Zhilin Yang',\n",
       "   'Zihang Dai',\n",
       "   'Yiming Yang',\n",
       "   'Jaime Carbonell',\n",
       "   'Ruslan Salakhutdinov',\n",
       "   'V Quoc',\n",
       "   'V Quoc',\n",
       "   'V Quoc',\n",
       "   'Yinhan Liu',\n",
       "   'Myle Ott',\n",
       "   'Naman Goyal',\n",
       "   'Jingfei Du',\n",
       "   'Mandar Joshi',\n",
       "   'Danqi Chen',\n",
       "   'Omer Levy',\n",
       "   'Mike Lewis',\n",
       "   'Luke Zettlemoyer',\n",
       "   'Veselin Stoyanov',\n",
       "   'Veselin Stoyanov',\n",
       "   'Jiasen Lu',\n",
       "   'Vedanuj Goswami',\n",
       "   'Marcus Rohrbach',\n",
       "   'Devi Parikh',\n",
       "   'Stefan Lee',\n",
       "   'Hao Hao',\n",
       "   'Hao Hao',\n",
       "   'Mohit Bansal',\n",
       "   'Liunian Harold',\n",
       "   'Liunian Harold',\n",
       "   'Mark Yatskar',\n",
       "   'Cho-Jui Da Yin',\n",
       "   'Kai-Wei Hsieh',\n",
       "   'Kai-Wei Hsieh',\n",
       "   'Chris Alberti',\n",
       "   'Jeffrey Ling',\n",
       "   'Michael Collins',\n",
       "   'David Reitter',\n",
       "   'Gen Li',\n",
       "   'Nan Duan',\n",
       "   'Yuejian Fang',\n",
       "   'Ming Gong',\n",
       "   'Daxin Jiang',\n",
       "   'Weijie Su',\n",
       "   'Xizhou Zhu',\n",
       "   'Yue Cao',\n",
       "   'Bin Li',\n",
       "   'Lewei Lu',\n",
       "   'Furu Wei',\n",
       "   'Jifeng Dai',\n",
       "   'Jifeng Dai',\n",
       "   'Luowei Zhou',\n",
       "   'Hamid Palangi',\n",
       "   'Lei Zhang',\n",
       "   'Houdong Hu',\n",
       "   'Jason Corso',\n",
       "   'Jianfeng Gao',\n",
       "   'Yen-Chun Chen',\n",
       "   'Linjie Li',\n",
       "   'Licheng Yu',\n",
       "   'Ahmed El Kholy',\n",
       "   'Faisal Ahmed',\n",
       "   'Zhe Gan',\n",
       "   'Yu Cheng',\n",
       "   'Jingjing Liu',\n",
       "   'Ranjay Krishna',\n",
       "   'Yuke Zhu',\n",
       "   'Oliver Groth',\n",
       "   'Justin Johnson',\n",
       "   'Kenji Hata',\n",
       "   'Joshua Kravitz',\n",
       "   'Stephanie Chen',\n",
       "   'Yannis Kalantidis',\n",
       "   'Li-Jia Li',\n",
       "   'David Shamma',\n",
       "   'Michael Bernstein',\n",
       "   'Li Fei-Fei',\n",
       "   'Yukun Zhu',\n",
       "   'Ryan Kiros',\n",
       "   'Rich Zemel',\n",
       "   'Ruslan Salakhutdinov',\n",
       "   'Raquel Urtasun',\n",
       "   'Antonio Torralba',\n",
       "   'Sanja Fidler',\n",
       "   'Amanpreet Singh',\n",
       "   'Vivek Natarajan',\n",
       "   'Meet Shah',\n",
       "   'Yu Jiang',\n",
       "   'Xinlei Chen',\n",
       "   'Dhruv Batra',\n",
       "   'Devi Parikh',\n",
       "   'Marcus Rohrbach',\n",
       "   'Amanpreet Singh',\n",
       "   'Vivek Natarajan',\n",
       "   'Meet Shah',\n",
       "   'Yu Jiang',\n",
       "   'Xinlei Chen',\n",
       "   'Dhruv Batra',\n",
       "   'Devi Parikh',\n",
       "   'Marcus Rohrbach',\n",
       "   'Yonghui Wu',\n",
       "   'Mike Schuster',\n",
       "   'Zhifeng Chen',\n",
       "   'Quoc Le',\n",
       "   'Mohammad Norouzi',\n",
       "   'Wolfgang Macherey',\n",
       "   'Maxim Krikun',\n",
       "   'Yuan Cao',\n",
       "   'Qin Gao',\n",
       "   'Klaus Macherey',\n",
       "   'Jeff Klingner',\n",
       "   'Apurva Shah',\n",
       "   'Melvin Johnson',\n",
       "   'Xiaobing Liu',\n",
       "   'Łukasz Kaiser',\n",
       "   'Stephan Gouws',\n",
       "   'Yoshikiyo Kato',\n",
       "   'Taku Kudo',\n",
       "   'Hideto Kazawa',\n",
       "   'Keith Stevens',\n",
       "   'George Kurian',\n",
       "   'Nishant Patil',\n",
       "   'Wei Wang',\n",
       "   'Jeremy Howard',\n",
       "   'Sebastian Ruder',\n",
       "   'Pinghua Gong',\n",
       "   'Jieping Ye',\n",
       "   'Changshui Zhang',\n",
       "   'Kuang-Huei Lee',\n",
       "   'Xi Chen',\n",
       "   'Gang Hua',\n",
       "   'Houdong Hu',\n",
       "   'Xiaodong He',\n",
       "   'Botian Shi',\n",
       "   'Lei Ji',\n",
       "   'Pan Lu',\n",
       "   'Zhendong Niu',\n",
       "   'Nan Duan',\n",
       "   'Yaxiong Wang',\n",
       "   'Hao Yang',\n",
       "   'Xueming Qian',\n",
       "   'Lin Ma',\n",
       "   'Jing Lu',\n",
       "   'Biao Li',\n",
       "   'Xin Fan',\n",
       "   'Forrest Iandola',\n",
       "   'Matthew Moskewicz',\n",
       "   'Kurt Keutzer',\n",
       "   'Christian Szegedy',\n",
       "   'Christian Szegedy',\n",
       "   'Christian Szegedy',\n",
       "   'Pierre Sermanet',\n",
       "   'Scott Reed',\n",
       "   'Dragomir Anguelov',\n",
       "   'Dumitru Erhan',\n",
       "   'Vincent Vanhoucke',\n",
       "   'Andrew Rabinovich']},\n",
       " 'LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL': {'abstract': 'Methods that combine local and global features have recently shown excellent performance on multiple challenging deep image retrieval benchmarks, but their use of local features raises at least two issues. First, these local features simply boil down to the localized map activations of a neural network, and hence can be extremely redundant. Second, they are typically trained with a global loss that only acts on top of an aggregation of local features; by contrast, testing is based on local feature matching, which creates a discrepancy between training and testing. In this paper, we propose a novel architecture for deep image retrieval, based solely on mid-level features that we call Super-features. These Super-features are constructed by an iterative attention module and constitute an ordered set in which each element focuses on a localized and discriminant image pattern. For training, they require only image labels. A contrastive loss operates directly at the level of Super-features and focuses on those that match across images. A second complementary loss encourages diversity. Experiments on common landmark retrieval benchmarks validate that Super-features substantially outperform state-of-the-art methods when using the same number of features, and only require a significantly smaller memory footprint to match their performance. Code and models are available at: https://github.com/naver/FIRe.',\n",
       "  'authors': ['Philippe Weinzaepfel',\n",
       "   'Thomas Lucas',\n",
       "   'Diane Larlus',\n",
       "   'Yannis Kalantidis',\n",
       "   'Artem Babenko',\n",
       "   'Victor Lempitsky',\n",
       "   'Dzmitry Bahdanau',\n",
       "   'Kyunghyun Cho',\n",
       "   'Yoshua Bengio',\n",
       "   'Y-Lan Boureau',\n",
       "   'Francis Bach',\n",
       "   'Yann Lecun',\n",
       "   'Jean Ponce',\n",
       "   'Bingyi Cao',\n",
       "   'André Araujo',\n",
       "   'Jack Sim',\n",
       "   'Nicolas Carion',\n",
       "   'Francisco Massa',\n",
       "   'Gabriel Synnaeve',\n",
       "   'Nicolas Usunier',\n",
       "   'Alexander Kirillov',\n",
       "   'Sergey Zagoruyko',\n",
       "   'Mathilde Caron',\n",
       "   'Hugo Touvron',\n",
       "   'Ishan Misra',\n",
       "   'Herve Jegou',\n",
       "   'Julien Mairal',\n",
       "   'Piotr Bojanowski',\n",
       "   'Armand Joulin',\n",
       "   'Kai Chen',\n",
       "   'Lanqing Hong',\n",
       "   'Hang Xu',\n",
       "   'Zhenguo Li',\n",
       "   'Dit-Yan Yeung',\n",
       "   'Yunpeng Chen',\n",
       "   'Marcus Rohrbach',\n",
       "   'Zhicheng Yan',\n",
       "   'Yan Shuicheng',\n",
       "   'Jiashi Feng',\n",
       "   'Yannis Kalantidis',\n",
       "   'Gabriela Csurka',\n",
       "   'Martin Humenberger',\n",
       "   'Gabriela Csurka',\n",
       "   'Christopher Dance',\n",
       "   'Florent Perronnin',\n",
       "   'Jutta Willamowski',\n",
       "   'Albert Gordo',\n",
       "   'Albert Gordo',\n",
       "   'Jon Almazán',\n",
       "   'Jerome Revaud',\n",
       "   'Diane Larlus',\n",
       "   'Jean-Bastien Grill',\n",
       "   'Florian Strub',\n",
       "   'Florent Altché',\n",
       "   'Corentin Tallec',\n",
       "   'Kavukcuoglu Koray',\n",
       "   'Rémi Munos',\n",
       "   'Michal Valko',\n",
       "   'Stephen Hausler',\n",
       "   'Sourav Garg',\n",
       "   'Ming Xu',\n",
       "   'Michael Milford',\n",
       "   'Tobias Fischer',\n",
       "   'Kaiming He',\n",
       "   'Xiangyu Zhang',\n",
       "   'Shaoqing Ren',\n",
       "   'Jian Sun',\n",
       "   'Martin Humenberger',\n",
       "   'Yohann Cabon',\n",
       "   'Nicolas Guerin',\n",
       "   'Julien Morat',\n",
       "   'Jérôme Revaud',\n",
       "   'Philippe Rerole',\n",
       "   'Noé Pion',\n",
       "   'Cesar De Souza',\n",
       "   'Vincent Leroy',\n",
       "   'Gabriela Csurka',\n",
       "   'Andrew Jaegle',\n",
       "   'Felix Gimeno',\n",
       "   'Andrew Brock',\n",
       "   'Andrew Zisserman',\n",
       "   'Oriol Vinyals',\n",
       "   'Joao Carreira',\n",
       "   'Herve Jegou',\n",
       "   'Matthijs Douze',\n",
       "   'Cordelia Schmid',\n",
       "   'Patrick Perez',\n",
       "   'Huaizu Jiang',\n",
       "   'Jingdong Wang',\n",
       "   'Zejian Yuan',\n",
       "   'Yang Wu',\n",
       "   'Nanning Zheng',\n",
       "   'Shipeng Li',\n",
       "   'Juho Lee',\n",
       "   'Yoonho Lee',\n",
       "   'Jungtaek Kim',\n",
       "   'Adam Kosiorek',\n",
       "   'Seungjin Choi',\n",
       "   'Yee Teh',\n",
       "   'Shijie Li',\n",
       "   'Junmin Liu',\n",
       "   'Weilin Shen',\n",
       "   'Jianyong Sun',\n",
       "   'Chengli Tan',\n",
       "   'Francesco Locatello',\n",
       "   'Dirk Weissenborn',\n",
       "   'Thomas Unterthiner',\n",
       "   'Aravindh Mahendran',\n",
       "   'Georg Heigold',\n",
       "   'Jakob Uszkoreit',\n",
       "   'Alexey Dosovitskiy',\n",
       "   'Thomas Kipf',\n",
       "   'Thomas Kipf',\n",
       "   'Grégoire Mialon',\n",
       "   'Dexiong Chen',\n",
       "   'Tony Ng',\n",
       "   'Vassileios Balntas',\n",
       "   'Yurun Tian',\n",
       "   'Krystian Mikolajczyk',\n",
       "   'Hyeonwoo Noh',\n",
       "   'Andre Araujo',\n",
       "   'Jack Sim',\n",
       "   'Tobias Weyand',\n",
       "   'Bohyung Han',\n",
       "   'Florent Perronnin',\n",
       "   'Yan Liu',\n",
       "   'Jorge Sanchez',\n",
       "   'Herve Poirier',\n",
       "   'James Philbin',\n",
       "   'Ondrej Chum',\n",
       "   'Michael Isard',\n",
       "   'Josef Sivic',\n",
       "   'Andrew Zisserman',\n",
       "   'James Philbin',\n",
       "   'Ondrej Chum',\n",
       "   'Michael Isard',\n",
       "   'Josef Sivic',\n",
       "   'Andrew Zisserman',\n",
       "   'Filip Radenovic',\n",
       "   'Ahmet Iscen',\n",
       "   'Giorgos Tolias',\n",
       "   'Yannis Avrithis',\n",
       "   'Ondrej Chum',\n",
       "   'Filip Radenovic',\n",
       "   'Giorgos Tolias',\n",
       "   'Ondrej Chum',\n",
       "   'Jerome Revaud',\n",
       "   'Jon Almazan',\n",
       "   'Rafael Rezende',\n",
       "   'Cesar Souza',\n",
       "   'Jerome Revaud',\n",
       "   'Cesar Souza',\n",
       "   'Martin Humenberger',\n",
       "   'Philippe Weinzaepfel',\n",
       "   'Torsten Sattler',\n",
       "   'Will Maddern',\n",
       "   'Carl Toft',\n",
       "   'Akihiko Torii',\n",
       "   'Lars Hammarstrand',\n",
       "   'Erik Stenborg',\n",
       "   'Daniel Safari',\n",
       "   'Masatoshi Okutomi',\n",
       "   'Marc Pollefeys',\n",
       "   'Josef Sivic',\n",
       "   'Fredrik Kahl',\n",
       "   'Tomas Pajdla',\n",
       "   'Saurabh Singh',\n",
       "   'Abhinav Gupta',\n",
       "   'Alexei Efros',\n",
       "   'Alexei Efros',\n",
       "   'Alexei Efros',\n",
       "   'Kihyuk Sohn',\n",
       "   'David Berthelot',\n",
       "   'Chun-Liang Li',\n",
       "   'Zizhao Zhang',\n",
       "   'Nicholas Carlini',\n",
       "   'Ekin Cubuk',\n",
       "   'Alex Kurakin',\n",
       "   'Han Zhang',\n",
       "   'Colin Raffel',\n",
       "   'Marvin Teichmann',\n",
       "   'Andre Araujo',\n",
       "   'Menglong Zhu',\n",
       "   'Jack Sim',\n",
       "   'Giorgos Tolias',\n",
       "   'Yannis Avrithis',\n",
       "   'Herve Jegou',\n",
       "   'Giorgos Tolias',\n",
       "   'Ronan Sicre',\n",
       "   'Hervé Jégou',\n",
       "   'Giorgos Tolias',\n",
       "   'Tomas Jenicek',\n",
       "   'Ondřej Chum',\n",
       "   'Anne Treisman',\n",
       "   'Garry Gelade',\n",
       "   'Ashish Vaswani',\n",
       "   'Noam Shazeer',\n",
       "   'Niki Parmar',\n",
       "   'Jakob Uszkoreit',\n",
       "   'Llion Jones',\n",
       "   'Aidan Gomez',\n",
       "   'Łukasz Kaiser',\n",
       "   'Illia Polosukhin',\n",
       "   'Qianqian Wang',\n",
       "   'Xiaowei Zhou',\n",
       "   'Bharath Hariharan',\n",
       "   'Noah Snavely',\n",
       "   'Xinlong Wang',\n",
       "   'Rufeng Zhang',\n",
       "   'Chunhua Shen',\n",
       "   'Tao Kong',\n",
       "   'Lei Li',\n",
       "   'Tobias Weyand',\n",
       "   'Andre Araujo',\n",
       "   'Bingyi Cao',\n",
       "   'Jack Sim',\n",
       "   'Tianjun Xiao',\n",
       "   'Yichong Xu',\n",
       "   'Kuiyuan Yang',\n",
       "   'Jiaxing Zhang',\n",
       "   'Yuxin Peng',\n",
       "   'Zheng Zhang',\n",
       "   'Zhenda Xie',\n",
       "   'Yutong Lin',\n",
       "   'Zheng Zhang',\n",
       "   'Yue Cao',\n",
       "   'Stephen Lin',\n",
       "   'Han Hu',\n",
       "   'Shuhei Yokoo',\n",
       "   'Kohei Ozaki',\n",
       "   'Edgar Simo-Serra',\n",
       "   'Satoshi Iizuka',\n",
       "   'Qunjie Zhou',\n",
       "   'Torsten Sattler',\n",
       "   'Laura Leal-Taixe']},\n",
       " 'Paper11879': {'abstract': 'Cloud computing has had a significant impact on small and medium-sized businesses (SMBs). The benefits of cloud computing adoption, including cost savings, scalability, flexibility, and improved data security, have made this technology increasingly popular among SMBs. However, several challenges prevent SMBs from adopting cloud computing, including a lack of technical knowledge and expertise, security concerns, and concerns around the reliability and availability of cloud computing services. Despite these challenges, many SMBs have successfully implemented cloud computing and have seen significant improvements in their operations. Case studies have shown that SMBs that migrated to cloud-based services experienced a reduction in IT costs and an increase in revenue. Additionally, SMBs that adopted cloud computing services saw a reduction in downtime and improved disaster recovery capabilities. The impact of cloud computing on SMBs extends beyond operational improvements. Cloud computing has also improved the competitiveness and profitability of SMBs, enabling them to access enterprise-grade technology at an affordable cost and allowing them to scale their operations quickly and efficiently. Cloud computing has also enabled SMBs to compete with larger businesses by providing them with the same technological capabilities. Cloud computing has had a significant impact on SMBs by providing them with cost-effective access to enterprise-grade technology, improving operational efficiency, and enhancing competitiveness and profitability. While several challenges prevent SMBs from adopting cloud computing, successful implementation can bring significant benefits to their operations',\n",
       "  'authors': ['Bismah Killedar',\n",
       "   'Maaz Datey',\n",
       "   'Maaz Datey',\n",
       "   'Maaz Datey',\n",
       "   'P Mell',\n",
       "   'T Grance',\n",
       "   'Michael Armbrust',\n",
       "   'Armando Fox',\n",
       "   'Rean Griffith',\n",
       "   'Anthony Joseph',\n",
       "   'Randy Katz',\n",
       "   'Andy Konwinski',\n",
       "   'Gunho Lee',\n",
       "   'David Patterson',\n",
       "   'Ariel Rabkin',\n",
       "   'Ion Stoica',\n",
       "   'Matei Zaharia',\n",
       "   'Rajkumar Buyya',\n",
       "   'Chee Yeo',\n",
       "   'Srikumar Venugopal',\n",
       "   'James Broberg',\n",
       "   'Ivona Brandic',\n",
       "   'Ivona Brandic',\n",
       "   'Virginia Braun',\n",
       "   'Victoria Clarke',\n",
       "   'Victoria Clarke',\n",
       "   'Jeba Sonia',\n",
       "   'D Joel Devadass',\n",
       "   'Dr Daniel',\n",
       "   'Dr Sabin Begum',\n",
       "   'Dr Sabin Begum',\n",
       "   'Dr Sabin Begum',\n",
       "   'K Pathan',\n",
       "   'Dr Veera Talukdar',\n",
       "   'Vivek Dadasaheb',\n",
       "   'Vivek Dadasaheb',\n",
       "   'J Creswell',\n",
       "   'K Laudon',\n",
       "   'J Laudon',\n",
       "   'J Lin',\n",
       "   'T Wu']},\n",
       " 'Sketch Based Image Retrieval for Architecture': {'abstract': 'Sketch-based image retrieval (SBIR) is an image retrieval task that takes a sketch as input and outputs colour images matching the sketch. Most recent SBIR methods utilise deep learning methods with complicated network designs, which are resource-intensive for practical use. This paper proposes a novel compact framework that takes the siamese network with image view angle information, targeting the SBIR task for architecture images. In particular, the proposed siamese network engages a compact SwinTiny transformer as the backbone encoder. View angle information of the architecture image is fed to the model to further improve search accuracy. To cope with the insufficient sketches issue, simulated building sketches are used in training, which are generated by a pre-trained edge extractor. Experiments show that our model achieves 0.859 top-one accuracy exceeding many baseline models for an architecture retrieval task.',\n",
       "  'authors': ['Yuxin Xu',\n",
       "   'Yuyao Yan',\n",
       "   'Yiming Lin',\n",
       "   'Xi Yang',\n",
       "   'Kaizhu Huang',\n",
       "   'M Eitz',\n",
       "   'K Hildebrand',\n",
       "   'T Boubekeur',\n",
       "   'M Alexa',\n",
       "   'C Hoi',\n",
       "   'C H Chan',\n",
       "   'K Huang',\n",
       "   'M Lyu',\n",
       "   'I King',\n",
       "   'Francisco García Triviño',\n",
       "   'K Huang',\n",
       "   'A Hussain',\n",
       "   'Q Wang',\n",
       "   'R Zhang',\n",
       "   'Filip Radenović',\n",
       "   'Giorgos Tolias',\n",
       "   'Ondřej Chum',\n",
       "   'Sounak Dey',\n",
       "   'Pau Riba',\n",
       "   'Anjan Dutta',\n",
       "   'Josep Llados',\n",
       "   'Yi-Zhe Song',\n",
       "   'Elad Hoffer',\n",
       "   'Nir Ailon',\n",
       "   'A Vaswani',\n",
       "   'N Shazeer',\n",
       "   'N Parmar',\n",
       "   'J Uszkoreit',\n",
       "   'L Jones',\n",
       "   'A Gomez',\n",
       "   'L Kaiser',\n",
       "   'I ; Polosukhin',\n",
       "   'S Bengio',\n",
       "   'H Wallach',\n",
       "   'R Fergus',\n",
       "   'S Vishwanathan',\n",
       "   'R Garnett',\n",
       "   'A Dosovitskiy',\n",
       "   'L Beyer',\n",
       "   'A Kolesnikov',\n",
       "   'D Weissenborn',\n",
       "   'X Zhai',\n",
       "   'T Unterthiner',\n",
       "   'M Dehghani',\n",
       "   'M Minderer',\n",
       "   'G Heigold',\n",
       "   'S Gelly',\n",
       "   'J Uszkoreit',\n",
       "   'N Houlsby',\n",
       "   'Ze Liu',\n",
       "   'Yutong Lin',\n",
       "   'Yue Cao',\n",
       "   'Han Hu',\n",
       "   'Yixuan Wei',\n",
       "   'Zheng Zhang',\n",
       "   'Stephen Lin',\n",
       "   'Baining Guo',\n",
       "   'Xavier Soria',\n",
       "   'Edgar Riba',\n",
       "   'Angel Sappa',\n",
       "   'Kaiming He',\n",
       "   'Xiangyu Zhang',\n",
       "   'Shaoqing Ren',\n",
       "   'Jian Sun']},\n",
       " 'tft': {'abstract': \"Multi-horizon forecasting often contains a complex mix of inputs -including static (i.e. time-invariant) covariates, known future inputs, and other exogenous time series that are only observed in the past -without any prior information on how they interact with the target. Several deep learning methods have been proposed, but they are typically 'black-box' models that do not shed light on how they use the full range of inputs present in practical scenarios. In this paper, we introduce the Temporal Fusion Transformer (TFT) -a novel attention-based architecture that combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. To learn temporal relationships at different scales, TFT uses recurrent layers for local processing and interpretable self-attention layers for long-term dependencies. TFT utilizes specialized components to select relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of scenarios. On a variety of real-world datasets, we demonstrate significant performance improvements over existing benchmarks, and highlight three practical interpretability use cases of TFT.\",\n",
       "  'authors': ['Bryan Lim',\n",
       "   'Sercan Arık',\n",
       "   'Nicolas Loeff',\n",
       "   'Tomas Pfister',\n",
       "   'Ahmed Alaa',\n",
       "   'Thomas Bolton',\n",
       "   'Emanuele Di Angelantonio',\n",
       "   'James Rudd',\n",
       "   'Mihaela Van Der Schaar',\n",
       "   'Andrew Ang',\n",
       "   'Allan Timmermann',\n",
       "   'Sercan Arik',\n",
       "   'Tomas Pfister',\n",
       "   'Scott Baker',\n",
       "   'Nicholas Bloom',\n",
       "   'Steven Davis',\n",
       "   'Kyle Kost',\n",
       "   'Marco Sammon',\n",
       "   'Tasaneeya Viratyosin',\n",
       "   'B Baltagi',\n",
       "   'Joos-Hendrik Böse',\n",
       "   'Valentin Flunkert',\n",
       "   'Jan Gasthaus',\n",
       "   'Tim Januschowski',\n",
       "   'Dustin Lange',\n",
       "   'David Salinas',\n",
       "   'Sebastian Schelter',\n",
       "   'Matthias Seeger',\n",
       "   'Yuyang Wang',\n",
       "   'Carlos Capistrán',\n",
       "   'Christian Constandse',\n",
       "   'Manuel Ramos-Francia',\n",
       "   'E Choi',\n",
       "   'D.-A Clevert',\n",
       "   'T Unterthiner',\n",
       "   'S Hochreiter',\n",
       "   'D Comaniciu',\n",
       "   'V Ramesh',\n",
       "   'P Meer',\n",
       "   'Pascal Courty',\n",
       "   'Hao Li',\n",
       "   'Josue Cox',\n",
       "   'Daniel Greenwald',\n",
       "   'Sydney Ludvigson',\n",
       "   'Y Dauphin',\n",
       "   'A Fan',\n",
       "   'M Auli',\n",
       "   'D Grangier',\n",
       "   'Sizhen Du',\n",
       "   'Guojie Song',\n",
       "   'Lei Han',\n",
       "   'Haikun Hong',\n",
       "   'Chenyou Fan',\n",
       "   'Yuze Zhang',\n",
       "   'Yi Pan',\n",
       "   'Xiaoyue Li',\n",
       "   'Chi Zhang',\n",
       "   'Rong Yuan',\n",
       "   'Di Wu',\n",
       "   'Wensheng Wang',\n",
       "   'Jian Pei',\n",
       "   'Heng Huang',\n",
       "   'C Favorita',\n",
       "   'Y Gal',\n",
       "   'Z Ghahramani',\n",
       "   'Eleftherios Giovanis',\n",
       "   'T Guo',\n",
       "   'T Lin',\n",
       "   'N Antulov-Fantulin',\n",
       "   'G Heber',\n",
       "   'A Lunde',\n",
       "   'N Shephard',\n",
       "   'K Sheppard',\n",
       "   'Sepp Hochreiter',\n",
       "   'Jürgen Schmidhuber',\n",
       "   'S Hylleberg',\n",
       "   'T Kailath',\n",
       "   'Jan Koutník',\n",
       "   'Juergen Schmidhuber',\n",
       "   'Faustino Gomez',\n",
       "   'J Lei Ba',\n",
       "   'J Kiros',\n",
       "   'G Hinton',\n",
       "   'S Li',\n",
       "   'B Lim',\n",
       "   'A Alaa',\n",
       "   'M Van Der Schaar',\n",
       "   'S Lundberg',\n",
       "   'S.-I Lee',\n",
       "   'Spyros Makridakis',\n",
       "   'Evangelos Spiliotis',\n",
       "   'Vassilios Assimakopoulos',\n",
       "   'Massimiliano Marcellino',\n",
       "   'James Stock',\n",
       "   'Mark Watson',\n",
       "   'D Neil',\n",
       "   'S Rangapuram',\n",
       "   'Marco Ribeiro',\n",
       "   'Sameer Singh',\n",
       "   'Carlos Guestrin',\n",
       "   'David Salinas',\n",
       "   'Valentin Flunkert',\n",
       "   'Jan Gasthaus',\n",
       "   'Tim Januschowski',\n",
       "   'Huan Song',\n",
       "   'Deepta Rajan',\n",
       "   'Jayaraman Thiagarajan',\n",
       "   'Andreas Spanias',\n",
       "   'S Taieb',\n",
       "   'A Sorjamaa',\n",
       "   'G Bontempi',\n",
       "   'A Vaswani',\n",
       "   'N Shazeer',\n",
       "   'N Parmar',\n",
       "   'J Uszkoreit',\n",
       "   'L Jones',\n",
       "   'A Gomez',\n",
       "   'Fei Wang',\n",
       "   'Mengqing Jiang',\n",
       "   'Chen Qian',\n",
       "   'Shuo Yang',\n",
       "   'Cheng Li',\n",
       "   'Honggang Zhang',\n",
       "   'Xiaogang Wang',\n",
       "   'Xiaoou Tang',\n",
       "   'Y Wang',\n",
       "   'R Wen',\n",
       "   'J Yoon',\n",
       "   'S Arik',\n",
       "   'T Pfister',\n",
       "   'H.-F Yu',\n",
       "   'N Rao',\n",
       "   'I Dhillon',\n",
       "   'J Zhang',\n",
       "   'K Nawata']},\n",
       " 'VISION TRANSFORMERS NEED REGISTERS': {'abstract': 'Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.',\n",
       "  'authors': ['Timothée Darcet',\n",
       "   'Maxime Oquab',\n",
       "   'Julien Mairal',\n",
       "   'Piotr Bojanowski',\n",
       "   'Meta Fair',\n",
       "   'Hangbo Bao',\n",
       "   'Li Dong',\n",
       "   'Furu Wei',\n",
       "   'Aydar Bulatov',\n",
       "   'Yuri Kuratov',\n",
       "   'Yermek Kapushev',\n",
       "   'Mikhail Burtsev',\n",
       "   'Yuri Mikhail S Burtsev',\n",
       "   'Anton Kuratov',\n",
       "   'Grigory Peganov',\n",
       "   'Grigory Peganov',\n",
       "   'Nicolas Carion',\n",
       "   'Francisco Massa',\n",
       "   'Gabriel Synnaeve',\n",
       "   'Nicolas Usunier',\n",
       "   'Alexander Kirillov',\n",
       "   'Sergey Zagoruyko',\n",
       "   'Mathilde Caron',\n",
       "   'Hugo Touvron',\n",
       "   'Ishan Misra',\n",
       "   'Herve Jegou',\n",
       "   'Julien Mairal',\n",
       "   'Piotr Bojanowski',\n",
       "   'Armand Joulin',\n",
       "   'Jacob Devlin',\n",
       "   'Ming-Wei Chang',\n",
       "   'Kenton Lee',\n",
       "   'Kristina Toutanova',\n",
       "   'Carl Doersch',\n",
       "   'Abhinav Gupta',\n",
       "   'Alexei Efros',\n",
       "   'Alexey Dosovitskiy',\n",
       "   'Lucas Beyer',\n",
       "   'Alexander Kolesnikov',\n",
       "   'Dirk Weissenborn',\n",
       "   'Xiaohua Zhai',\n",
       "   'Thomas Unterthiner',\n",
       "   'Mostafa Dehghani',\n",
       "   'Matthias Minderer',\n",
       "   'Georg Heigold',\n",
       "   'Sylvain Gelly',\n",
       "   'Yuxin Fang',\n",
       "   'Bencheng Liao',\n",
       "   'Xinggang Wang',\n",
       "   'Jiemin Fang',\n",
       "   'Jiyang Qi',\n",
       "   'Rui Wu',\n",
       "   'Jianwei Niu',\n",
       "   'Wenyu Liu',\n",
       "   'Kaiming He',\n",
       "   'Haoqi Fan',\n",
       "   'Yuxin Wu',\n",
       "   'Saining Xie',\n",
       "   'Ross Girshick',\n",
       "   'Kaiming He',\n",
       "   'Xinlei Chen',\n",
       "   'Saining Xie',\n",
       "   'Yanghao Li',\n",
       "   'Piotr Dollar',\n",
       "   'Ross Girshick',\n",
       "   'Gabriel Ilharco',\n",
       "   'Mitchell Wortsman',\n",
       "   'Ross Wightman',\n",
       "   'Cade Gordon',\n",
       "   'Nicholas Carlini',\n",
       "   'Rohan Taori',\n",
       "   'Achal Dave',\n",
       "   'Vaishaal Shankar',\n",
       "   'Hongseok Namkoong',\n",
       "   'John Miller',\n",
       "   'Hannaneh Hajishirzi',\n",
       "   'Ali Farhadi',\n",
       "   'Ludwig Schmidt',\n",
       "   'Ludwig Schmidt',\n",
       "   'Andrew Jaegle',\n",
       "   'Felix Gimeno',\n",
       "   'Andy Brock',\n",
       "   'Oriol Vinyals',\n",
       "   'Andrew Zisserman',\n",
       "   'Joao Carreira',\n",
       "   'Andrew Jaegle',\n",
       "   'Sebastian Borgeaud',\n",
       "   'Jean-Baptiste Alayrac',\n",
       "   'Carl Doersch',\n",
       "   'Catalin Ionescu',\n",
       "   'David Ding',\n",
       "   'Skanda Koppula',\n",
       "   'Andrew Brock',\n",
       "   'Evan Shelhamer',\n",
       "   'J Olivier',\n",
       "   \"Matthew H'enaff\",\n",
       "   'Andrew Botvinick',\n",
       "   'Oriol Zisserman',\n",
       "   'João Vinyals',\n",
       "   'João Vinyals',\n",
       "   'Alexander Kirillov',\n",
       "   'Eric Mintun',\n",
       "   'Nikhila Ravi',\n",
       "   'Hanzi Mao',\n",
       "   'Chloe Rolland',\n",
       "   'Laura Gustafson',\n",
       "   'Tete Xiao',\n",
       "   'Spencer Whitehead',\n",
       "   'Alexander Berg',\n",
       "   'Wan-Yen Lo',\n",
       "   'Piotr Dollár',\n",
       "   'Ross Girshick',\n",
       "   'Alex Krizhevsky',\n",
       "   'Ilya Sutskever',\n",
       "   'Geoffrey Hinton',\n",
       "   'Francesco Locatello',\n",
       "   'Dirk Weissenborn',\n",
       "   'Thomas Unterthiner',\n",
       "   'Aravindh Mahendran',\n",
       "   'Georg Heigold',\n",
       "   'Jakob Uszkoreit',\n",
       "   'Alexey Dosovitskiy',\n",
       "   'Thomas Kipf',\n",
       "   'Thomas Kipf',\n",
       "   'Maxime Oquab',\n",
       "   'Timothée Darcet',\n",
       "   'Théo Moutakanni',\n",
       "   'Huy Vo',\n",
       "   'Marc Szafraniec',\n",
       "   'Vasil Khalidov',\n",
       "   'Pierre Fernandez',\n",
       "   'Daniel Haziza',\n",
       "   'Francisco Massa',\n",
       "   'Alaaeldin El-Nouby',\n",
       "   'Alec Radford',\n",
       "   'Jong Kim',\n",
       "   'Chris Hallacy',\n",
       "   'Aditya Ramesh',\n",
       "   'Gabriel Goh',\n",
       "   'Sandhini Agarwal',\n",
       "   'Girish Sastry',\n",
       "   'Amanda Askell',\n",
       "   'Pamela Mishkin',\n",
       "   'Jack Clark',\n",
       "   'Olga Russakovsky',\n",
       "   'Jia Deng',\n",
       "   'Hao Su',\n",
       "   'Jonathan Krause',\n",
       "   'Sanjeev Satheesh',\n",
       "   'Sean Ma',\n",
       "   'Zhiheng Huang',\n",
       "   'Andrej Karpathy',\n",
       "   'Aditya Khosla',\n",
       "   'Michael Bernstein',\n",
       "   'Alexander Berg',\n",
       "   'Li Fei-Fei',\n",
       "   'Mark Sandler',\n",
       "   'Andrey Zhmoginov',\n",
       "   'Max Vladymyrov',\n",
       "   'Andrew Jackson',\n",
       "   'Oriane Siméoni',\n",
       "   'Gilles Puy',\n",
       "   'V Huy',\n",
       "   'Simon Vo',\n",
       "   'Spyros Roburin',\n",
       "   'Andrei Gidaris',\n",
       "   'Patrick Bursuc',\n",
       "   'Renaud Pérez',\n",
       "   'Jean Marlet',\n",
       "   'Jean Marlet',\n",
       "   'Hwanjun Song',\n",
       "   'Deqing Sun',\n",
       "   'Sanghyuk Chun',\n",
       "   'Varun Jampani',\n",
       "   'Dongyoon Han',\n",
       "   'Byeongho Heo',\n",
       "   'Wonjae Kim',\n",
       "   'Ming-Hsuan Yang',\n",
       "   'Antonio Torralba',\n",
       "   'Alexei Efros',\n",
       "   'Hugo Touvron',\n",
       "   'Matthieu Cord',\n",
       "   'Hervé Jégou',\n",
       "   'Xudong Wang',\n",
       "   'Rohit Girdhar',\n",
       "   'Stella Yu',\n",
       "   'Ishan Misra',\n",
       "   'Fuzhao Xue',\n",
       "   'Valerii Likhosherstov',\n",
       "   'Anurag Arnab',\n",
       "   'Neil Houlsby',\n",
       "   'Mostafa Dehghani',\n",
       "   'Yang You',\n",
       "   'Sixiao Zheng',\n",
       "   'Jiachen Lu',\n",
       "   'Hengshuang Zhao',\n",
       "   'Xiatian Zhu',\n",
       "   'Zekun Luo',\n",
       "   'Yabiao Wang',\n",
       "   'Yanwei Fu',\n",
       "   'Jianfeng Feng',\n",
       "   'Tao Xiang',\n",
       "   'Philip Torr',\n",
       "   'Li Zhang',\n",
       "   'Jinghao Zhou',\n",
       "   'Chen Wei',\n",
       "   'Huiyu Wang',\n",
       "   'Wei Shen',\n",
       "   'Cihang Xie',\n",
       "   'Alan Yuille',\n",
       "   'Tao Kong']}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMILARITY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La similitud entre el documento 11621ijccsa02 y el documento 1709.01907 es sim 0.022268354892730713\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 1802.05799 es sim 0.018618298694491386\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 2007.03051 es sim 0.0440535768866539\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 208 es sim 0.045304201543331146\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.21600942313671112\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 6114nsa03 es sim 0.25234338641166687\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 9-12 es sim 0.051240257918834686\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.01917664147913456\n",
      "La similitud entre el documento 11621ijccsa02 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.010931157507002354\n",
      "La similitud entre el documento 11621ijccsa02 y el documento hir-22-351 es sim 0.02943280339241028\n",
      "La similitud entre el documento 11621ijccsa02 y el documento How good are deep models in understanding generated images es sim 0.005171922966837883\n",
      "La similitud entre el documento 11621ijccsa02 y el documento IJISRT23AUG773 es sim 0.21243876218795776\n",
      "La similitud entre el documento 11621ijccsa02 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.003396933665499091\n",
      "La similitud entre el documento 11621ijccsa02 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.0035566070582717657\n",
      "La similitud entre el documento 11621ijccsa02 y el documento Paper11879 es sim 0.18387076258659363\n",
      "La similitud entre el documento 11621ijccsa02 y el documento Sketch Based Image Retrieval for Architecture es sim 0.008616222068667412\n",
      "La similitud entre el documento 11621ijccsa02 y el documento tft es sim 0.014443309977650642\n",
      "La similitud entre el documento 11621ijccsa02 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.014934487640857697\n",
      "La similitud entre el documento 1709.01907 y el documento 1802.05799 es sim 0.006358814425766468\n",
      "La similitud entre el documento 1709.01907 y el documento 2007.03051 es sim 0.020259998738765717\n",
      "La similitud entre el documento 1709.01907 y el documento 208 es sim 0.009491898119449615\n",
      "La similitud entre el documento 1709.01907 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.016023611649870872\n",
      "La similitud entre el documento 1709.01907 y el documento 6114nsa03 es sim 0.018005508929491043\n",
      "La similitud entre el documento 1709.01907 y el documento 9-12 es sim 0.010601626709103584\n",
      "La similitud entre el documento 1709.01907 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.002904281485825777\n",
      "La similitud entre el documento 1709.01907 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.04665391147136688\n",
      "La similitud entre el documento 1709.01907 y el documento hir-22-351 es sim 0.0045777251943945885\n",
      "La similitud entre el documento 1709.01907 y el documento How good are deep models in understanding generated images es sim 0.013204552233219147\n",
      "La similitud entre el documento 1709.01907 y el documento IJISRT23AUG773 es sim 0.024176495149731636\n",
      "La similitud entre el documento 1709.01907 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.011989818885922432\n",
      "La similitud entre el documento 1709.01907 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.01836450770497322\n",
      "La similitud entre el documento 1709.01907 y el documento Paper11879 es sim 0.013347569853067398\n",
      "La similitud entre el documento 1709.01907 y el documento Sketch Based Image Retrieval for Architecture es sim 0.03254225105047226\n",
      "La similitud entre el documento 1709.01907 y el documento tft es sim 0.09431024640798569\n",
      "La similitud entre el documento 1709.01907 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.040033720433712006\n",
      "La similitud entre el documento 1802.05799 y el documento 2007.03051 es sim 0.10709698498249054\n",
      "La similitud entre el documento 1802.05799 y el documento 208 es sim 0.019403114914894104\n",
      "La similitud entre el documento 1802.05799 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.010583772324025631\n",
      "La similitud entre el documento 1802.05799 y el documento 6114nsa03 es sim 0.00391280697658658\n",
      "La similitud entre el documento 1802.05799 y el documento 9-12 es sim 0.026848789304494858\n",
      "La similitud entre el documento 1802.05799 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.004009630531072617\n",
      "La similitud entre el documento 1802.05799 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.03458882123231888\n",
      "La similitud entre el documento 1802.05799 y el documento hir-22-351 es sim 0.02041728049516678\n",
      "La similitud entre el documento 1802.05799 y el documento How good are deep models in understanding generated images es sim 0.02031589113175869\n",
      "La similitud entre el documento 1802.05799 y el documento IJISRT23AUG773 es sim 0.003041214542463422\n",
      "La similitud entre el documento 1802.05799 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.006063825450837612\n",
      "La similitud entre el documento 1802.05799 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.03859197348356247\n",
      "La similitud entre el documento 1802.05799 y el documento Paper11879 es sim 0.020491210743784904\n",
      "La similitud entre el documento 1802.05799 y el documento Sketch Based Image Retrieval for Architecture es sim 0.009295986965298653\n",
      "La similitud entre el documento 1802.05799 y el documento tft es sim 0.015483912080526352\n",
      "La similitud entre el documento 1802.05799 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.005256659351289272\n",
      "La similitud entre el documento 2007.03051 y el documento 208 es sim 0.025235813111066818\n",
      "La similitud entre el documento 2007.03051 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.05446472018957138\n",
      "La similitud entre el documento 2007.03051 y el documento 6114nsa03 es sim 0.03806239366531372\n",
      "La similitud entre el documento 2007.03051 y el documento 9-12 es sim 0.041584767401218414\n",
      "La similitud entre el documento 2007.03051 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.021960606798529625\n",
      "La similitud entre el documento 2007.03051 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.03320314735174179\n",
      "La similitud entre el documento 2007.03051 y el documento hir-22-351 es sim 0.019039882346987724\n",
      "La similitud entre el documento 2007.03051 y el documento How good are deep models in understanding generated images es sim 0.022595901042222977\n",
      "La similitud entre el documento 2007.03051 y el documento IJISRT23AUG773 es sim 0.046047333627939224\n",
      "La similitud entre el documento 2007.03051 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.02150655724108219\n",
      "La similitud entre el documento 2007.03051 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.06082051992416382\n",
      "La similitud entre el documento 2007.03051 y el documento Paper11879 es sim 0.040660277009010315\n",
      "La similitud entre el documento 2007.03051 y el documento Sketch Based Image Retrieval for Architecture es sim 0.01366021204739809\n",
      "La similitud entre el documento 2007.03051 y el documento tft es sim 0.05994286388158798\n",
      "La similitud entre el documento 2007.03051 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.03142338618636131\n",
      "La similitud entre el documento 208 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.004075708333402872\n",
      "La similitud entre el documento 208 y el documento 6114nsa03 es sim 0.042983971536159515\n",
      "La similitud entre el documento 208 y el documento 9-12 es sim 0.2930697202682495\n",
      "La similitud entre el documento 208 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.1955643594264984\n",
      "La similitud entre el documento 208 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.01326720044016838\n",
      "La similitud entre el documento 208 y el documento hir-22-351 es sim 0.07869458943605423\n",
      "La similitud entre el documento 208 y el documento How good are deep models in understanding generated images es sim 0.03287221118807793\n",
      "La similitud entre el documento 208 y el documento IJISRT23AUG773 es sim 0.01904100365936756\n",
      "La similitud entre el documento 208 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.003938582725822926\n",
      "La similitud entre el documento 208 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.02365797758102417\n",
      "La similitud entre el documento 208 y el documento Paper11879 es sim 0.04804679751396179\n",
      "La similitud entre el documento 208 y el documento Sketch Based Image Retrieval for Architecture es sim 0.006295825354754925\n",
      "La similitud entre el documento 208 y el documento tft es sim 0.03473251685500145\n",
      "La similitud entre el documento 208 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.0049843257293105125\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento 6114nsa03 es sim 0.23569341003894806\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento 9-12 es sim 0.007512201555073261\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento 907-Article Text-2692-1-10-20230720 es sim 0.03190731629729271\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.034231044352054596\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento hir-22-351 es sim 0.010767989791929722\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento How good are deep models in understanding generated images es sim 0.020192952826619148\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento IJISRT23AUG773 es sim 0.18524028360843658\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.03307023644447327\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.01866886019706726\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento Paper11879 es sim 0.18385788798332214\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento Sketch Based Image Retrieval for Architecture es sim 0.020073752850294113\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento tft es sim 0.014960000291466713\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.010987944900989532\n",
      "La similitud entre el documento 6114nsa03 y el documento 9-12 es sim 0.03651692718267441\n",
      "La similitud entre el documento 6114nsa03 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.010418219491839409\n",
      "La similitud entre el documento 6114nsa03 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.026069127023220062\n",
      "La similitud entre el documento 6114nsa03 y el documento hir-22-351 es sim 0.01868584007024765\n",
      "La similitud entre el documento 6114nsa03 y el documento How good are deep models in understanding generated images es sim 0.009218871593475342\n",
      "La similitud entre el documento 6114nsa03 y el documento IJISRT23AUG773 es sim 0.2305990755558014\n",
      "La similitud entre el documento 6114nsa03 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.014171261340379715\n",
      "La similitud entre el documento 6114nsa03 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.038991015404462814\n",
      "La similitud entre el documento 6114nsa03 y el documento Paper11879 es sim 0.19125492870807648\n",
      "La similitud entre el documento 6114nsa03 y el documento Sketch Based Image Retrieval for Architecture es sim 0.021959418430924416\n",
      "La similitud entre el documento 6114nsa03 y el documento tft es sim 0.018405554816126823\n",
      "La similitud entre el documento 6114nsa03 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.020757999271154404\n",
      "La similitud entre el documento 9-12 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.17416615784168243\n",
      "La similitud entre el documento 9-12 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.01916787400841713\n",
      "La similitud entre el documento 9-12 y el documento hir-22-351 es sim 0.03820112347602844\n",
      "La similitud entre el documento 9-12 y el documento How good are deep models in understanding generated images es sim 0.01600266620516777\n",
      "La similitud entre el documento 9-12 y el documento IJISRT23AUG773 es sim 0.07064438611268997\n",
      "La similitud entre el documento 9-12 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.003781010629609227\n",
      "La similitud entre el documento 9-12 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.010812017135322094\n",
      "La similitud entre el documento 9-12 y el documento Paper11879 es sim 0.02174542471766472\n",
      "La similitud entre el documento 9-12 y el documento Sketch Based Image Retrieval for Architecture es sim 0.013656055554747581\n",
      "La similitud entre el documento 9-12 y el documento tft es sim 0.006192849949002266\n",
      "La similitud entre el documento 9-12 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.011944621801376343\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.0037038419395685196\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento hir-22-351 es sim 0.03148350864648819\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento How good are deep models in understanding generated images es sim 0.013025185093283653\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento IJISRT23AUG773 es sim 0.02443832717835903\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.0004679235862568021\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.007593429647386074\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento Paper11879 es sim 0.0154190082103014\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento Sketch Based Image Retrieval for Architecture es sim 0.003859858959913254\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento tft es sim 0.00550801632925868\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.0032684968318790197\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento hir-22-351 es sim 0.023405812680721283\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento How good are deep models in understanding generated images es sim 0.07100421190261841\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento IJISRT23AUG773 es sim 0.01482588704675436\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.03696475178003311\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.11737371981143951\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento Paper11879 es sim 0.01523938961327076\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento Sketch Based Image Retrieval for Architecture es sim 0.10228697955608368\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento tft es sim 0.05087042599916458\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.02162054181098938\n",
      "La similitud entre el documento hir-22-351 y el documento How good are deep models in understanding generated images es sim 0.03646470978856087\n",
      "La similitud entre el documento hir-22-351 y el documento IJISRT23AUG773 es sim 0.009465666487812996\n",
      "La similitud entre el documento hir-22-351 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.007337093353271484\n",
      "La similitud entre el documento hir-22-351 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.00865382980555296\n",
      "La similitud entre el documento hir-22-351 y el documento Paper11879 es sim 0.027707340195775032\n",
      "La similitud entre el documento hir-22-351 y el documento Sketch Based Image Retrieval for Architecture es sim 0.02287200465798378\n",
      "La similitud entre el documento hir-22-351 y el documento tft es sim 0.0570279024541378\n",
      "La similitud entre el documento hir-22-351 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.01187829952687025\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento IJISRT23AUG773 es sim 0.009096469730138779\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.03595055267214775\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.03399570286273956\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento Paper11879 es sim 0.010394366458058357\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento Sketch Based Image Retrieval for Architecture es sim 0.06761106848716736\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento tft es sim 0.025596335530281067\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.05745764821767807\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.014719258062541485\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.0217756237834692\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento Paper11879 es sim 0.17953866720199585\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento Sketch Based Image Retrieval for Architecture es sim 0.02510959655046463\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento tft es sim 0.01855628192424774\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.016351375728845596\n",
      "La similitud entre el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.018322788178920746\n",
      "La similitud entre el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH y el documento Paper11879 es sim 0.005842684768140316\n",
      "La similitud entre el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH y el documento Sketch Based Image Retrieval for Architecture es sim 0.061263177543878555\n",
      "La similitud entre el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH y el documento tft es sim 0.020919833332300186\n",
      "La similitud entre el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.020112192258238792\n",
      "La similitud entre el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL y el documento Paper11879 es sim 0.03254666551947594\n",
      "La similitud entre el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL y el documento Sketch Based Image Retrieval for Architecture es sim 0.12098342180252075\n",
      "La similitud entre el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL y el documento tft es sim 0.09914323687553406\n",
      "La similitud entre el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.03338383138179779\n",
      "La similitud entre el documento Paper11879 y el documento Sketch Based Image Retrieval for Architecture es sim 0.010553169995546341\n",
      "La similitud entre el documento Paper11879 y el documento tft es sim 0.015866180881857872\n",
      "La similitud entre el documento Paper11879 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.013481883332133293\n",
      "La similitud entre el documento Sketch Based Image Retrieval for Architecture y el documento tft es sim 0.0406905896961689\n",
      "La similitud entre el documento Sketch Based Image Retrieval for Architecture y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.02430581860244274\n",
      "La similitud entre el documento tft y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.016678886488080025\n"
     ]
    }
   ],
   "source": [
    "textos = [resumen.split() for resumen in papers.values()]\n",
    "\n",
    "diccionario = corpora.Dictionary(textos)\n",
    "\n",
    "corpus = [diccionario.doc2bow(texto) for texto in textos]\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "index = similarities.MatrixSimilarity(tfidf[corpus])\n",
    "\n",
    "for i in range(len(textos)):\n",
    "    for j in range(i + 1, len(textos)):\n",
    "        vec_i = diccionario.doc2bow(textos[i])\n",
    "        vec_j = diccionario.doc2bow(textos[j])\n",
    "        sim_ij = index[tfidf[vec_i]][j]\n",
    "        print(\n",
    "            f\"La similitud entre el documento {list(papers.keys())[i]} y el documento {list(papers.keys())[j]} es sim {sim_ij}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicov\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La similitud entre el documento 11621ijccsa02 y el documento 1709.01907 es sim 0.18909263610839844\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 1802.05799 es sim 0.32246485352516174\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 2007.03051 es sim 0.38316184282302856\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 208 es sim 0.37224164605140686\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.6741066575050354\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 6114nsa03 es sim 0.7781059741973877\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 9-12 es sim 0.4558578133583069\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.33682113885879517\n",
      "La similitud entre el documento 11621ijccsa02 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.12774041295051575\n",
      "La similitud entre el documento 11621ijccsa02 y el documento hir-22-351 es sim 0.3331562876701355\n",
      "La similitud entre el documento 11621ijccsa02 y el documento How good are deep models in understanding generated images es sim 0.15266136825084686\n",
      "La similitud entre el documento 11621ijccsa02 y el documento IJISRT23AUG773 es sim 0.7527927160263062\n",
      "La similitud entre el documento 11621ijccsa02 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.1456475555896759\n",
      "La similitud entre el documento 11621ijccsa02 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.18402864038944244\n",
      "La similitud entre el documento 11621ijccsa02 y el documento Paper11879 es sim 0.6483190059661865\n",
      "La similitud entre el documento 11621ijccsa02 y el documento Sketch Based Image Retrieval for Architecture es sim 0.10967686772346497\n",
      "La similitud entre el documento 11621ijccsa02 y el documento tft es sim 0.28413596749305725\n",
      "La similitud entre el documento 11621ijccsa02 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.06570909917354584\n",
      "La similitud entre el documento 1709.01907 y el documento 1802.05799 es sim 0.2755492627620697\n",
      "La similitud entre el documento 1709.01907 y el documento 2007.03051 es sim 0.3994024693965912\n",
      "La similitud entre el documento 1709.01907 y el documento 208 es sim 0.37621599435806274\n",
      "La similitud entre el documento 1709.01907 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.24646547436714172\n",
      "La similitud entre el documento 1709.01907 y el documento 6114nsa03 es sim 0.23073741793632507\n",
      "La similitud entre el documento 1709.01907 y el documento 9-12 es sim 0.33437538146972656\n",
      "La similitud entre el documento 1709.01907 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.32342636585235596\n",
      "La similitud entre el documento 1709.01907 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.12864874303340912\n",
      "La similitud entre el documento 1709.01907 y el documento hir-22-351 es sim 0.216549351811409\n",
      "La similitud entre el documento 1709.01907 y el documento How good are deep models in understanding generated images es sim 0.3624846041202545\n",
      "La similitud entre el documento 1709.01907 y el documento IJISRT23AUG773 es sim 0.19442200660705566\n",
      "La similitud entre el documento 1709.01907 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.23184871673583984\n",
      "La similitud entre el documento 1709.01907 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.2294052243232727\n",
      "La similitud entre el documento 1709.01907 y el documento Paper11879 es sim 0.1346801519393921\n",
      "La similitud entre el documento 1709.01907 y el documento Sketch Based Image Retrieval for Architecture es sim 0.12338881939649582\n",
      "La similitud entre el documento 1709.01907 y el documento tft es sim 0.553356945514679\n",
      "La similitud entre el documento 1709.01907 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.17936937510967255\n",
      "La similitud entre el documento 1802.05799 y el documento 2007.03051 es sim 0.6693546772003174\n",
      "La similitud entre el documento 1802.05799 y el documento 208 es sim 0.2649080753326416\n",
      "La similitud entre el documento 1802.05799 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.3053826093673706\n",
      "La similitud entre el documento 1802.05799 y el documento 6114nsa03 es sim 0.3603825569152832\n",
      "La similitud entre el documento 1802.05799 y el documento 9-12 es sim 0.2970978915691376\n",
      "La similitud entre el documento 1802.05799 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.31215962767601013\n",
      "La similitud entre el documento 1802.05799 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.250278502702713\n",
      "La similitud entre el documento 1802.05799 y el documento hir-22-351 es sim 0.6281082630157471\n",
      "La similitud entre el documento 1802.05799 y el documento How good are deep models in understanding generated images es sim 0.4335462152957916\n",
      "La similitud entre el documento 1802.05799 y el documento IJISRT23AUG773 es sim 0.3815722167491913\n",
      "La similitud entre el documento 1802.05799 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.4376979172229767\n",
      "La similitud entre el documento 1802.05799 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.5399653315544128\n",
      "La similitud entre el documento 1802.05799 y el documento Paper11879 es sim 0.33296236395835876\n",
      "La similitud entre el documento 1802.05799 y el documento Sketch Based Image Retrieval for Architecture es sim 0.2827311158180237\n",
      "La similitud entre el documento 1802.05799 y el documento tft es sim 0.510079562664032\n",
      "La similitud entre el documento 1802.05799 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.33160972595214844\n",
      "La similitud entre el documento 2007.03051 y el documento 208 es sim 0.24511383473873138\n",
      "La similitud entre el documento 2007.03051 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.3801521956920624\n",
      "La similitud entre el documento 2007.03051 y el documento 6114nsa03 es sim 0.36494770646095276\n",
      "La similitud entre el documento 2007.03051 y el documento 9-12 es sim 0.2623075544834137\n",
      "La similitud entre el documento 2007.03051 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.2590760588645935\n",
      "La similitud entre el documento 2007.03051 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.3153303265571594\n",
      "La similitud entre el documento 2007.03051 y el documento hir-22-351 es sim 0.6317008137702942\n",
      "La similitud entre el documento 2007.03051 y el documento How good are deep models in understanding generated images es sim 0.41396549344062805\n",
      "La similitud entre el documento 2007.03051 y el documento IJISRT23AUG773 es sim 0.45828789472579956\n",
      "La similitud entre el documento 2007.03051 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.3403204381465912\n",
      "La similitud entre el documento 2007.03051 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.4715039134025574\n",
      "La similitud entre el documento 2007.03051 y el documento Paper11879 es sim 0.32300013303756714\n",
      "La similitud entre el documento 2007.03051 y el documento Sketch Based Image Retrieval for Architecture es sim 0.2688371539115906\n",
      "La similitud entre el documento 2007.03051 y el documento tft es sim 0.4855729341506958\n",
      "La similitud entre el documento 2007.03051 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.34031447768211365\n",
      "La similitud entre el documento 208 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.2677820920944214\n",
      "La similitud entre el documento 208 y el documento 6114nsa03 es sim 0.2853899300098419\n",
      "La similitud entre el documento 208 y el documento 9-12 es sim 0.8266420960426331\n",
      "La similitud entre el documento 208 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.7309472560882568\n",
      "La similitud entre el documento 208 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.17645123600959778\n",
      "La similitud entre el documento 208 y el documento hir-22-351 es sim 0.37329426407814026\n",
      "La similitud entre el documento 208 y el documento How good are deep models in understanding generated images es sim 0.35981225967407227\n",
      "La similitud entre el documento 208 y el documento IJISRT23AUG773 es sim 0.260785311460495\n",
      "La similitud entre el documento 208 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.302123486995697\n",
      "La similitud entre el documento 208 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.24082818627357483\n",
      "La similitud entre el documento 208 y el documento Paper11879 es sim 0.13024507462978363\n",
      "La similitud entre el documento 208 y el documento Sketch Based Image Retrieval for Architecture es sim 0.24326066672801971\n",
      "La similitud entre el documento 208 y el documento tft es sim 0.22323386371135712\n",
      "La similitud entre el documento 208 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.2501963675022125\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento 6114nsa03 es sim 0.7596637606620789\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento 9-12 es sim 0.2191670835018158\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento 907-Article Text-2692-1-10-20230720 es sim 0.2560056447982788\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.17794449627399445\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento hir-22-351 es sim 0.2832280397415161\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento How good are deep models in understanding generated images es sim 0.22979529201984406\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento IJISRT23AUG773 es sim 0.7465469241142273\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.18407735228538513\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.15734514594078064\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento Paper11879 es sim 0.6723699569702148\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento Sketch Based Image Retrieval for Architecture es sim 0.16976290941238403\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento tft es sim 0.2084035575389862\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.053897857666015625\n",
      "La similitud entre el documento 6114nsa03 y el documento 9-12 es sim 0.35044583678245544\n",
      "La similitud entre el documento 6114nsa03 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.27910086512565613\n",
      "La similitud entre el documento 6114nsa03 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.16635407507419586\n",
      "La similitud entre el documento 6114nsa03 y el documento hir-22-351 es sim 0.28499943017959595\n",
      "La similitud entre el documento 6114nsa03 y el documento How good are deep models in understanding generated images es sim 0.21973727643489838\n",
      "La similitud entre el documento 6114nsa03 y el documento IJISRT23AUG773 es sim 0.7518483400344849\n",
      "La similitud entre el documento 6114nsa03 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.21076659858226776\n",
      "La similitud entre el documento 6114nsa03 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.29329341650009155\n",
      "La similitud entre el documento 6114nsa03 y el documento Paper11879 es sim 0.6843827366828918\n",
      "La similitud entre el documento 6114nsa03 y el documento Sketch Based Image Retrieval for Architecture es sim 0.14922329783439636\n",
      "La similitud entre el documento 6114nsa03 y el documento tft es sim 0.3397369384765625\n",
      "La similitud entre el documento 6114nsa03 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.11993280053138733\n",
      "La similitud entre el documento 9-12 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.6839179992675781\n",
      "La similitud entre el documento 9-12 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.16600526869297028\n",
      "La similitud entre el documento 9-12 y el documento hir-22-351 es sim 0.321656197309494\n",
      "La similitud entre el documento 9-12 y el documento How good are deep models in understanding generated images es sim 0.3282428979873657\n",
      "La similitud entre el documento 9-12 y el documento IJISRT23AUG773 es sim 0.28672948479652405\n",
      "La similitud entre el documento 9-12 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.28060492873191833\n",
      "La similitud entre el documento 9-12 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.2955491244792938\n",
      "La similitud entre el documento 9-12 y el documento Paper11879 es sim 0.2357879877090454\n",
      "La similitud entre el documento 9-12 y el documento Sketch Based Image Retrieval for Architecture es sim 0.20081590116024017\n",
      "La similitud entre el documento 9-12 y el documento tft es sim 0.2606315314769745\n",
      "La similitud entre el documento 9-12 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.2612053155899048\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.21054378151893616\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento hir-22-351 es sim 0.3262462913990021\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento How good are deep models in understanding generated images es sim 0.39080843329429626\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento IJISRT23AUG773 es sim 0.3021806478500366\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.282321035861969\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.25177040696144104\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento Paper11879 es sim 0.21264180541038513\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento Sketch Based Image Retrieval for Architecture es sim 0.17785252630710602\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento tft es sim 0.17851263284683228\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.23986057937145233\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento hir-22-351 es sim 0.2841953933238983\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento How good are deep models in understanding generated images es sim 0.5150306820869446\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento IJISRT23AUG773 es sim 0.23573637008666992\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.3489418923854828\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.5181499123573303\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento Paper11879 es sim 0.218425914645195\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento Sketch Based Image Retrieval for Architecture es sim 0.7249884009361267\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento tft es sim 0.21481262147426605\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.3885040283203125\n",
      "La similitud entre el documento hir-22-351 y el documento How good are deep models in understanding generated images es sim 0.3902369737625122\n",
      "La similitud entre el documento hir-22-351 y el documento IJISRT23AUG773 es sim 0.34636732935905457\n",
      "La similitud entre el documento hir-22-351 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.2951207756996155\n",
      "La similitud entre el documento hir-22-351 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.4414534866809845\n",
      "La similitud entre el documento hir-22-351 y el documento Paper11879 es sim 0.21835173666477203\n",
      "La similitud entre el documento hir-22-351 y el documento Sketch Based Image Retrieval for Architecture es sim 0.3250139355659485\n",
      "La similitud entre el documento hir-22-351 y el documento tft es sim 0.3738747239112854\n",
      "La similitud entre el documento hir-22-351 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.3174748420715332\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento IJISRT23AUG773 es sim 0.27232301235198975\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.5921612977981567\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.5644641518592834\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento Paper11879 es sim 0.19140082597732544\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento Sketch Based Image Retrieval for Architecture es sim 0.385023295879364\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento tft es sim 0.42662671208381653\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.4612431228160858\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.18167239427566528\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.2176433950662613\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento Paper11879 es sim 0.7260582447052002\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento Sketch Based Image Retrieval for Architecture es sim 0.1511768400669098\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento tft es sim 0.24682484567165375\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.10521510988473892\n",
      "La similitud entre el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.4798661768436432\n",
      "La similitud entre el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH y el documento Paper11879 es sim 0.13599684834480286\n",
      "La similitud entre el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH y el documento Sketch Based Image Retrieval for Architecture es sim 0.38138043880462646\n",
      "La similitud entre el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH y el documento tft es sim 0.446127325296402\n",
      "La similitud entre el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.5999336242675781\n",
      "La similitud entre el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL y el documento Paper11879 es sim 0.25640764832496643\n",
      "La similitud entre el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL y el documento Sketch Based Image Retrieval for Architecture es sim 0.40793949365615845\n",
      "La similitud entre el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL y el documento tft es sim 0.4325717091560364\n",
      "La similitud entre el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.44570109248161316\n",
      "La similitud entre el documento Paper11879 y el documento Sketch Based Image Retrieval for Architecture es sim 0.1834026724100113\n",
      "La similitud entre el documento Paper11879 y el documento tft es sim 0.1834719181060791\n",
      "La similitud entre el documento Paper11879 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.04340558499097824\n",
      "La similitud entre el documento Sketch Based Image Retrieval for Architecture y el documento tft es sim 0.23592932522296906\n",
      "La similitud entre el documento Sketch Based Image Retrieval for Architecture y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.40446603298187256\n",
      "La similitud entre el documento tft y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.3464566469192505\n"
     ]
    }
   ],
   "source": [
    "# USING TRANSFORMERS\n",
    "\n",
    "# If we want to improve the similarity and use a word embeddings approach, we may use sentence transformers. This may take a while:\n",
    "sbert_model = SentenceTransformer(\n",
    "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")\n",
    "sentence_embeddings = sbert_model.encode(list(papers.values()))\n",
    "for i in range(len(sentence_embeddings)):\n",
    "    for j in range(i + 1, len(sentence_embeddings)):\n",
    "        sim = cosine(sentence_embeddings[i], sentence_embeddings[j])\n",
    "        print(\n",
    "            f\"La similitud entre el documento {list(papers.keys())[i]} y el documento {list(papers.keys())[j]} es sim {sim}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import pandas as pd\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(list(papers.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_matrix = cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             document  cluster\n",
      "0   With recent advances in technology, internet h...        0\n",
      "1   Reliable uncertainty estimation for time serie...        1\n",
      "2   Training modern deep learning models requires ...        0\n",
      "3   Deep learning (DL) can achieve impressive resu...        0\n",
      "4   Artificial Intelligence (AI), sometimes called...        0\n",
      "5   Cloud computing is a computing model which pro...        0\n",
      "6   Cloud computing has formed the conceptual and ...        0\n",
      "7   Artificial Intelligence is making a machine be...        0\n",
      "8   As artificial intelligence (AI) technology bec...        0\n",
      "9   In a Content Based Image Retrieval (CBIR) Syst...        1\n",
      "10  Deep learning is a form of machine learning th...        0\n",
      "11  My goal in this paper is twofold: to study how...        1\n",
      "12  Cloud computing has revolutionized the way bus...        0\n",
      "13  In this paper, we introduce a new vision-langu...        1\n",
      "14  Methods that combine local and global features...        1\n",
      "15  Cloud computing has had a significant impact o...        0\n",
      "16  Sketch-based image retrieval (SBIR) is an imag...        1\n",
      "17  Multi-horizon forecasting often contains a com...        1\n",
      "18  Transformers have recently emerged as a powerf...        1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicov\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters=2, affinity='cosine', linkage='complete')\n",
    "labels = clustering.fit_predict(cos_sim_matrix)\n",
    "\n",
    "#kmeans = KMeans(n_clusters=3, init='random', n_init=10, max_iter=300)\n",
    "#labels = kmeans.fit_predict(cos_sim_matrix)\n",
    "\n",
    "#dbscan = DBSCAN(eps=0.1, min_samples=2, metric='precomputed')\n",
    "#labels = dbscan.fit_predict(cos_sim_matrix)\n",
    "\n",
    "# print the clusters\n",
    "df = pd.DataFrame({'document': list(papers.values()), 'cluster': labels})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=2, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=2, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(n_components=2, random_state=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's do a countvectorizer now. This is different from TF-IDF\n",
    "count_vectorizer = CountVectorizer()\n",
    "X = count_vectorizer.fit_transform(list(papers.values()))\n",
    "# we are only creating 2 topics\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=0)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "the of and to on\n",
      "Topic 1:\n",
      "the and cloud computing of\n"
     ]
    }
   ],
   "source": [
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "for topic_id, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_id}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-6:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic distribution for 11621ijccsa02:\n",
      "Topic 0: 0.0045\n",
      "Topic 1: 0.9955\n",
      "Topic distribution for 1709.01907:\n",
      "Topic 0: 0.9952\n",
      "Topic 1: 0.0048\n",
      "Topic distribution for 1802.05799:\n",
      "Topic 0: 0.9937\n",
      "Topic 1: 0.0063\n",
      "Topic distribution for 2007.03051:\n",
      "Topic 0: 0.9608\n",
      "Topic 1: 0.0392\n",
      "Topic distribution for 208:\n",
      "Topic 0: 0.0037\n",
      "Topic 1: 0.9963\n",
      "Topic distribution for 269 An Insight into Cloud Computing Paradigm and Services:\n",
      "Topic 0: 0.0108\n",
      "Topic 1: 0.9892\n",
      "Topic distribution for 6114nsa03:\n",
      "Topic 0: 0.0036\n",
      "Topic 1: 0.9964\n",
      "Topic distribution for 9-12:\n",
      "Topic 0: 0.0041\n",
      "Topic 1: 0.9959\n",
      "Topic distribution for 907-Article Text-2692-1-10-20230720:\n",
      "Topic 0: 0.0062\n",
      "Topic 1: 0.9938\n",
      "Topic distribution for CBIR USING FEATURES DERIVED BY DEEP LEARNING:\n",
      "Topic 0: 0.9962\n",
      "Topic 1: 0.0038\n",
      "Topic distribution for hir-22-351:\n",
      "Topic 0: 0.9913\n",
      "Topic 1: 0.0087\n",
      "Topic distribution for How good are deep models in understanding generated images:\n",
      "Topic 0: 0.9964\n",
      "Topic 1: 0.0036\n",
      "Topic distribution for IJISRT23AUG773:\n",
      "Topic 0: 0.0035\n",
      "Topic 1: 0.9965\n",
      "Topic distribution for IMAGEBERT CROSS-MODAL PRE-TRAINING WITH:\n",
      "Topic 0: 0.9944\n",
      "Topic 1: 0.0056\n",
      "Topic distribution for LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL:\n",
      "Topic 0: 0.9972\n",
      "Topic 1: 0.0028\n",
      "Topic distribution for Paper11879:\n",
      "Topic 0: 0.0025\n",
      "Topic 1: 0.9975\n",
      "Topic distribution for Sketch Based Image Retrieval for Architecture:\n",
      "Topic 0: 0.9958\n",
      "Topic 1: 0.0042\n",
      "Topic distribution for tft:\n",
      "Topic 0: 0.9965\n",
      "Topic 1: 0.0035\n",
      "Topic distribution for VISION TRANSFORMERS NEED REGISTERS:\n",
      "Topic 0: 0.9954\n",
      "Topic 1: 0.0046\n"
     ]
    }
   ],
   "source": [
    "#Now let's see the probability of one of the sentence to belong to each topic\n",
    "\n",
    "for name, abstract in papers.items():\n",
    "    new_doc_bow = count_vectorizer.transform([abstract])\n",
    "# Compute the topic distribution for the new document\n",
    "    topic_distribution = lda.transform(new_doc_bow)\n",
    "    print(f\"Topic distribution for {name}:\")\n",
    "    for topic_idx, topic_prob in enumerate(topic_distribution[0]):\n",
    "        print(f\"Topic {topic_idx}: {topic_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "956"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99538326, 0.00461674])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_distribution[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence score: -0.66\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.models import LdaMulticore\n",
    "preprocessed_documents = []\n",
    "for document in papers.values():\n",
    "    tokens = vectorizer.get_feature_names_out()\n",
    "    preprocessed_documents.append(tokens)\n",
    "\n",
    "#print(tokens)\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(preprocessed_documents)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]\n",
    "\n",
    "lda_model = gensim.models.LdaModel(corpus=corpus, num_topics=2, id2word=dictionary, passes=10)\n",
    "coherence_model = gensim.models.CoherenceModel(model=lda_model, texts=preprocessed_documents, dictionary=dictionary, coherence='c_npmi')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence score: {coherence_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.001*\"becomes\" + 0.001*\"estimation\" + 0.001*\"recurrent\" + 0.001*\"fully\" + 0.001*\"background\" + 0.001*\"computations\" + 0.001*\"much\" + 0.001*\"instantly\" + 0.001*\"without\" + 0.001*\"conduct\"\n",
      "Topic 1: 0.002*\"outperform\" + 0.001*\"resulted\" + 0.001*\"collected\" + 0.001*\"dependencies\" + 0.001*\"adoption\" + 0.001*\"taken\" + 0.001*\"digitalized\" + 0.001*\"similarity\" + 0.001*\"tasks\" + 0.001*\"testing\"\n"
     ]
    }
   ],
   "source": [
    "for topic_id, topic_words in lda_model.print_topics(num_words=10):\n",
    "    print(f\"Topic {topic_id}: {topic_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.054*\"cloud\" + 0.048*\"computing\" + 0.020*\"big\" + 0.016*\"based\" + 0.016*\"will\"')\n",
      "(1, '0.031*\"computing\" + 0.031*\"cloud\" + 0.022*\"training\" + 0.014*\"significant\" + 0.009*\"may\"')\n",
      "(2, '0.040*\"artificial\" + 0.040*\"intelligence\" + 0.025*\"cloud\" + 0.020*\"human\" + 0.020*\"computing\"')\n",
      "(3, '0.024*\"series\" + 0.020*\"time\" + 0.014*\"deep\" + 0.014*\"layers\" + 0.010*\"models\"')\n",
      "(4, '0.025*\"visual\" + 0.019*\"maps\" + 0.013*\"solution\" + 0.013*\"tokens\" + 0.013*\"supervised\"')\n",
      "(5, '0.029*\"features\" + 0.017*\"images\" + 0.012*\"large\" + 0.012*\"propose\" + 0.012*\"results\"')\n",
      "(6, '0.028*\"model\" + 0.021*\"generated\" + 0.018*\"image\" + 0.018*\"images\" + 0.018*\"object\"')\n"
     ]
    }
   ],
   "source": [
    "# TOPIC MODELLING\n",
    "\n",
    "stop_words = get_stop_words(\"english\")\n",
    "keywords = [\n",
    "    [\n",
    "        word\n",
    "        for word in resumen.lower().split()\n",
    "        if word.isalpha() and word not in stop_words\n",
    "    ]\n",
    "    for resumen in papers.values()\n",
    "]\n",
    "dictionary = corpora.Dictionary(keywords)\n",
    "doc_term_matrix = [dictionary.doc2bow(title) for title in keywords]\n",
    "\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "lda_model = LDA(\n",
    "    corpus=doc_term_matrix,\n",
    "    id2word=dictionary,\n",
    "    num_topics=7,\n",
    "    random_state=100,\n",
    "    chunksize=1000,\n",
    "    passes=50,\n",
    ")\n",
    "\n",
    "temas = lda_model.print_topics(num_words=5)\n",
    "for tema in temas:\n",
    "    print(tema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prueba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
