{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grobid_client.grobid_client import GrobidClient\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "import os\n",
    "from stop_words import get_stop_words\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN\n",
    "import numpy as np\n",
    "# Now let's do topic modeling using LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "# si no hace bien los imports de utilsdescomenta esta linea\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils import remove_files, get_abstract, cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET ABSTRACTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorio donde se encuentran los archivos XML\n",
    "xml_dir = os.path.join(parent_dir, \"output\")\n",
    "papers_dir = os.path.join(parent_dir, \"papers\")\n",
    "\n",
    "remove_files(xml_dir)\n",
    "\n",
    "client = GrobidClient(config_path=\"./config.json\")\n",
    "client.process(\"processFulltextDocument\", papers_dir, output=xml_dir, consolidate_citations=True, tei_coordinates=True, n=20)\n",
    "\n",
    "# Lista para almacenar los resúmenes\n",
    "abstracts = {}\n",
    "\n",
    "# Procesar cada archivo XML en el directorio\n",
    "for file in os.listdir(xml_dir):\n",
    "    if file.endswith(\".xml\"):  # Verificar que el archivo sea XML\n",
    "        file_path = os.path.join(xml_dir, file)\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        abstract = get_abstract(root)\n",
    "        file_name = os.path.basename(file_path)[:-15]\n",
    "        abstracts[file_name] = abstract\n",
    "\n",
    "print(len(abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_dir = os.path.join(parent_dir, \"output\")\n",
    "papers_dir = os.path.join(parent_dir, \"papers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\nicov\\\\Documents\\\\Github\\\\OpenScience\\\\papers'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\nicov\\\\Documents\\\\Github\\\\OpenScience\\\\output'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMILARITY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La similitud entre el documento 11621ijccsa02 y el documento 1802.05799 es sim 0.015940522775053978\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.1687125712633133\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 9-12 es sim 0.0468757227063179\n",
      "La similitud entre el documento 11621ijccsa02 y el documento hir-22-351 es sim 0.030683305114507675\n",
      "La similitud entre el documento 11621ijccsa02 y el documento IJISRT23AUG773 es sim 0.15756084024906158\n",
      "La similitud entre el documento 11621ijccsa02 y el documento Paper11879 es sim 0.1512293517589569\n",
      "La similitud entre el documento 11621ijccsa02 y el documento Sketch Based Image Retrieval for Architecture es sim 0.015656426548957825\n",
      "La similitud entre el documento 11621ijccsa02 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.016979750245809555\n",
      "La similitud entre el documento 1802.05799 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.016517315059900284\n",
      "La similitud entre el documento 1802.05799 y el documento 9-12 es sim 0.030726730823516846\n",
      "La similitud entre el documento 1802.05799 y el documento hir-22-351 es sim 0.020018571987748146\n",
      "La similitud entre el documento 1802.05799 y el documento IJISRT23AUG773 es sim 0.005169818643480539\n",
      "La similitud entre el documento 1802.05799 y el documento Paper11879 es sim 0.02183684892952442\n",
      "La similitud entre el documento 1802.05799 y el documento Sketch Based Image Retrieval for Architecture es sim 0.010476387105882168\n",
      "La similitud entre el documento 1802.05799 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.007585856132209301\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento 9-12 es sim 0.013582476414740086\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento hir-22-351 es sim 0.010077143087983131\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento IJISRT23AUG773 es sim 0.15064319968223572\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento Paper11879 es sim 0.1574077308177948\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento Sketch Based Image Retrieval for Architecture es sim 0.03166283667087555\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.026579778641462326\n",
      "La similitud entre el documento 9-12 y el documento hir-22-351 es sim 0.04669714346528053\n",
      "La similitud entre el documento 9-12 y el documento IJISRT23AUG773 es sim 0.08365246653556824\n",
      "La similitud entre el documento 9-12 y el documento Paper11879 es sim 0.019938986748456955\n",
      "La similitud entre el documento 9-12 y el documento Sketch Based Image Retrieval for Architecture es sim 0.0213056281208992\n",
      "La similitud entre el documento 9-12 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.02267347276210785\n",
      "La similitud entre el documento hir-22-351 y el documento IJISRT23AUG773 es sim 0.011507067829370499\n",
      "La similitud entre el documento hir-22-351 y el documento Paper11879 es sim 0.031326424330472946\n",
      "La similitud entre el documento hir-22-351 y el documento Sketch Based Image Retrieval for Architecture es sim 0.026210684329271317\n",
      "La similitud entre el documento hir-22-351 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.016369465738534927\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento Paper11879 es sim 0.13991254568099976\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento Sketch Based Image Retrieval for Architecture es sim 0.03872150182723999\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.0402546301484108\n",
      "La similitud entre el documento Paper11879 y el documento Sketch Based Image Retrieval for Architecture es sim 0.011118465103209019\n",
      "La similitud entre el documento Paper11879 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.01654956489801407\n",
      "La similitud entre el documento Sketch Based Image Retrieval for Architecture y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.03435197472572327\n"
     ]
    }
   ],
   "source": [
    "textos = [resumen.split() for resumen in abstracts.values()]\n",
    "\n",
    "diccionario = corpora.Dictionary(textos)\n",
    "\n",
    "corpus = [diccionario.doc2bow(texto) for texto in textos]\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "index = similarities.MatrixSimilarity(tfidf[corpus])\n",
    "\n",
    "for i in range(len(textos)):\n",
    "    for j in range(i + 1, len(textos)):\n",
    "        vec_i = diccionario.doc2bow(textos[i])\n",
    "        vec_j = diccionario.doc2bow(textos[j])\n",
    "        sim_ij = index[tfidf[vec_i]][j]\n",
    "        print(\n",
    "            f\"La similitud entre el documento {list(abstracts.keys())[i]} y el documento {list(abstracts.keys())[j]} es sim {sim_ij}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicov\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05414c4205ba4b5b84f94a4192b063b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicov\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nicov\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b0cce0fabf4035a7bce85bda0d9347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a809454eef4bf29fd42b5807f5f1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733161f8c2d846e8a835c5adf007e6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f120c984b540e7a444ca0f37d50af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La similitud entre el documento 11621ijccsa02 y el documento 1802.05799 es sim 0.32246485352516174\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.6741066575050354\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 9-12 es sim 0.4558578133583069\n",
      "La similitud entre el documento 11621ijccsa02 y el documento hir-22-351 es sim 0.3331562876701355\n",
      "La similitud entre el documento 11621ijccsa02 y el documento IJISRT23AUG773 es sim 0.7527927160263062\n",
      "La similitud entre el documento 11621ijccsa02 y el documento Paper11879 es sim 0.6483190059661865\n",
      "La similitud entre el documento 11621ijccsa02 y el documento Sketch Based Image Retrieval for Architecture es sim 0.10967686772346497\n",
      "La similitud entre el documento 11621ijccsa02 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.06570909917354584\n",
      "La similitud entre el documento 1802.05799 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.3053826093673706\n",
      "La similitud entre el documento 1802.05799 y el documento 9-12 es sim 0.2970978915691376\n",
      "La similitud entre el documento 1802.05799 y el documento hir-22-351 es sim 0.6281082630157471\n",
      "La similitud entre el documento 1802.05799 y el documento IJISRT23AUG773 es sim 0.3815722167491913\n",
      "La similitud entre el documento 1802.05799 y el documento Paper11879 es sim 0.33296236395835876\n",
      "La similitud entre el documento 1802.05799 y el documento Sketch Based Image Retrieval for Architecture es sim 0.2827311158180237\n",
      "La similitud entre el documento 1802.05799 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.33160972595214844\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento 9-12 es sim 0.2191670835018158\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento hir-22-351 es sim 0.2832280397415161\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento IJISRT23AUG773 es sim 0.7465469241142273\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento Paper11879 es sim 0.6723699569702148\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento Sketch Based Image Retrieval for Architecture es sim 0.16976290941238403\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.053897857666015625\n",
      "La similitud entre el documento 9-12 y el documento hir-22-351 es sim 0.321656197309494\n",
      "La similitud entre el documento 9-12 y el documento IJISRT23AUG773 es sim 0.28672948479652405\n",
      "La similitud entre el documento 9-12 y el documento Paper11879 es sim 0.2357879877090454\n",
      "La similitud entre el documento 9-12 y el documento Sketch Based Image Retrieval for Architecture es sim 0.20081590116024017\n",
      "La similitud entre el documento 9-12 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.2612053155899048\n",
      "La similitud entre el documento hir-22-351 y el documento IJISRT23AUG773 es sim 0.34636732935905457\n",
      "La similitud entre el documento hir-22-351 y el documento Paper11879 es sim 0.21835173666477203\n",
      "La similitud entre el documento hir-22-351 y el documento Sketch Based Image Retrieval for Architecture es sim 0.3250139355659485\n",
      "La similitud entre el documento hir-22-351 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.3174748420715332\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento Paper11879 es sim 0.7260582447052002\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento Sketch Based Image Retrieval for Architecture es sim 0.1511768400669098\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.10521510988473892\n",
      "La similitud entre el documento Paper11879 y el documento Sketch Based Image Retrieval for Architecture es sim 0.1834026724100113\n",
      "La similitud entre el documento Paper11879 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.04340558499097824\n",
      "La similitud entre el documento Sketch Based Image Retrieval for Architecture y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.40446603298187256\n"
     ]
    }
   ],
   "source": [
    "# USING TRANSFORMERS\n",
    "\n",
    "# If we want to improve the similarity and use a word embeddings approach, we may use sentence transformers. This may take a while:\n",
    "sbert_model = SentenceTransformer(\n",
    "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")\n",
    "sentence_embeddings = sbert_model.encode(list(abstracts.values()))\n",
    "for i in range(len(sentence_embeddings)):\n",
    "    for j in range(i + 1, len(sentence_embeddings)):\n",
    "        sim = cosine(sentence_embeddings[i], sentence_embeddings[j])\n",
    "        print(\n",
    "            f\"La similitud entre el documento {list(abstracts.keys())[i]} y el documento {list(abstracts.keys())[j]} es sim {sim}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import pandas as pd\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(list(abstracts.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_matrix = cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            document  cluster\n",
      "0  With recent advances in technology, internet h...        0\n",
      "1  Training modern deep learning models requires ...        1\n",
      "2  Cloud computing is a computing model which pro...        0\n",
      "3  Artificial Intelligence is making a machine be...        0\n",
      "4  Deep learning is a form of machine learning th...        0\n",
      "5  Cloud computing has revolutionized the way bus...        0\n",
      "6  Cloud computing has had a significant impact o...        0\n",
      "7  Sketch-based image retrieval (SBIR) is an imag...        0\n",
      "8  Transformers have recently emerged as a powerf...        0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicov\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters=2, affinity='cosine', linkage='complete')\n",
    "labels = clustering.fit_predict(cos_sim_matrix)\n",
    "\n",
    "#kmeans = KMeans(n_clusters=3, init='random', n_init=10, max_iter=300)\n",
    "#labels = kmeans.fit_predict(cos_sim_matrix)\n",
    "\n",
    "#dbscan = DBSCAN(eps=0.1, min_samples=2, metric='precomputed')\n",
    "#labels = dbscan.fit_predict(cos_sim_matrix)\n",
    "\n",
    "# print the clusters\n",
    "df = pd.DataFrame({'document': list(abstracts.values()), 'cluster': labels})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=2, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=2, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(n_components=2, random_state=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's do a countvectorizer now. This is different from TF-IDF\n",
    "count_vectorizer = CountVectorizer()\n",
    "X = count_vectorizer.fit_transform(list(abstracts.values()))\n",
    "# we are only creating 2 topics\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=0)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "the and data of for\n",
      "Topic 1:\n",
      "the and to computing of\n"
     ]
    }
   ],
   "source": [
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "for topic_id, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_id}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-6:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic distribution for 11621ijccsa02:\n",
      "Topic 0: 0.0050\n",
      "Topic 1: 0.9950\n",
      "Topic distribution for 1802.05799:\n",
      "Topic 0: 0.9933\n",
      "Topic 1: 0.0067\n",
      "Topic distribution for 269 An Insight into Cloud Computing Paradigm and Services:\n",
      "Topic 0: 0.0097\n",
      "Topic 1: 0.9903\n",
      "Topic distribution for 9-12:\n",
      "Topic 0: 0.9967\n",
      "Topic 1: 0.0033\n",
      "Topic distribution for hir-22-351:\n",
      "Topic 0: 0.0072\n",
      "Topic 1: 0.9928\n",
      "Topic distribution for IJISRT23AUG773:\n",
      "Topic 0: 0.9959\n",
      "Topic 1: 0.0041\n",
      "Topic distribution for Paper11879:\n",
      "Topic 0: 0.0027\n",
      "Topic 1: 0.9973\n",
      "Topic distribution for Sketch Based Image Retrieval for Architecture:\n",
      "Topic 0: 0.0044\n",
      "Topic 1: 0.9956\n",
      "Topic distribution for VISION TRANSFORMERS NEED REGISTERS:\n",
      "Topic 0: 0.0046\n",
      "Topic 1: 0.9954\n"
     ]
    }
   ],
   "source": [
    "#Now let's see the probability of one of the sentence to belong to each topic\n",
    "\n",
    "for name, abstract in abstracts.items():\n",
    "    new_doc_bow = count_vectorizer.transform([abstract])\n",
    "# Compute the topic distribution for the new document\n",
    "    topic_distribution = lda.transform(new_doc_bow)\n",
    "    print(f\"Topic distribution for {name}:\")\n",
    "    for topic_idx, topic_prob in enumerate(topic_distribution[0]):\n",
    "        print(f\"Topic {topic_idx}: {topic_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.004632, 0.995368])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_distribution[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence score: -0.66\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.models import LdaMulticore\n",
    "preprocessed_documents = []\n",
    "for document in abstracts.values():\n",
    "    tokens = vectorizer.get_feature_names_out()\n",
    "    preprocessed_documents.append(tokens)\n",
    "\n",
    "#print(tokens)\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(preprocessed_documents)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]\n",
    "\n",
    "lda_model = gensim.models.LdaModel(corpus=corpus, num_topics=2, id2word=dictionary, passes=10)\n",
    "coherence_model = gensim.models.CoherenceModel(model=lda_model, texts=preprocessed_documents, dictionary=dictionary, coherence='c_npmi')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence score: {coherence_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.002*\"developments\" + 0.002*\"provided\" + 0.002*\"adoption\" + 0.002*\"implementation\" + 0.002*\"this\" + 0.002*\"capabilities\" + 0.002*\"also\" + 0.002*\"business\" + 0.002*\"time\" + 0.002*\"customer\"\n",
      "Topic 1: 0.002*\"some\" + 0.002*\"matching\" + 0.002*\"terms\" + 0.002*\"significant\" + 0.002*\"api\" + 0.002*\"while\" + 0.002*\"changing\" + 0.002*\"despite\" + 0.002*\"lot\" + 0.002*\"vision\"\n"
     ]
    }
   ],
   "source": [
    "for topic_id, topic_words in lda_model.print_topics(num_words=10):\n",
    "    print(f\"Topic {topic_id}: {topic_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.035*\"cloud\" + 0.029*\"computing\" + 0.026*\"smbs\" + 0.023*\"intelligence\" + 0.023*\"artificial\"')\n",
      "(1, '0.021*\"image\" + 0.021*\"visual\" + 0.016*\"methods\" + 0.016*\"network\" + 0.016*\"retrieval\"')\n",
      "(2, '0.003*\"artificial\" + 0.003*\"intelligence\" + 0.003*\"computing\" + 0.003*\"cloud\" + 0.003*\"may\"')\n",
      "(3, '0.003*\"visual\" + 0.003*\"image\" + 0.003*\"architecture\" + 0.003*\"network\" + 0.003*\"methods\"')\n",
      "(4, '0.003*\"computing\" + 0.003*\"artificial\" + 0.003*\"cloud\" + 0.003*\"intelligence\" + 0.003*\"will\"')\n",
      "(5, '0.053*\"cloud\" + 0.053*\"computing\" + 0.024*\"training\" + 0.015*\"significant\" + 0.010*\"can\"')\n",
      "(6, '0.080*\"computing\" + 0.073*\"cloud\" + 0.067*\"big\" + 0.040*\"will\" + 0.034*\"data\"')\n"
     ]
    }
   ],
   "source": [
    "# TOPIC MODELLING\n",
    "\n",
    "stop_words = get_stop_words(\"english\")\n",
    "keywords = [\n",
    "    [\n",
    "        word\n",
    "        for word in resumen.lower().split()\n",
    "        if word.isalpha() and word not in stop_words\n",
    "    ]\n",
    "    for resumen in abstracts.values()\n",
    "]\n",
    "dictionary = corpora.Dictionary(keywords)\n",
    "doc_term_matrix = [dictionary.doc2bow(title) for title in keywords]\n",
    "\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "lda_model = LDA(\n",
    "    corpus=doc_term_matrix,\n",
    "    id2word=dictionary,\n",
    "    num_topics=7,\n",
    "    random_state=100,\n",
    "    chunksize=1000,\n",
    "    passes=50,\n",
    ")\n",
    "\n",
    "temas = lda_model.print_topics(num_words=5)\n",
    "for tema in temas:\n",
    "    print(tema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prueba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
