{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from grobid_client.grobid_client import GrobidClient\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "import os\n",
    "from stop_words import get_stop_words\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN\n",
    "import numpy as np\n",
    "# Now let's do topic modeling using LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "# si no hace bien los imports de utilsdescomenta esta linea\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils import remove_files, get_abstract, cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET ABSTRACTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1709.01907': 'Reliable uncertainty estimation for time series prediction is critical in many fields, including physics, biology, and manufacturing. At Uber, probabilistic time series forecasting is used for robust prediction of number of trips during special events, driver incentive allocation, as well as real-time anomaly detection across millions of metrics. Classical time series models are often used in conjunction with a probabilistic formulation for uncertainty estimation. However, such models are hard to tune, scale, and add exogenous variables to. Motivated by the recent resurgence of Long Short Term Memory networks, we propose a novel end-to-end Bayesian deep model that provides time series prediction along with uncertainty estimation. We provide detailed experiments of the proposed solution on completed trips data, and successfully apply it to large-scale time series anomaly detection at Uber.', '1802.05799': \"Training modern deep learning models requires large amounts of computation, often provided by GPUs. Scaling computation from one GPU to many can enable much faster training and research progress but entails two complications. First, the training library must support inter-GPU communication. Depending on the particular methods employed, this communication may entail anywhere from negligible to significant overhead. Second, the user must modify his or her training code to take advantage of inter-GPU communication. Depending on the training library's API, the modification required may be either significant or minimal.\", '2007.03051': 'Deep learning (DL) can achieve impressive results across a wide variety of tasks, but this often comes at the cost of training models for extensive periods on specialized hardware accelerators. This energy-intensive workload has seen immense growth in recent years. Machine learning (ML) may become a significant contributor to climate change if this exponential trend continues. If practitioners are aware of their energy and carbon footprint, then they may actively take steps to reduce it whenever possible. In this work, we present carbontracker, a tool for tracking and predicting the energy and carbon footprint of training DL models. We propose that energy and carbon footprint of model development and training is reported alongside performance metrics using tools like carbontracker. We hope this will promote responsible computing in ML and encourage research into energy-efficient deep neural networks. 1  ', 'hir-22-351': 'Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator formally to specify all of the knowledge needed by the computer. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in relation to deep learning.'}\n"
     ]
    }
   ],
   "source": [
    "# Directorio donde se encuentran los archivos XML\n",
    "xml_dir = os.path.join(parent_dir, \"output\")\n",
    "# remove_files(xml_dir)\n",
    "\n",
    "# client = GrobidClient(config_path=\"code_KG/config.json\")\n",
    "# client.process(\"processFulltextDocument\", \"./papers\", output=\"./output/\", consolidate_citations=True, tei_coordinates=True, n=20)\n",
    "\n",
    "# Lista para almacenar los res√∫menes\n",
    "abstracts = {}\n",
    "\n",
    "# Procesar cada archivo XML en el directorio\n",
    "for file in os.listdir(xml_dir):\n",
    "    if file.endswith(\".xml\"):  # Verificar que el archivo sea XML\n",
    "        file_path = os.path.join(xml_dir, file)\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        abstract = get_abstract(root)\n",
    "        file_name = os.path.basename(file_path)[:-15]\n",
    "        abstracts[file_name] = abstract\n",
    "\n",
    "print(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMILARITY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La similitud entre el documento 1709.01907 y el documento 1802.05799 es sim 0.002790855709463358\n",
      "La similitud entre el documento 1709.01907 y el documento 2007.03051 es sim 0.028237465769052505\n",
      "La similitud entre el documento 1709.01907 y el documento hir-22-351 es sim 0.00998421385884285\n",
      "La similitud entre el documento 1802.05799 y el documento 2007.03051 es sim 0.07665908336639404\n",
      "La similitud entre el documento 1802.05799 y el documento hir-22-351 es sim 0.02041800133883953\n",
      "La similitud entre el documento 2007.03051 y el documento hir-22-351 es sim 0.019381240010261536\n"
     ]
    }
   ],
   "source": [
    "textos = [resumen.split() for resumen in abstracts.values()]\n",
    "\n",
    "diccionario = corpora.Dictionary(textos)\n",
    "\n",
    "corpus = [diccionario.doc2bow(texto) for texto in textos]\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "index = similarities.MatrixSimilarity(tfidf[corpus])\n",
    "\n",
    "for i in range(len(textos)):\n",
    "    for j in range(i + 1, len(textos)):\n",
    "        vec_i = diccionario.doc2bow(textos[i])\n",
    "        vec_j = diccionario.doc2bow(textos[j])\n",
    "        sim_ij = index[tfidf[vec_i]][j]\n",
    "        print(\n",
    "            f\"La similitud entre el documento {list(abstracts.keys())[i]} y el documento {list(abstracts.keys())[j]} es sim {sim_ij}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La similitud entre el documento 1709.01907 y el documento 1802.05799 es sim 0.2755492925643921\n",
      "La similitud entre el documento 1709.01907 y el documento 2007.03051 es sim 0.3994024991989136\n",
      "La similitud entre el documento 1709.01907 y el documento hir-22-351 es sim 0.21654930710792542\n",
      "La similitud entre el documento 1802.05799 y el documento 2007.03051 es sim 0.6693546772003174\n",
      "La similitud entre el documento 1802.05799 y el documento hir-22-351 es sim 0.6281082630157471\n",
      "La similitud entre el documento 2007.03051 y el documento hir-22-351 es sim 0.6317006945610046\n"
     ]
    }
   ],
   "source": [
    "# USING TRANSFORMERS\n",
    "\n",
    "# If we want to improve the similarity and use a word embeddings approach, we may use sentence transformers. This may take a while:\n",
    "sbert_model = SentenceTransformer(\n",
    "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")\n",
    "sentence_embeddings = sbert_model.encode(list(abstracts.values()))\n",
    "for i in range(len(sentence_embeddings)):\n",
    "    for j in range(i + 1, len(sentence_embeddings)):\n",
    "        sim = cosine(sentence_embeddings[i], sentence_embeddings[j])\n",
    "        print(\n",
    "            f\"La similitud entre el documento {list(abstracts.keys())[i]} y el documento {list(abstracts.keys())[j]} es sim {sim}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import pandas as pd\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(list(abstracts.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_matrix = cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            document  cluster\n",
      "0  Reliable uncertainty estimation for time serie...        1\n",
      "1  Training modern deep learning models requires ...        0\n",
      "2  Deep learning (DL) can achieve impressive resu...        0\n",
      "3  Deep learning is a form of machine learning th...        0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nvegamun\\AppData\\Local\\anaconda3\\envs\\prueba\\lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters=2, affinity='cosine', linkage='complete')\n",
    "labels = clustering.fit_predict(cos_sim_matrix)\n",
    "\n",
    "#kmeans = KMeans(n_clusters=3, init='random', n_init=10, max_iter=300)\n",
    "#labels = kmeans.fit_predict(cos_sim_matrix)\n",
    "\n",
    "#dbscan = DBSCAN(eps=0.1, min_samples=2, metric='precomputed')\n",
    "#labels = dbscan.fit_predict(cos_sim_matrix)\n",
    "\n",
    "# print the clusters\n",
    "df = pd.DataFrame({'document': list(abstracts.values()), 'cluster': labels})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {color: black;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=2, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=2, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(n_components=2, random_state=0)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's do a countvectorizer now. This is different from TF-IDF\n",
    "count_vectorizer = CountVectorizer()\n",
    "X = count_vectorizer.fit_transform(list(abstracts.values()))\n",
    "# we are only creating 2 topics\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=0)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "time of series to for\n",
      "Topic 1:\n",
      "of the to training and\n"
     ]
    }
   ],
   "source": [
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "for topic_id, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_id}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-6:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic distribution for 1709.01907:\n",
      "Topic 0: 0.9955\n",
      "Topic 1: 0.0045\n",
      "Topic distribution for 1802.05799:\n",
      "Topic 0: 0.0060\n",
      "Topic 1: 0.9940\n",
      "Topic distribution for 2007.03051:\n",
      "Topic 0: 0.0043\n",
      "Topic 1: 0.9957\n",
      "Topic distribution for hir-22-351:\n",
      "Topic 0: 0.0062\n",
      "Topic 1: 0.9938\n"
     ]
    }
   ],
   "source": [
    "#Now let's see the probability of one of the sentence to belong to each topic\n",
    "\n",
    "for name, abstract in abstracts.items():\n",
    "    new_doc_bow = count_vectorizer.transform([abstract])\n",
    "# Compute the topic distribution for the new document\n",
    "    topic_distribution = lda.transform(new_doc_bow)\n",
    "    print(f\"Topic distribution for {name}:\")\n",
    "    for topic_idx, topic_prob in enumerate(topic_distribution[0]):\n",
    "        print(f\"Topic {topic_idx}: {topic_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00622981, 0.99377019])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_distribution[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence score: -0.69\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.models import LdaMulticore\n",
    "preprocessed_documents = []\n",
    "for document in abstracts.values():\n",
    "    tokens = vectorizer.get_feature_names_out()\n",
    "    preprocessed_documents.append(tokens)\n",
    "\n",
    "#print(tokens)\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(preprocessed_documents)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]\n",
    "\n",
    "lda_model = gensim.models.LdaModel(corpus=corpus, num_topics=2, id2word=dictionary, passes=10)\n",
    "coherence_model = gensim.models.CoherenceModel(model=lda_model, texts=preprocessed_documents, dictionary=dictionary, coherence='c_npmi')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence score: {coherence_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.004*\"either\" + 0.004*\"completed\" + 0.004*\"performance\" + 0.004*\"complicated\" + 0.004*\"terms\" + 0.004*\"library\" + 0.004*\"building\" + 0.004*\"hierarchies\" + 0.004*\"topics\" + 0.004*\"seen\"\n",
      "Topic 1: 0.004*\"driver\" + 0.004*\"gpus\" + 0.004*\"then\" + 0.004*\"simpler\" + 0.004*\"critical\" + 0.004*\"conjunction\" + 0.004*\"to\" + 0.004*\"no\" + 0.004*\"series\" + 0.004*\"extensive\"\n"
     ]
    }
   ],
   "source": [
    "for topic_id, topic_words in lda_model.print_topics(num_words=10):\n",
    "    print(f\"Topic {topic_id}: {topic_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.006*\"series\" + 0.006*\"time\" + 0.006*\"uncertainty\" + 0.006*\"models\" + 0.006*\"prediction\"')\n",
      "(1, '0.006*\"series\" + 0.006*\"time\" + 0.006*\"prediction\" + 0.006*\"anomaly\" + 0.006*\"used\"')\n",
      "(2, '0.006*\"time\" + 0.006*\"training\" + 0.006*\"series\" + 0.006*\"models\" + 0.006*\"deep\"')\n",
      "(3, '0.053*\"series\" + 0.053*\"time\" + 0.032*\"prediction\" + 0.032*\"uncertainty\" + 0.022*\"models\"')\n",
      "(4, '0.031*\"training\" + 0.031*\"carbon\" + 0.031*\"energy\" + 0.021*\"deep\" + 0.021*\"may\"')\n",
      "(5, '0.006*\"training\" + 0.006*\"series\" + 0.006*\"time\" + 0.006*\"models\" + 0.006*\"often\"')\n",
      "(6, '0.043*\"training\" + 0.026*\"deep\" + 0.026*\"computer\" + 0.026*\"learning\" + 0.018*\"significant\"')\n"
     ]
    }
   ],
   "source": [
    "# TOPIC MODELLING\n",
    "\n",
    "stop_words = get_stop_words(\"english\")\n",
    "keywords = [\n",
    "    [\n",
    "        word\n",
    "        for word in resumen.lower().split()\n",
    "        if word.isalpha() and word not in stop_words\n",
    "    ]\n",
    "    for resumen in abstracts.values()\n",
    "]\n",
    "dictionary = corpora.Dictionary(keywords)\n",
    "doc_term_matrix = [dictionary.doc2bow(title) for title in keywords]\n",
    "\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "lda_model = LDA(\n",
    "    corpus=doc_term_matrix,\n",
    "    id2word=dictionary,\n",
    "    num_topics=7,\n",
    "    random_state=100,\n",
    "    chunksize=1000,\n",
    "    passes=50,\n",
    ")\n",
    "\n",
    "temas = lda_model.print_topics(num_words=5)\n",
    "for tema in temas:\n",
    "    print(tema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prueba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
