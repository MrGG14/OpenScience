{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from grobid_client.grobid_client import GrobidClient\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "import os\n",
    "from stop_words import get_stop_words\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN\n",
    "import numpy as np\n",
    "# Now let's do topic modeling using LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "# si no hace bien los imports de utilsdescomenta esta linea\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils import remove_files, get_abstract, cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET ABSTRACTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de documentos: 19\n"
     ]
    }
   ],
   "source": [
    "# Directorio donde se encuentran los archivos XML\n",
    "xml_dir = os.path.join(parent_dir, \"output\")\n",
    "# papers_dir = os.path.join(parent_dir, \"papers\")\n",
    "\n",
    "# remove_files(xml_dir)\n",
    "\n",
    "# client = GrobidClient(config_path=\"./config.json\")\n",
    "# client.process(\"processFulltextDocument\", papers_dir, output=xml_dir, consolidate_citations=True, tei_coordinates=True, n=20)\n",
    "\n",
    "# Lista para almacenar los res√∫menes\n",
    "abstracts = {}\n",
    "\n",
    "# Procesar cada archivo XML en el directorio\n",
    "for file in os.listdir(xml_dir):\n",
    "    if file.endswith(\".xml\"):  # Verificar que el archivo sea XML\n",
    "        file_path = os.path.join(xml_dir, file)\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        abstract = get_abstract(root)\n",
    "        file_name = os.path.basename(file_path)[:-15]\n",
    "        abstracts[file_name] = abstract\n",
    "\n",
    "print(f'Numero de documentos: {len(abstracts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NA \n",
      "\t\t\t\t\t\t\t\n",
      "NA \n",
      "\t\t\t\t\t\t\t\n",
      "NA \n",
      "\t\t\t\n",
      "NA \n",
      "\t\t\t\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parsear el XML\n",
    "tree = ET.parse(os.path.join(xml_dir, '9-12.grobid.tei.xml'))\n",
    "\n",
    "root = tree.getroot()\n",
    "\n",
    "# Definir el espacio de nombres\n",
    "ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "# Encontrar todos los autores\n",
    "autores = root.findall('.//tei:author', ns)\n",
    "\n",
    "# Obtener los nombres completos de los autores\n",
    "nombres_completos = []\n",
    "for autor in autores:\n",
    "    ET.tostring(autor, encoding=\"utf8\").decode(\"utf8\")\n",
    "    # Obtener el nombre completo del autor concatenando nombre y apellido\n",
    "    try:\n",
    "        nombre = autor.find('../tei:forename', ns).text\n",
    "        nombre_completo = f\"{nombre} {apellido}\"\n",
    "        nombres_completos.append(nombre_completo)\n",
    "    except:\n",
    "        apellido = autor.text\n",
    "        nombre_completo = f\"NA {apellido}\"\n",
    "        nombres_completos.append(nombre_completo)\n",
    "\n",
    "# Imprimir los nombres completos de los autores\n",
    "for nombre_completo in nombres_completos:\n",
    "    print(nombre_completo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ms Mani\n",
      "Fatima Raju\n",
      "Madhura Ayachit\n",
      "Munnish Sabharwal\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Parsear el XML\n",
    "tree = ET.parse(os.path.join(xml_dir, '9-12.grobid.tei.xml'))\n",
    "root = tree.getroot()\n",
    "\n",
    "# Definir el espacio de nombres\n",
    "ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "# Encontrar todos los autores\n",
    "autores = root.findall('.//tei:author', ns)\n",
    "\n",
    "# Obtener los nombres completos de los autores\n",
    "nombres_completos = []\n",
    "for autor in autores:\n",
    "    # Encontrar el elemento forename dentro de author\n",
    "    forename_elem = autor.find('.//tei:forename', ns)\n",
    "    # Encontrar el elemento surname dentro de author\n",
    "    surname_elem = autor.find('.//tei:surname', ns)\n",
    "    \n",
    "    # Verificar si se encontraron los elementos forename y surname\n",
    "    try:\n",
    "        # Obtener el texto de los elementos forename y surname\n",
    "        nombre = forename_elem.text\n",
    "        apellido = surname_elem.text\n",
    "        nombre_completo = f\"{nombre} {apellido}\"\n",
    "    except:\n",
    "        # Si no se encontraron los elementos, asignar NA a nombre y apellido\n",
    "        print('Missing name and/or surname. Cant get full name.')\n",
    "    \n",
    "    nombres_completos.append(nombre_completo)\n",
    "\n",
    "# Imprimir los nombres completos de los autores\n",
    "for nombre_completo in nombres_completos:\n",
    "    print(nombre_completo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "autores[2].find('../tei:forename', ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_dir = os.path.join(parent_dir, \"output\")\n",
    "papers_dir = os.path.join(parent_dir, \"papers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMILARITY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La similitud entre el documento 11621ijccsa02 y el documento 1709.01907 es sim 0.022268354892730713\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 1802.05799 es sim 0.018618298694491386\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 2007.03051 es sim 0.0440535768866539\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 208 es sim 0.045304201543331146\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.21600942313671112\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 6114nsa03 es sim 0.25234338641166687\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 9-12 es sim 0.051240257918834686\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.01917664147913456\n",
      "La similitud entre el documento 11621ijccsa02 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.010931157507002354\n",
      "La similitud entre el documento 11621ijccsa02 y el documento hir-22-351 es sim 0.02943280339241028\n",
      "La similitud entre el documento 11621ijccsa02 y el documento How good are deep models in understanding generated images es sim 0.005171922966837883\n",
      "La similitud entre el documento 11621ijccsa02 y el documento IJISRT23AUG773 es sim 0.21243876218795776\n",
      "La similitud entre el documento 11621ijccsa02 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.003396933665499091\n",
      "La similitud entre el documento 11621ijccsa02 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.0035566070582717657\n",
      "La similitud entre el documento 11621ijccsa02 y el documento Paper11879 es sim 0.18387076258659363\n",
      "La similitud entre el documento 11621ijccsa02 y el documento Sketch Based Image Retrieval for Architecture es sim 0.008616222068667412\n",
      "La similitud entre el documento 11621ijccsa02 y el documento tft es sim 0.014443309977650642\n",
      "La similitud entre el documento 11621ijccsa02 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.014934487640857697\n",
      "La similitud entre el documento 1709.01907 y el documento 1802.05799 es sim 0.006358814425766468\n",
      "La similitud entre el documento 1709.01907 y el documento 2007.03051 es sim 0.020259998738765717\n",
      "La similitud entre el documento 1709.01907 y el documento 208 es sim 0.009491898119449615\n",
      "La similitud entre el documento 1709.01907 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.016023611649870872\n",
      "La similitud entre el documento 1709.01907 y el documento 6114nsa03 es sim 0.018005508929491043\n",
      "La similitud entre el documento 1709.01907 y el documento 9-12 es sim 0.010601626709103584\n",
      "La similitud entre el documento 1709.01907 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.002904281485825777\n",
      "La similitud entre el documento 1709.01907 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.04665391147136688\n",
      "La similitud entre el documento 1709.01907 y el documento hir-22-351 es sim 0.0045777251943945885\n",
      "La similitud entre el documento 1709.01907 y el documento How good are deep models in understanding generated images es sim 0.013204552233219147\n",
      "La similitud entre el documento 1709.01907 y el documento IJISRT23AUG773 es sim 0.024176495149731636\n",
      "La similitud entre el documento 1709.01907 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.011989818885922432\n",
      "La similitud entre el documento 1709.01907 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.01836450770497322\n",
      "La similitud entre el documento 1709.01907 y el documento Paper11879 es sim 0.013347569853067398\n",
      "La similitud entre el documento 1709.01907 y el documento Sketch Based Image Retrieval for Architecture es sim 0.03254225105047226\n",
      "La similitud entre el documento 1709.01907 y el documento tft es sim 0.09431024640798569\n",
      "La similitud entre el documento 1709.01907 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.040033720433712006\n",
      "La similitud entre el documento 1802.05799 y el documento 2007.03051 es sim 0.10709698498249054\n",
      "La similitud entre el documento 1802.05799 y el documento 208 es sim 0.019403114914894104\n",
      "La similitud entre el documento 1802.05799 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.010583772324025631\n",
      "La similitud entre el documento 1802.05799 y el documento 6114nsa03 es sim 0.00391280697658658\n",
      "La similitud entre el documento 1802.05799 y el documento 9-12 es sim 0.026848789304494858\n",
      "La similitud entre el documento 1802.05799 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.004009630531072617\n",
      "La similitud entre el documento 1802.05799 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.03458882123231888\n",
      "La similitud entre el documento 1802.05799 y el documento hir-22-351 es sim 0.02041728049516678\n",
      "La similitud entre el documento 1802.05799 y el documento How good are deep models in understanding generated images es sim 0.02031589113175869\n",
      "La similitud entre el documento 1802.05799 y el documento IJISRT23AUG773 es sim 0.003041214542463422\n",
      "La similitud entre el documento 1802.05799 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.006063825450837612\n",
      "La similitud entre el documento 1802.05799 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.03859197348356247\n",
      "La similitud entre el documento 1802.05799 y el documento Paper11879 es sim 0.020491210743784904\n",
      "La similitud entre el documento 1802.05799 y el documento Sketch Based Image Retrieval for Architecture es sim 0.009295986965298653\n",
      "La similitud entre el documento 1802.05799 y el documento tft es sim 0.015483912080526352\n",
      "La similitud entre el documento 1802.05799 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.005256659351289272\n",
      "La similitud entre el documento 2007.03051 y el documento 208 es sim 0.025235813111066818\n",
      "La similitud entre el documento 2007.03051 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.05446472018957138\n",
      "La similitud entre el documento 2007.03051 y el documento 6114nsa03 es sim 0.03806239366531372\n",
      "La similitud entre el documento 2007.03051 y el documento 9-12 es sim 0.041584767401218414\n",
      "La similitud entre el documento 2007.03051 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.021960606798529625\n",
      "La similitud entre el documento 2007.03051 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.03320314735174179\n",
      "La similitud entre el documento 2007.03051 y el documento hir-22-351 es sim 0.019039882346987724\n",
      "La similitud entre el documento 2007.03051 y el documento How good are deep models in understanding generated images es sim 0.022595901042222977\n",
      "La similitud entre el documento 2007.03051 y el documento IJISRT23AUG773 es sim 0.046047333627939224\n",
      "La similitud entre el documento 2007.03051 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.02150655724108219\n",
      "La similitud entre el documento 2007.03051 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.06082051992416382\n",
      "La similitud entre el documento 2007.03051 y el documento Paper11879 es sim 0.040660277009010315\n",
      "La similitud entre el documento 2007.03051 y el documento Sketch Based Image Retrieval for Architecture es sim 0.01366021204739809\n",
      "La similitud entre el documento 2007.03051 y el documento tft es sim 0.05994286388158798\n",
      "La similitud entre el documento 2007.03051 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.03142338618636131\n",
      "La similitud entre el documento 208 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.004075708333402872\n",
      "La similitud entre el documento 208 y el documento 6114nsa03 es sim 0.042983971536159515\n",
      "La similitud entre el documento 208 y el documento 9-12 es sim 0.2930697202682495\n",
      "La similitud entre el documento 208 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.1955643594264984\n",
      "La similitud entre el documento 208 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.01326720044016838\n",
      "La similitud entre el documento 208 y el documento hir-22-351 es sim 0.07869458943605423\n",
      "La similitud entre el documento 208 y el documento How good are deep models in understanding generated images es sim 0.03287221118807793\n",
      "La similitud entre el documento 208 y el documento IJISRT23AUG773 es sim 0.01904100365936756\n",
      "La similitud entre el documento 208 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.003938582725822926\n",
      "La similitud entre el documento 208 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.02365797758102417\n",
      "La similitud entre el documento 208 y el documento Paper11879 es sim 0.04804679751396179\n",
      "La similitud entre el documento 208 y el documento Sketch Based Image Retrieval for Architecture es sim 0.006295825354754925\n",
      "La similitud entre el documento 208 y el documento tft es sim 0.03473251685500145\n",
      "La similitud entre el documento 208 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.0049843257293105125\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento 6114nsa03 es sim 0.23569341003894806\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento 9-12 es sim 0.007512201555073261\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento 907-Article Text-2692-1-10-20230720 es sim 0.03190731629729271\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.034231044352054596\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento hir-22-351 es sim 0.010767989791929722\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento How good are deep models in understanding generated images es sim 0.020192952826619148\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento IJISRT23AUG773 es sim 0.18524028360843658\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.03307023644447327\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.01866886019706726\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento Paper11879 es sim 0.18385788798332214\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento Sketch Based Image Retrieval for Architecture es sim 0.020073752850294113\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento tft es sim 0.014960000291466713\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.010987944900989532\n",
      "La similitud entre el documento 6114nsa03 y el documento 9-12 es sim 0.03651692718267441\n",
      "La similitud entre el documento 6114nsa03 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.010418219491839409\n",
      "La similitud entre el documento 6114nsa03 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.026069127023220062\n",
      "La similitud entre el documento 6114nsa03 y el documento hir-22-351 es sim 0.01868584007024765\n",
      "La similitud entre el documento 6114nsa03 y el documento How good are deep models in understanding generated images es sim 0.009218871593475342\n",
      "La similitud entre el documento 6114nsa03 y el documento IJISRT23AUG773 es sim 0.2305990755558014\n",
      "La similitud entre el documento 6114nsa03 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.014171261340379715\n",
      "La similitud entre el documento 6114nsa03 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.038991015404462814\n",
      "La similitud entre el documento 6114nsa03 y el documento Paper11879 es sim 0.19125492870807648\n",
      "La similitud entre el documento 6114nsa03 y el documento Sketch Based Image Retrieval for Architecture es sim 0.021959418430924416\n",
      "La similitud entre el documento 6114nsa03 y el documento tft es sim 0.018405554816126823\n",
      "La similitud entre el documento 6114nsa03 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.020757999271154404\n",
      "La similitud entre el documento 9-12 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.17416615784168243\n",
      "La similitud entre el documento 9-12 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.01916787400841713\n",
      "La similitud entre el documento 9-12 y el documento hir-22-351 es sim 0.03820112347602844\n",
      "La similitud entre el documento 9-12 y el documento How good are deep models in understanding generated images es sim 0.01600266620516777\n",
      "La similitud entre el documento 9-12 y el documento IJISRT23AUG773 es sim 0.07064438611268997\n",
      "La similitud entre el documento 9-12 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.003781010629609227\n",
      "La similitud entre el documento 9-12 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.010812017135322094\n",
      "La similitud entre el documento 9-12 y el documento Paper11879 es sim 0.02174542471766472\n",
      "La similitud entre el documento 9-12 y el documento Sketch Based Image Retrieval for Architecture es sim 0.013656055554747581\n",
      "La similitud entre el documento 9-12 y el documento tft es sim 0.006192849949002266\n",
      "La similitud entre el documento 9-12 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.011944621801376343\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.0037038419395685196\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento hir-22-351 es sim 0.03148350864648819\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento How good are deep models in understanding generated images es sim 0.013025185093283653\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento IJISRT23AUG773 es sim 0.02443832717835903\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.0004679235862568021\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.007593429647386074\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento Paper11879 es sim 0.0154190082103014\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento Sketch Based Image Retrieval for Architecture es sim 0.003859858959913254\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento tft es sim 0.00550801632925868\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.0032684968318790197\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento hir-22-351 es sim 0.023405812680721283\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento How good are deep models in understanding generated images es sim 0.07100421190261841\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento IJISRT23AUG773 es sim 0.01482588704675436\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.03696475178003311\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.11737371981143951\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento Paper11879 es sim 0.01523938961327076\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento Sketch Based Image Retrieval for Architecture es sim 0.10228697955608368\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento tft es sim 0.05087042599916458\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.02162054181098938\n",
      "La similitud entre el documento hir-22-351 y el documento How good are deep models in understanding generated images es sim 0.03646470978856087\n",
      "La similitud entre el documento hir-22-351 y el documento IJISRT23AUG773 es sim 0.009465666487812996\n",
      "La similitud entre el documento hir-22-351 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.007337093353271484\n",
      "La similitud entre el documento hir-22-351 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.00865382980555296\n",
      "La similitud entre el documento hir-22-351 y el documento Paper11879 es sim 0.027707340195775032\n",
      "La similitud entre el documento hir-22-351 y el documento Sketch Based Image Retrieval for Architecture es sim 0.02287200465798378\n",
      "La similitud entre el documento hir-22-351 y el documento tft es sim 0.0570279024541378\n",
      "La similitud entre el documento hir-22-351 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.01187829952687025\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento IJISRT23AUG773 es sim 0.009096469730138779\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.03595055267214775\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.03399570286273956\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento Paper11879 es sim 0.010394366458058357\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento Sketch Based Image Retrieval for Architecture es sim 0.06761106848716736\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento tft es sim 0.025596335530281067\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.05745764821767807\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.014719258062541485\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.0217756237834692\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento Paper11879 es sim 0.17953866720199585\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento Sketch Based Image Retrieval for Architecture es sim 0.02510959655046463\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento tft es sim 0.01855628192424774\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.016351375728845596\n",
      "La similitud entre el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.018322788178920746\n",
      "La similitud entre el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH y el documento Paper11879 es sim 0.005842684768140316\n",
      "La similitud entre el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH y el documento Sketch Based Image Retrieval for Architecture es sim 0.061263177543878555\n",
      "La similitud entre el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH y el documento tft es sim 0.020919833332300186\n",
      "La similitud entre el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.020112192258238792\n",
      "La similitud entre el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL y el documento Paper11879 es sim 0.03254666551947594\n",
      "La similitud entre el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL y el documento Sketch Based Image Retrieval for Architecture es sim 0.12098342180252075\n",
      "La similitud entre el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL y el documento tft es sim 0.09914323687553406\n",
      "La similitud entre el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.03338383138179779\n",
      "La similitud entre el documento Paper11879 y el documento Sketch Based Image Retrieval for Architecture es sim 0.010553169995546341\n",
      "La similitud entre el documento Paper11879 y el documento tft es sim 0.015866180881857872\n",
      "La similitud entre el documento Paper11879 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.013481883332133293\n",
      "La similitud entre el documento Sketch Based Image Retrieval for Architecture y el documento tft es sim 0.0406905896961689\n",
      "La similitud entre el documento Sketch Based Image Retrieval for Architecture y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.02430581860244274\n",
      "La similitud entre el documento tft y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.016678886488080025\n"
     ]
    }
   ],
   "source": [
    "textos = [resumen.split() for resumen in abstracts.values()]\n",
    "\n",
    "diccionario = corpora.Dictionary(textos)\n",
    "\n",
    "corpus = [diccionario.doc2bow(texto) for texto in textos]\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "index = similarities.MatrixSimilarity(tfidf[corpus])\n",
    "\n",
    "for i in range(len(textos)):\n",
    "    for j in range(i + 1, len(textos)):\n",
    "        vec_i = diccionario.doc2bow(textos[i])\n",
    "        vec_j = diccionario.doc2bow(textos[j])\n",
    "        sim_ij = index[tfidf[vec_i]][j]\n",
    "        print(\n",
    "            f\"La similitud entre el documento {list(abstracts.keys())[i]} y el documento {list(abstracts.keys())[j]} es sim {sim_ij}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicov\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La similitud entre el documento 11621ijccsa02 y el documento 1709.01907 es sim 0.18909263610839844\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 1802.05799 es sim 0.32246485352516174\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 2007.03051 es sim 0.38316184282302856\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 208 es sim 0.37224164605140686\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.6741066575050354\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 6114nsa03 es sim 0.7781059741973877\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 9-12 es sim 0.4558578133583069\n",
      "La similitud entre el documento 11621ijccsa02 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.33682113885879517\n",
      "La similitud entre el documento 11621ijccsa02 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.12774041295051575\n",
      "La similitud entre el documento 11621ijccsa02 y el documento hir-22-351 es sim 0.3331562876701355\n",
      "La similitud entre el documento 11621ijccsa02 y el documento How good are deep models in understanding generated images es sim 0.15266136825084686\n",
      "La similitud entre el documento 11621ijccsa02 y el documento IJISRT23AUG773 es sim 0.7527927160263062\n",
      "La similitud entre el documento 11621ijccsa02 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.1456475555896759\n",
      "La similitud entre el documento 11621ijccsa02 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.18402864038944244\n",
      "La similitud entre el documento 11621ijccsa02 y el documento Paper11879 es sim 0.6483190059661865\n",
      "La similitud entre el documento 11621ijccsa02 y el documento Sketch Based Image Retrieval for Architecture es sim 0.10967686772346497\n",
      "La similitud entre el documento 11621ijccsa02 y el documento tft es sim 0.28413596749305725\n",
      "La similitud entre el documento 11621ijccsa02 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.06570909917354584\n",
      "La similitud entre el documento 1709.01907 y el documento 1802.05799 es sim 0.2755492627620697\n",
      "La similitud entre el documento 1709.01907 y el documento 2007.03051 es sim 0.3994024693965912\n",
      "La similitud entre el documento 1709.01907 y el documento 208 es sim 0.37621599435806274\n",
      "La similitud entre el documento 1709.01907 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.24646547436714172\n",
      "La similitud entre el documento 1709.01907 y el documento 6114nsa03 es sim 0.23073741793632507\n",
      "La similitud entre el documento 1709.01907 y el documento 9-12 es sim 0.33437538146972656\n",
      "La similitud entre el documento 1709.01907 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.32342636585235596\n",
      "La similitud entre el documento 1709.01907 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.12864874303340912\n",
      "La similitud entre el documento 1709.01907 y el documento hir-22-351 es sim 0.216549351811409\n",
      "La similitud entre el documento 1709.01907 y el documento How good are deep models in understanding generated images es sim 0.3624846041202545\n",
      "La similitud entre el documento 1709.01907 y el documento IJISRT23AUG773 es sim 0.19442200660705566\n",
      "La similitud entre el documento 1709.01907 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.23184871673583984\n",
      "La similitud entre el documento 1709.01907 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.2294052243232727\n",
      "La similitud entre el documento 1709.01907 y el documento Paper11879 es sim 0.1346801519393921\n",
      "La similitud entre el documento 1709.01907 y el documento Sketch Based Image Retrieval for Architecture es sim 0.12338881939649582\n",
      "La similitud entre el documento 1709.01907 y el documento tft es sim 0.553356945514679\n",
      "La similitud entre el documento 1709.01907 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.17936937510967255\n",
      "La similitud entre el documento 1802.05799 y el documento 2007.03051 es sim 0.6693546772003174\n",
      "La similitud entre el documento 1802.05799 y el documento 208 es sim 0.2649080753326416\n",
      "La similitud entre el documento 1802.05799 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.3053826093673706\n",
      "La similitud entre el documento 1802.05799 y el documento 6114nsa03 es sim 0.3603825569152832\n",
      "La similitud entre el documento 1802.05799 y el documento 9-12 es sim 0.2970978915691376\n",
      "La similitud entre el documento 1802.05799 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.31215962767601013\n",
      "La similitud entre el documento 1802.05799 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.250278502702713\n",
      "La similitud entre el documento 1802.05799 y el documento hir-22-351 es sim 0.6281082630157471\n",
      "La similitud entre el documento 1802.05799 y el documento How good are deep models in understanding generated images es sim 0.4335462152957916\n",
      "La similitud entre el documento 1802.05799 y el documento IJISRT23AUG773 es sim 0.3815722167491913\n",
      "La similitud entre el documento 1802.05799 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.4376979172229767\n",
      "La similitud entre el documento 1802.05799 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.5399653315544128\n",
      "La similitud entre el documento 1802.05799 y el documento Paper11879 es sim 0.33296236395835876\n",
      "La similitud entre el documento 1802.05799 y el documento Sketch Based Image Retrieval for Architecture es sim 0.2827311158180237\n",
      "La similitud entre el documento 1802.05799 y el documento tft es sim 0.510079562664032\n",
      "La similitud entre el documento 1802.05799 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.33160972595214844\n",
      "La similitud entre el documento 2007.03051 y el documento 208 es sim 0.24511383473873138\n",
      "La similitud entre el documento 2007.03051 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.3801521956920624\n",
      "La similitud entre el documento 2007.03051 y el documento 6114nsa03 es sim 0.36494770646095276\n",
      "La similitud entre el documento 2007.03051 y el documento 9-12 es sim 0.2623075544834137\n",
      "La similitud entre el documento 2007.03051 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.2590760588645935\n",
      "La similitud entre el documento 2007.03051 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.3153303265571594\n",
      "La similitud entre el documento 2007.03051 y el documento hir-22-351 es sim 0.6317008137702942\n",
      "La similitud entre el documento 2007.03051 y el documento How good are deep models in understanding generated images es sim 0.41396549344062805\n",
      "La similitud entre el documento 2007.03051 y el documento IJISRT23AUG773 es sim 0.45828789472579956\n",
      "La similitud entre el documento 2007.03051 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.3403204381465912\n",
      "La similitud entre el documento 2007.03051 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.4715039134025574\n",
      "La similitud entre el documento 2007.03051 y el documento Paper11879 es sim 0.32300013303756714\n",
      "La similitud entre el documento 2007.03051 y el documento Sketch Based Image Retrieval for Architecture es sim 0.2688371539115906\n",
      "La similitud entre el documento 2007.03051 y el documento tft es sim 0.4855729341506958\n",
      "La similitud entre el documento 2007.03051 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.34031447768211365\n",
      "La similitud entre el documento 208 y el documento 269 An Insight into Cloud Computing Paradigm and Services es sim 0.2677820920944214\n",
      "La similitud entre el documento 208 y el documento 6114nsa03 es sim 0.2853899300098419\n",
      "La similitud entre el documento 208 y el documento 9-12 es sim 0.8266420960426331\n",
      "La similitud entre el documento 208 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.7309472560882568\n",
      "La similitud entre el documento 208 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.17645123600959778\n",
      "La similitud entre el documento 208 y el documento hir-22-351 es sim 0.37329426407814026\n",
      "La similitud entre el documento 208 y el documento How good are deep models in understanding generated images es sim 0.35981225967407227\n",
      "La similitud entre el documento 208 y el documento IJISRT23AUG773 es sim 0.260785311460495\n",
      "La similitud entre el documento 208 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.302123486995697\n",
      "La similitud entre el documento 208 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.24082818627357483\n",
      "La similitud entre el documento 208 y el documento Paper11879 es sim 0.13024507462978363\n",
      "La similitud entre el documento 208 y el documento Sketch Based Image Retrieval for Architecture es sim 0.24326066672801971\n",
      "La similitud entre el documento 208 y el documento tft es sim 0.22323386371135712\n",
      "La similitud entre el documento 208 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.2501963675022125\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento 6114nsa03 es sim 0.7596637606620789\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento 9-12 es sim 0.2191670835018158\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento 907-Article Text-2692-1-10-20230720 es sim 0.2560056447982788\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.17794449627399445\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento hir-22-351 es sim 0.2832280397415161\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento How good are deep models in understanding generated images es sim 0.22979529201984406\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento IJISRT23AUG773 es sim 0.7465469241142273\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.18407735228538513\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.15734514594078064\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento Paper11879 es sim 0.6723699569702148\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento Sketch Based Image Retrieval for Architecture es sim 0.16976290941238403\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento tft es sim 0.2084035575389862\n",
      "La similitud entre el documento 269 An Insight into Cloud Computing Paradigm and Services y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.053897857666015625\n",
      "La similitud entre el documento 6114nsa03 y el documento 9-12 es sim 0.35044583678245544\n",
      "La similitud entre el documento 6114nsa03 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.27910086512565613\n",
      "La similitud entre el documento 6114nsa03 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.16635407507419586\n",
      "La similitud entre el documento 6114nsa03 y el documento hir-22-351 es sim 0.28499943017959595\n",
      "La similitud entre el documento 6114nsa03 y el documento How good are deep models in understanding generated images es sim 0.21973727643489838\n",
      "La similitud entre el documento 6114nsa03 y el documento IJISRT23AUG773 es sim 0.7518483400344849\n",
      "La similitud entre el documento 6114nsa03 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.21076659858226776\n",
      "La similitud entre el documento 6114nsa03 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.29329341650009155\n",
      "La similitud entre el documento 6114nsa03 y el documento Paper11879 es sim 0.6843827366828918\n",
      "La similitud entre el documento 6114nsa03 y el documento Sketch Based Image Retrieval for Architecture es sim 0.14922329783439636\n",
      "La similitud entre el documento 6114nsa03 y el documento tft es sim 0.3397369384765625\n",
      "La similitud entre el documento 6114nsa03 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.11993280053138733\n",
      "La similitud entre el documento 9-12 y el documento 907-Article Text-2692-1-10-20230720 es sim 0.6839179992675781\n",
      "La similitud entre el documento 9-12 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.16600526869297028\n",
      "La similitud entre el documento 9-12 y el documento hir-22-351 es sim 0.321656197309494\n",
      "La similitud entre el documento 9-12 y el documento How good are deep models in understanding generated images es sim 0.3282428979873657\n",
      "La similitud entre el documento 9-12 y el documento IJISRT23AUG773 es sim 0.28672948479652405\n",
      "La similitud entre el documento 9-12 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.28060492873191833\n",
      "La similitud entre el documento 9-12 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.2955491244792938\n",
      "La similitud entre el documento 9-12 y el documento Paper11879 es sim 0.2357879877090454\n",
      "La similitud entre el documento 9-12 y el documento Sketch Based Image Retrieval for Architecture es sim 0.20081590116024017\n",
      "La similitud entre el documento 9-12 y el documento tft es sim 0.2606315314769745\n",
      "La similitud entre el documento 9-12 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.2612053155899048\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING es sim 0.21054378151893616\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento hir-22-351 es sim 0.3262462913990021\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento How good are deep models in understanding generated images es sim 0.39080843329429626\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento IJISRT23AUG773 es sim 0.3021806478500366\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.282321035861969\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.25177040696144104\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento Paper11879 es sim 0.21264180541038513\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento Sketch Based Image Retrieval for Architecture es sim 0.17785252630710602\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento tft es sim 0.17851263284683228\n",
      "La similitud entre el documento 907-Article Text-2692-1-10-20230720 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.23986057937145233\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento hir-22-351 es sim 0.2841953933238983\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento How good are deep models in understanding generated images es sim 0.5150306820869446\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento IJISRT23AUG773 es sim 0.23573637008666992\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.3489418923854828\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.5181499123573303\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento Paper11879 es sim 0.218425914645195\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento Sketch Based Image Retrieval for Architecture es sim 0.7249884009361267\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento tft es sim 0.21481262147426605\n",
      "La similitud entre el documento CBIR USING FEATURES DERIVED BY DEEP LEARNING y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.3885040283203125\n",
      "La similitud entre el documento hir-22-351 y el documento How good are deep models in understanding generated images es sim 0.3902369737625122\n",
      "La similitud entre el documento hir-22-351 y el documento IJISRT23AUG773 es sim 0.34636732935905457\n",
      "La similitud entre el documento hir-22-351 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.2951207756996155\n",
      "La similitud entre el documento hir-22-351 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.4414534866809845\n",
      "La similitud entre el documento hir-22-351 y el documento Paper11879 es sim 0.21835173666477203\n",
      "La similitud entre el documento hir-22-351 y el documento Sketch Based Image Retrieval for Architecture es sim 0.3250139355659485\n",
      "La similitud entre el documento hir-22-351 y el documento tft es sim 0.3738747239112854\n",
      "La similitud entre el documento hir-22-351 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.3174748420715332\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento IJISRT23AUG773 es sim 0.27232301235198975\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.5921612977981567\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.5644641518592834\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento Paper11879 es sim 0.19140082597732544\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento Sketch Based Image Retrieval for Architecture es sim 0.385023295879364\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento tft es sim 0.42662671208381653\n",
      "La similitud entre el documento How good are deep models in understanding generated images y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.4612431228160858\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH es sim 0.18167239427566528\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.2176433950662613\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento Paper11879 es sim 0.7260582447052002\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento Sketch Based Image Retrieval for Architecture es sim 0.1511768400669098\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento tft es sim 0.24682484567165375\n",
      "La similitud entre el documento IJISRT23AUG773 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.10521510988473892\n",
      "La similitud entre el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH y el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL es sim 0.4798661768436432\n",
      "La similitud entre el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH y el documento Paper11879 es sim 0.13599684834480286\n",
      "La similitud entre el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH y el documento Sketch Based Image Retrieval for Architecture es sim 0.38138043880462646\n",
      "La similitud entre el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH y el documento tft es sim 0.446127325296402\n",
      "La similitud entre el documento IMAGEBERT CROSS-MODAL PRE-TRAINING WITH y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.5999336242675781\n",
      "La similitud entre el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL y el documento Paper11879 es sim 0.25640764832496643\n",
      "La similitud entre el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL y el documento Sketch Based Image Retrieval for Architecture es sim 0.40793949365615845\n",
      "La similitud entre el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL y el documento tft es sim 0.4325717091560364\n",
      "La similitud entre el documento LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.44570109248161316\n",
      "La similitud entre el documento Paper11879 y el documento Sketch Based Image Retrieval for Architecture es sim 0.1834026724100113\n",
      "La similitud entre el documento Paper11879 y el documento tft es sim 0.1834719181060791\n",
      "La similitud entre el documento Paper11879 y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.04340558499097824\n",
      "La similitud entre el documento Sketch Based Image Retrieval for Architecture y el documento tft es sim 0.23592932522296906\n",
      "La similitud entre el documento Sketch Based Image Retrieval for Architecture y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.40446603298187256\n",
      "La similitud entre el documento tft y el documento VISION TRANSFORMERS NEED REGISTERS es sim 0.3464566469192505\n"
     ]
    }
   ],
   "source": [
    "# USING TRANSFORMERS\n",
    "\n",
    "# If we want to improve the similarity and use a word embeddings approach, we may use sentence transformers. This may take a while:\n",
    "sbert_model = SentenceTransformer(\n",
    "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")\n",
    "sentence_embeddings = sbert_model.encode(list(abstracts.values()))\n",
    "for i in range(len(sentence_embeddings)):\n",
    "    for j in range(i + 1, len(sentence_embeddings)):\n",
    "        sim = cosine(sentence_embeddings[i], sentence_embeddings[j])\n",
    "        print(\n",
    "            f\"La similitud entre el documento {list(abstracts.keys())[i]} y el documento {list(abstracts.keys())[j]} es sim {sim}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import pandas as pd\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(list(abstracts.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_matrix = cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             document  cluster\n",
      "0   With recent advances in technology, internet h...        0\n",
      "1   Reliable uncertainty estimation for time serie...        1\n",
      "2   Training modern deep learning models requires ...        0\n",
      "3   Deep learning (DL) can achieve impressive resu...        0\n",
      "4   Artificial Intelligence (AI), sometimes called...        0\n",
      "5   Cloud computing is a computing model which pro...        0\n",
      "6   Cloud computing has formed the conceptual and ...        0\n",
      "7   Artificial Intelligence is making a machine be...        0\n",
      "8   As artificial intelligence (AI) technology bec...        0\n",
      "9   In a Content Based Image Retrieval (CBIR) Syst...        1\n",
      "10  Deep learning is a form of machine learning th...        0\n",
      "11  My goal in this paper is twofold: to study how...        1\n",
      "12  Cloud computing has revolutionized the way bus...        0\n",
      "13  In this paper, we introduce a new vision-langu...        1\n",
      "14  Methods that combine local and global features...        1\n",
      "15  Cloud computing has had a significant impact o...        0\n",
      "16  Sketch-based image retrieval (SBIR) is an imag...        1\n",
      "17  Multi-horizon forecasting often contains a com...        1\n",
      "18  Transformers have recently emerged as a powerf...        1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicov\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters=2, affinity='cosine', linkage='complete')\n",
    "labels = clustering.fit_predict(cos_sim_matrix)\n",
    "\n",
    "#kmeans = KMeans(n_clusters=3, init='random', n_init=10, max_iter=300)\n",
    "#labels = kmeans.fit_predict(cos_sim_matrix)\n",
    "\n",
    "#dbscan = DBSCAN(eps=0.1, min_samples=2, metric='precomputed')\n",
    "#labels = dbscan.fit_predict(cos_sim_matrix)\n",
    "\n",
    "# print the clusters\n",
    "df = pd.DataFrame({'document': list(abstracts.values()), 'cluster': labels})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=2, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=2, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(n_components=2, random_state=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's do a countvectorizer now. This is different from TF-IDF\n",
    "count_vectorizer = CountVectorizer()\n",
    "X = count_vectorizer.fit_transform(list(abstracts.values()))\n",
    "# we are only creating 2 topics\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=0)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "the of and to on\n",
      "Topic 1:\n",
      "the and cloud computing of\n"
     ]
    }
   ],
   "source": [
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "for topic_id, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_id}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-6:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic distribution for 11621ijccsa02:\n",
      "Topic 0: 0.0045\n",
      "Topic 1: 0.9955\n",
      "Topic distribution for 1709.01907:\n",
      "Topic 0: 0.9952\n",
      "Topic 1: 0.0048\n",
      "Topic distribution for 1802.05799:\n",
      "Topic 0: 0.9937\n",
      "Topic 1: 0.0063\n",
      "Topic distribution for 2007.03051:\n",
      "Topic 0: 0.9608\n",
      "Topic 1: 0.0392\n",
      "Topic distribution for 208:\n",
      "Topic 0: 0.0037\n",
      "Topic 1: 0.9963\n",
      "Topic distribution for 269 An Insight into Cloud Computing Paradigm and Services:\n",
      "Topic 0: 0.0108\n",
      "Topic 1: 0.9892\n",
      "Topic distribution for 6114nsa03:\n",
      "Topic 0: 0.0036\n",
      "Topic 1: 0.9964\n",
      "Topic distribution for 9-12:\n",
      "Topic 0: 0.0041\n",
      "Topic 1: 0.9959\n",
      "Topic distribution for 907-Article Text-2692-1-10-20230720:\n",
      "Topic 0: 0.0062\n",
      "Topic 1: 0.9938\n",
      "Topic distribution for CBIR USING FEATURES DERIVED BY DEEP LEARNING:\n",
      "Topic 0: 0.9962\n",
      "Topic 1: 0.0038\n",
      "Topic distribution for hir-22-351:\n",
      "Topic 0: 0.9913\n",
      "Topic 1: 0.0087\n",
      "Topic distribution for How good are deep models in understanding generated images:\n",
      "Topic 0: 0.9964\n",
      "Topic 1: 0.0036\n",
      "Topic distribution for IJISRT23AUG773:\n",
      "Topic 0: 0.0035\n",
      "Topic 1: 0.9965\n",
      "Topic distribution for IMAGEBERT CROSS-MODAL PRE-TRAINING WITH:\n",
      "Topic 0: 0.9944\n",
      "Topic 1: 0.0056\n",
      "Topic distribution for LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL:\n",
      "Topic 0: 0.9972\n",
      "Topic 1: 0.0028\n",
      "Topic distribution for Paper11879:\n",
      "Topic 0: 0.0025\n",
      "Topic 1: 0.9975\n",
      "Topic distribution for Sketch Based Image Retrieval for Architecture:\n",
      "Topic 0: 0.9958\n",
      "Topic 1: 0.0042\n",
      "Topic distribution for tft:\n",
      "Topic 0: 0.9965\n",
      "Topic 1: 0.0035\n",
      "Topic distribution for VISION TRANSFORMERS NEED REGISTERS:\n",
      "Topic 0: 0.9954\n",
      "Topic 1: 0.0046\n"
     ]
    }
   ],
   "source": [
    "#Now let's see the probability of one of the sentence to belong to each topic\n",
    "\n",
    "for name, abstract in abstracts.items():\n",
    "    new_doc_bow = count_vectorizer.transform([abstract])\n",
    "# Compute the topic distribution for the new document\n",
    "    topic_distribution = lda.transform(new_doc_bow)\n",
    "    print(f\"Topic distribution for {name}:\")\n",
    "    for topic_idx, topic_prob in enumerate(topic_distribution[0]):\n",
    "        print(f\"Topic {topic_idx}: {topic_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "956"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99538326, 0.00461674])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_distribution[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence score: -0.66\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.models import LdaMulticore\n",
    "preprocessed_documents = []\n",
    "for document in abstracts.values():\n",
    "    tokens = vectorizer.get_feature_names_out()\n",
    "    preprocessed_documents.append(tokens)\n",
    "\n",
    "#print(tokens)\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(preprocessed_documents)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]\n",
    "\n",
    "lda_model = gensim.models.LdaModel(corpus=corpus, num_topics=2, id2word=dictionary, passes=10)\n",
    "coherence_model = gensim.models.CoherenceModel(model=lda_model, texts=preprocessed_documents, dictionary=dictionary, coherence='c_npmi')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence score: {coherence_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.001*\"becomes\" + 0.001*\"estimation\" + 0.001*\"recurrent\" + 0.001*\"fully\" + 0.001*\"background\" + 0.001*\"computations\" + 0.001*\"much\" + 0.001*\"instantly\" + 0.001*\"without\" + 0.001*\"conduct\"\n",
      "Topic 1: 0.002*\"outperform\" + 0.001*\"resulted\" + 0.001*\"collected\" + 0.001*\"dependencies\" + 0.001*\"adoption\" + 0.001*\"taken\" + 0.001*\"digitalized\" + 0.001*\"similarity\" + 0.001*\"tasks\" + 0.001*\"testing\"\n"
     ]
    }
   ],
   "source": [
    "for topic_id, topic_words in lda_model.print_topics(num_words=10):\n",
    "    print(f\"Topic {topic_id}: {topic_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.054*\"cloud\" + 0.048*\"computing\" + 0.020*\"big\" + 0.016*\"based\" + 0.016*\"will\"')\n",
      "(1, '0.031*\"computing\" + 0.031*\"cloud\" + 0.022*\"training\" + 0.014*\"significant\" + 0.009*\"may\"')\n",
      "(2, '0.040*\"artificial\" + 0.040*\"intelligence\" + 0.025*\"cloud\" + 0.020*\"human\" + 0.020*\"computing\"')\n",
      "(3, '0.024*\"series\" + 0.020*\"time\" + 0.014*\"deep\" + 0.014*\"layers\" + 0.010*\"models\"')\n",
      "(4, '0.025*\"visual\" + 0.019*\"maps\" + 0.013*\"solution\" + 0.013*\"tokens\" + 0.013*\"supervised\"')\n",
      "(5, '0.029*\"features\" + 0.017*\"images\" + 0.012*\"large\" + 0.012*\"propose\" + 0.012*\"results\"')\n",
      "(6, '0.028*\"model\" + 0.021*\"generated\" + 0.018*\"image\" + 0.018*\"images\" + 0.018*\"object\"')\n"
     ]
    }
   ],
   "source": [
    "# TOPIC MODELLING\n",
    "\n",
    "stop_words = get_stop_words(\"english\")\n",
    "keywords = [\n",
    "    [\n",
    "        word\n",
    "        for word in resumen.lower().split()\n",
    "        if word.isalpha() and word not in stop_words\n",
    "    ]\n",
    "    for resumen in abstracts.values()\n",
    "]\n",
    "dictionary = corpora.Dictionary(keywords)\n",
    "doc_term_matrix = [dictionary.doc2bow(title) for title in keywords]\n",
    "\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "lda_model = LDA(\n",
    "    corpus=doc_term_matrix,\n",
    "    id2word=dictionary,\n",
    "    num_topics=7,\n",
    "    random_state=100,\n",
    "    chunksize=1000,\n",
    "    passes=50,\n",
    ")\n",
    "\n",
    "temas = lda_model.print_topics(num_words=5)\n",
    "for tema in temas:\n",
    "    print(tema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prueba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
