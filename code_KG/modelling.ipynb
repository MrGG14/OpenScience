{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from grobid_client.grobid_client import GrobidClient\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "import os\n",
    "from stop_words import get_stop_words\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN\n",
    "import numpy as np\n",
    "# Now let's do topic modeling using LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "# si no hace bien los imports de utilsdescomenta esta linea\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils import remove_files, get_abstract, cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET ABSTRACTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorio donde se encuentran los archivos XML\n",
    "xml_dir = os.path.join(parent_dir, \"output\")\n",
    "# remove_files(xml_dir)\n",
    "\n",
    "# client = GrobidClient(config_path=\"code_KG/config.json\")\n",
    "# client.process(\"processFulltextDocument\", \"./papers\", output=\"./output/\", consolidate_citations=True, tei_coordinates=True, n=20)\n",
    "\n",
    "# Lista para almacenar los resúmenes\n",
    "abstracts = []\n",
    "\n",
    "# Procesar cada archivo XML en el directorio\n",
    "for file in os.listdir(xml_dir):\n",
    "    if file.endswith(\".xml\"):  # Verificar que el archivo sea XML\n",
    "        file_path = os.path.join(xml_dir, file)\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        abstract = get_abstract(root)\n",
    "        abstracts.append(abstract)\n",
    "\n",
    "# print(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMILARITY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La similitud entre el documento 1 y el documento 2 es sim 0.002790855709463358\n",
      "La similitud entre el documento 1 y el documento 3 es sim 0.028237465769052505\n",
      "La similitud entre el documento 1 y el documento 4 es sim 0.00998421385884285\n",
      "La similitud entre el documento 2 y el documento 3 es sim 0.07665908336639404\n",
      "La similitud entre el documento 2 y el documento 4 es sim 0.02041800133883953\n",
      "La similitud entre el documento 3 y el documento 4 es sim 0.019381240010261536\n"
     ]
    }
   ],
   "source": [
    "textos = [resumen.split() for resumen in abstracts]\n",
    "\n",
    "diccionario = corpora.Dictionary(textos)\n",
    "\n",
    "corpus = [diccionario.doc2bow(texto) for texto in textos]\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "index = similarities.MatrixSimilarity(tfidf[corpus])\n",
    "\n",
    "for i in range(len(textos)):\n",
    "    for j in range(i + 1, len(textos)):\n",
    "        vec_i = diccionario.doc2bow(textos[i])\n",
    "        vec_j = diccionario.doc2bow(textos[j])\n",
    "        sim_ij = index[tfidf[vec_i]][j]\n",
    "        print(\n",
    "            f\"La similitud entre el documento {i+1} y el documento {j+1} es sim {sim_ij}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La similitud entre el documento 1 y el documento 2 es sim 0.2755492925643921\n",
      "La similitud entre el documento 1 y el documento 3 es sim 0.3994024991989136\n",
      "La similitud entre el documento 1 y el documento 4 es sim 0.21654930710792542\n",
      "La similitud entre el documento 2 y el documento 3 es sim 0.6693546772003174\n",
      "La similitud entre el documento 2 y el documento 4 es sim 0.6281082630157471\n",
      "La similitud entre el documento 3 y el documento 4 es sim 0.6317006945610046\n"
     ]
    }
   ],
   "source": [
    "# USING TRANSFORMERS\n",
    "\n",
    "# If we want to improve the similarity and use a word embeddings approach, we may use sentence transformers. This may take a while:\n",
    "sbert_model = SentenceTransformer(\n",
    "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")\n",
    "sentence_embeddings = sbert_model.encode(abstracts)\n",
    "for i in range(len(sentence_embeddings)):\n",
    "    for j in range(i + 1, len(sentence_embeddings)):\n",
    "        sim = cosine(sentence_embeddings[i], sentence_embeddings[j])\n",
    "        print(\n",
    "            f\"La similitud entre el documento {i+1} y el documento {j+1} es sim {sim}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import pandas as pd\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(abstracts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_matrix = cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            document  cluster\n",
      "0  Reliable uncertainty estimation for time serie...        1\n",
      "1  Training modern deep learning models requires ...        0\n",
      "2  Deep learning (DL) can achieve impressive resu...        0\n",
      "3  Deep learning is a form of machine learning th...        0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nvegamun\\AppData\\Local\\anaconda3\\envs\\prueba\\lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters=2, affinity='cosine', linkage='complete')\n",
    "labels = clustering.fit_predict(cos_sim_matrix)\n",
    "\n",
    "#kmeans = KMeans(n_clusters=3, init='random', n_init=10, max_iter=300)\n",
    "#labels = kmeans.fit_predict(cos_sim_matrix)\n",
    "\n",
    "#dbscan = DBSCAN(eps=0.1, min_samples=2, metric='precomputed')\n",
    "#labels = dbscan.fit_predict(cos_sim_matrix)\n",
    "\n",
    "# print the clusters\n",
    "df = pd.DataFrame({'document': abstracts, 'cluster': labels})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=5, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=5, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(n_components=5, random_state=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's do a countvectorizer now. This is different from TF-IDF\n",
    "count_vectorizer = CountVectorizer()\n",
    "X = count_vectorizer.fit_transform(abstracts)\n",
    "# we are only creating 2 topics\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "deep and the to of\n",
      "Topic 1:\n",
      "the training communication gpu to\n",
      "Topic 2:\n",
      "time series of to for\n",
      "Topic 3:\n",
      "of the to computer deep\n",
      "Topic 4:\n",
      "and this energy of in\n"
     ]
    }
   ],
   "source": [
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "for topic_id, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_id}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-6:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic distribution for the new document:\n",
      "Topic 0: 0.05\n",
      "Topic 1: 0.05\n",
      "Topic 2: 0.36\n",
      "Topic 3: 0.05\n",
      "Topic 4: 0.49\n"
     ]
    }
   ],
   "source": [
    "#Now let's see the probability of one of the sentence to belong to each topic\n",
    "new_doc_bow = count_vectorizer.transform([\"This abstract deals with semantic web and eScience\"])\n",
    "#new_doc_bow = vectorizer.transform([\"This other paper is represents scientific advances in fruits. Specifically tomatoes and strawberries\"])\n",
    "\n",
    "# Compute the topic distribution for the new document\n",
    "topic_distribution = lda.transform(new_doc_bow)\n",
    "\n",
    "print(\"Topic distribution for the new document:\")\n",
    "for topic_idx, topic_prob in enumerate(topic_distribution[0]):\n",
    "    print(f\"Topic {topic_idx}: {topic_prob:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence score: -0.65\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.models import LdaMulticore\n",
    "preprocessed_documents = []\n",
    "for document in abstracts:\n",
    "    tokens = vectorizer.get_feature_names_out()\n",
    "    preprocessed_documents.append(tokens)\n",
    "\n",
    "#print(tokens)\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(preprocessed_documents)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]\n",
    "\n",
    "lda_model = gensim.models.LdaModel(corpus=corpus, num_topics=2, id2word=dictionary, passes=10)\n",
    "coherence_model = gensim.models.CoherenceModel(model=lda_model, texts=preprocessed_documents, dictionary=dictionary, coherence='c_npmi')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence score: {coherence_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.004*\"their\" + 0.004*\"real\" + 0.004*\"overhead\" + 0.004*\"present\" + 0.004*\"library\" + 0.004*\"entail\" + 0.004*\"terms\" + 0.004*\"form\" + 0.004*\"years\" + 0.004*\"to\"\n",
      "Topic 1: 0.004*\"of\" + 0.004*\"learning\" + 0.004*\"intensive\" + 0.004*\"probabilistic\" + 0.004*\"need\" + 0.004*\"solution\" + 0.004*\"millions\" + 0.004*\"series\" + 0.004*\"concepts\" + 0.004*\"specialized\"\n"
     ]
    }
   ],
   "source": [
    "for topic_id, topic_words in lda_model.print_topics(num_words=10):\n",
    "    print(f\"Topic {topic_id}: {topic_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.006*\"series\" + 0.006*\"time\" + 0.006*\"uncertainty\" + 0.006*\"models\" + 0.006*\"prediction\"')\n",
      "(1, '0.006*\"series\" + 0.006*\"time\" + 0.006*\"prediction\" + 0.006*\"anomaly\" + 0.006*\"used\"')\n",
      "(2, '0.006*\"time\" + 0.006*\"training\" + 0.006*\"series\" + 0.006*\"models\" + 0.006*\"deep\"')\n",
      "(3, '0.053*\"series\" + 0.053*\"time\" + 0.032*\"prediction\" + 0.032*\"uncertainty\" + 0.022*\"models\"')\n",
      "(4, '0.031*\"training\" + 0.031*\"carbon\" + 0.031*\"energy\" + 0.021*\"deep\" + 0.021*\"may\"')\n",
      "(5, '0.006*\"training\" + 0.006*\"series\" + 0.006*\"time\" + 0.006*\"models\" + 0.006*\"often\"')\n",
      "(6, '0.043*\"training\" + 0.026*\"deep\" + 0.026*\"computer\" + 0.026*\"learning\" + 0.018*\"significant\"')\n"
     ]
    }
   ],
   "source": [
    "# TOPIC MODELLING\n",
    "\n",
    "stop_words = get_stop_words(\"english\")\n",
    "keywords = [\n",
    "    [\n",
    "        word\n",
    "        for word in resumen.lower().split()\n",
    "        if word.isalpha() and word not in stop_words\n",
    "    ]\n",
    "    for resumen in abstracts\n",
    "]\n",
    "dictionary = corpora.Dictionary(keywords)\n",
    "doc_term_matrix = [dictionary.doc2bow(title) for title in keywords]\n",
    "\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "lda_model = LDA(\n",
    "    corpus=doc_term_matrix,\n",
    "    id2word=dictionary,\n",
    "    num_topics=7,\n",
    "    random_state=100,\n",
    "    chunksize=1000,\n",
    "    passes=50,\n",
    ")\n",
    "\n",
    "temas = lda_model.print_topics(num_words=5)\n",
    "for tema in temas:\n",
    "    print(tema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prueba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
