<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,119.65,99.57,372.70,14.93;1,105.21,119.50,402.13,14.93">IMAGEBERT: CROSS-MODAL PRE-TRAINING WITH LARGE-SCALE WEAK-SUPERVISED IMAGE-TEXT DATA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-01-23">23 Jan 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,175.42,182.62,20.76,8.64"><forename type="first">Di</forename><surname>Qi</surname></persName>
							<email>diqi@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Bing Multimedia Team</orgName>
								<address>
									<region>Microsoft</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,202.82,182.62,25.00,8.64"><forename type="first">Lin</forename><surname>Su</surname></persName>
							<email>lins@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Bing Multimedia Team</orgName>
								<address>
									<region>Microsoft</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,234.65,182.62,31.94,8.64"><forename type="first">Jia</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bing Multimedia Team</orgName>
								<address>
									<region>Microsoft</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.67,182.62,46.04,8.64"><forename type="first">Edward</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bing Multimedia Team</orgName>
								<address>
									<region>Microsoft</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,326.42,182.62,53.95,8.64"><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
							<email>tbharti@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Bing Multimedia Team</orgName>
								<address>
									<region>Microsoft</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,386.78,182.62,52.29,8.64"><forename type="first">Arun</forename><surname>Sacheti</surname></persName>
							<email>aruns@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Bing Multimedia Team</orgName>
								<address>
									<region>Microsoft</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,119.65,99.57,372.70,14.93;1,105.21,119.50,402.13,14.93">IMAGEBERT: CROSS-MODAL PRE-TRAINING WITH LARGE-SCALE WEAK-SUPERVISED IMAGE-TEXT DATA</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-01-23">23 Jan 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">FF286389D42D1D99F32FAFDB2879C775</idno>
					<idno type="arXiv">arXiv:2001.07966v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-06T16:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce a new vision-language pre-trained model -ImageBERT -for image-text joint embedding. Our model is a Transformer[1]-based model, which takes different modalities as input and models the relationship between them. The model is pre-trained on four tasks simultaneously: Masked Language Modeling (MLM), Masked Object Classification (MOC), Masked Region Feature Regression (MRFR), and Image Text Matching (ITM). To further enhance the pre-training quality, we have collected a Large-scale weAk-supervised Image-Text (LAIT) dataset from Web. We first pre-train the model on this dataset, then conduct a second stage pre-training on Conceptual Captions[2] and SBU Captions <ref type="bibr" coords="1,233.92,356.85,12.61,8.64" target="#b2">[3]</ref>. Our experiments show that multi-stage pre-training strategy outperforms single-stage pre-training. We also fine-tune and evaluate our pre-trained ImageBERT model on image retrieval and text retrieval[4] tasks, and achieve new state-of-the-art results on both MSCOCO[5] and Flickr30k[6]  datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, vision-language tasks have attracted a lot of attention in both natural language processing (NLP) and computer vision (CV) communities. For example, Text-Image Retrieval <ref type="bibr" coords="1,311.55,461.53,11.70,8.64" target="#b3">[4]</ref> aims to retrieve the most relevant image given a text, or vice versa. Visual Question Answering (VQA) <ref type="bibr" coords="1,253.93,472.44,14.87,8.64" target="#b6">[7]</ref> aims to predict the correct answer given an image and an associated question. Visual Commonsense Reasoning (VCR) <ref type="bibr" coords="1,264.53,483.35,14.27,8.64" target="#b7">[8]</ref> requires the model can not only answer the commonsense question but also select a rationale to support the answer. Image Captioning <ref type="bibr" coords="1,336.54,494.26,12.93,8.64" target="#b8">[9]</ref> aims to generate a natural language description for each input image. Based on pre-trained models trained by language and vision tasks separately (such as BERT <ref type="bibr" coords="1,519.38,505.17,20.62,8.64" target="#b9">[10]</ref> for language tasks and ResNet <ref type="bibr" coords="1,195.70,516.08,18.66,8.64" target="#b10">[11]</ref> for vision tasks), most previous methods used a late fusion manner to fuse the multi-modal inputs for downstream tasks. However, such late fusion layers usually require task-specific labeled data in training, but for many multi-modal tasks, acquiring enough task annotations is still very challenging and expensive.</p><p>Inspired by the success of pre-trained models in NLP, such as BERT <ref type="bibr" coords="1,339.02,554.28,19.54,8.64" target="#b9">[10]</ref>, XLNet <ref type="bibr" coords="1,390.53,554.28,19.67,8.64" target="#b11">[12]</ref> and RoBERTa <ref type="bibr" coords="1,464.12,554.28,19.74,8.64" target="#b12">[13]</ref>, cross-modal pre-training has become a heated research area. Such models can learn joint representations for language and vision contents in the early stage based on large-scale corpus and then be applied to downstream tasks by task-specific fine-tuning. In this paper, We will first review the latest work on cross-modal pre-training and compare their similarities and differences. Then, ImageBERT is proposed as a strong baseline for cross-modal pre-training, which has achieved new state-of-the-art results on text-to-image and image-to-text retrieval tasks on MSCOCO <ref type="bibr" coords="1,429.17,608.83,17.94,8.64" target="#b4">[5]</ref> and Flicker30k <ref type="bibr" coords="1,507.46,608.83,12.29,8.64" target="#b5">[6]</ref>. We also build a new corpus, which includes 10M text-image pairs mined from web. We hope this corpus can further advance the development of cross-modal pre-training research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>After Transformer <ref type="bibr" coords="1,142.79,691.70,13.02,8.64" target="#b0">[1]</ref> was proposed and widely used by cross-modal researches, the results on various tasks have been pushed to a new Everest in recent one year. Though almost all latest work are based on Transformer, they differ in various ways. We will review these work from different dimensions in below.</p><p>• Model architecture. BERT <ref type="bibr" coords="2,218.35,75.48,21.05,8.64" target="#b9">[10]</ref> model is pre-trained for NLP tasks whose input is one or two sentences.</p><p>To apply BERT structure to cross-modal tasks, there can be many ways to deal with different modalities.</p><p>ViLBERT <ref type="bibr" coords="2,143.11,97.30,20.35,8.64" target="#b13">[14]</ref> and LXMERT <ref type="bibr" coords="2,216.05,97.30,22.40,8.64" target="#b14">[15]</ref> applied a single-modal Transformer to image and sentence respectively, then combined the two modalities together with a cross-modal Transformer. Other work, such as VisualBERT <ref type="bibr" coords="2,518.32,108.20,18.34,8.64" target="#b15">[16]</ref>, B2T2 <ref type="bibr" coords="2,126.72,119.11,18.85,8.64" target="#b16">[17]</ref>, Unicoder-VL <ref type="bibr" coords="2,203.70,119.11,18.45,8.64" target="#b17">[18]</ref>, VL-BERT <ref type="bibr" coords="2,265.35,119.11,20.53,8.64" target="#b18">[19]</ref>, Unified VLP <ref type="bibr" coords="2,341.21,119.11,19.23,8.64" target="#b19">[20]</ref>, UNITER <ref type="bibr" coords="2,398.74,119.11,20.56,8.64" target="#b20">[21]</ref>, etc., all concatenated image and sentence as a single input to the Transformer. It is hard to argue which model structure is better, since its performance really depends on the specific scenario.</p><p>• Image visual tokens. Almost all recent paper applied an object detection model to the images and treated the detected regions of interest (RoIs) as image descriptors, just as linguistic tokens. Different from other work which used a pre-trained detection model, VL-BERT trained the detection network together with its image-text joint embedding network, and it also added global image features into model training. We can see region-based image features are good image descriptors, and they form a sequence of visual tokens that can be directly fed into Transformer.</p><p>• Pre-train data. Unlike language model pre-training that can leverage tremendous natural language data, vision-language tasks require high quality image descriptions that are hard to obtain for free. Conceptual Captions <ref type="bibr" coords="2,142.05,250.09,12.82,8.64" target="#b1">[2]</ref> is the most widely used data for image-text pre-training, given that it has 3M image descriptions and is relatively larger than other datasets. UNITER <ref type="bibr" coords="2,305.66,261.00,20.90,8.64" target="#b20">[21]</ref> combines four datasets (Conceptual Captions <ref type="bibr" coords="2,502.59,261.00,12.16,8.64" target="#b1">[2]</ref>, SBU Captions <ref type="bibr" coords="2,141.48,271.91,12.61,8.64" target="#b2">[3]</ref>, Visual Genome <ref type="bibr" coords="2,220.45,271.91,20.46,8.64" target="#b21">[22]</ref> and MSCOCO <ref type="bibr" coords="2,295.88,271.91,17.41,8.64" target="#b4">[5]</ref>) together to form a 9.6M training corpus and achieved state-of-the-art results on many image-text cross-modal tasks. LXMERT <ref type="bibr" coords="2,390.79,282.82,22.78,8.64" target="#b14">[15]</ref> added some VQA training data into pre-training and obtained state-of-the-art results on VQA task. We can see that data quality and volume play important roles in model training, and should be paid more attention to when designing new models.</p><p>3 Large-Scale Weak-supervised Image-Text Data Collection</p><p>Unlike language model based pre-training, which can use unlimited natural language texts, such as BooksCorpus <ref type="bibr" coords="2,511.03,358.84,18.57,8.64" target="#b22">[23]</ref> or Wikipedia, cross-modal pre-training requires large volume and high quality vision-language pairs. For example, most recent cross-modal pre-trained models <ref type="bibr" coords="2,225.14,380.66,15.67,8.64" target="#b15">[16,</ref><ref type="bibr" coords="2,243.29,380.66,12.41,8.64" target="#b16">17,</ref><ref type="bibr" coords="2,258.19,380.66,12.41,8.64" target="#b17">18,</ref><ref type="bibr" coords="2,273.09,380.66,12.41,8.64" target="#b18">19,</ref><ref type="bibr" coords="2,287.99,380.66,12.41,8.64" target="#b19">20,</ref><ref type="bibr" coords="2,302.89,380.66,13.23,8.64" target="#b20">21]</ref> use following 2 datasets in pre-training: The Conceptual Captions (CC) dataset <ref type="bibr" coords="2,160.56,391.57,11.59,8.64" target="#b1">[2]</ref>, which contains 3M images with descriptions harvested from the Alt-text HTML attribute of the web pages, and SBU Captions <ref type="bibr" coords="2,219.52,402.48,12.61,8.64" target="#b2">[3]</ref>, which consists of 1M images with user-associated captions. However, the size of these datasets is still not enough for pre-training a model which has several hundreds of million parameters, or even larger models in the future. In addition, image descriptions manually written by human can be of high-quality yet expensive. But there are innumerable web-pages on the Internet with associated images.</p><p>Motivated by this, this paper designs a weak-supervised approach (as illustrated in Figure <ref type="figure" coords="2,443.85,451.60,4.23,8.64" target="#fig_0">1</ref>) to collect large-scale image-text data from Web, whose volume and quality are essential to vision-language pre-train tasks. The resulting dataset LAIT (Large-scale weAk-supervised Image-Text) contains 10M images along with their descriptions with an average length of 13 words. We will show in experiments that LAIT is beneficial for vision-language pre-training. Figure <ref type="figure" coords="2,100.50,495.23,4.98,8.64" target="#fig_2">2</ref> gives some examples. We will explain the data collection methodology in below.</p><p>Web-page Collection. We crawl billions of web-pages from Web and discard all non-English ones, given that all downstream tasks are in English. And then we parse each web-page to collect image URLs and detect dominant images by HTML tags and DOM tree features. Non-dominant images are discarded because they are more likely to be unrelated to the web-page content.</p><p>Image Content Based Filtering. We further filter the data based on image content. We only keep images whose width and height are both larger than 300 pixels. Also, images that contain pornographic or racy content are also discarded. Besides, given that the images in downstream tasks are all natural, realistic pictures taken from the real world, we apply a binary classifier to discard unnatural, non-realistic and non-learn-able images. Figure <ref type="figure" coords="2,419.43,593.46,4.96,8.64" target="#fig_3">3</ref> shows some examples of the non-eligible images that have been discarded during this process.</p><p>Sentence Detection &amp; Cleaning. We use below data sources as textual descriptions of the images: user-defined metadata in the HTML such as Alt or Title attribute, surrounding text of the image, etc.; we make a sequence of heuristic rules to filter out bad spans and noisy words (spam/pornography) in the sentences, and just keep sentences within a normal length. Finally we discard sentences that have high ratio of out-of-vocabulary terms.</p><p>Image-Text Semantic Scoring. After filtering bad images and cleaning up noisy text, we want to make sure the text and image are semantically relevant. With small supervised image-text data, we have trained a weak image-text semantic model to predict whether the &lt; text, image &gt; pair is semantically relevant or not, and then apply it to the billion scale image-text pairs to filter out irrelevant pairs. The semantic model is trained upon hundreds of features including text-only features, image-content features, and text-image cross-modal features.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ImageBERT Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Embedding Modeling</head><p>We first introduce how we prepare the input for the Transformer through embedding layers.</p><p>Linguistic embedding. We adopt the similar word pre-prossessing method as BERT. The input sentence is tokenized into n sub-word tokens {w 0 , • • • , w n-1 } using WordPiece <ref type="bibr" coords="4,302.88,607.55,18.45,8.64" target="#b25">[26]</ref> approach. Special tokens such as [CLS] and [SEP] are also added to the tokenized text sequence. The final embedding for each sub-word token is generated by combining its original word embedding, segment embedding and sequence position embedding (which will explained in details later). All these embeddings are initialized from public pre-trained BERT model.</p><p>Image embedding. Similar to linguistic embedding, image embedding is also generated from visual input by a similar process. A Faster-RCNN model is used to extract features from o RoIs, denoted by {r 0 , • • • , r o-1 }, from the image to represent its visual content. The detected objects can not only provide visual contexts of the whole image for the linguistic part, but also be related to specific terms through detailed region information. We also add position embeddings to image embeddings by encoding the object location with respect to the global image into a 5-D vector c (i) = ( x tl W , y tl H , x br W , y br H , (x br -x tl )(y br -y tl ) W H</p><p>), where (x tl , y tl ) and (x br , y br ) denote top-left and bottom-right ...  denotes the proportional area with respect to the whole image. Both object features and location embeddings are projected into the same dimension with linguistic embeddings. The final representation e (i) for each image RoI r (i) is obtained via summing up its object embedding v (i) = ImageEmbed(r (i) ), segment embedding s (i) = SegmentEmbed(i), image postion embedding p (i) img = P ositionEmbed(c (i) ) and sequence position embedding p (i) seq = P ositionEmbed(i):</p><formula xml:id="formula_0" coords="5,229.56,409.13,146.12,14.07">e (i) = LN (v (i) + s (i) + p (i) img + p (i)</formula><p>seq ) Each embedding is projected to a vector with the same embedding size as the hidden size in Transformer sub-layers, followed by Layer Normalization (LN). We also use the classification label of each region from the detection model in a label prediction task (which will be explained in section 4.3). During our ablation study, we also experiment on adding global image features in addition to the region features.</p><p>Sequence position and segment embedding. Sequence position embedding for each token is used for indicating the order of the input tokens. We use a fixed dummy position for all the visual tokens, because there is no order of the detected RoIs, and the coordinates of the objects have already been added into the image embeddings. For the linguistic part, an ascending sequence is used to indicate the order of words in the textual description. Moreover, segment embedding is added to each input token to distinguish different modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-stage Pre-training</head><p>Since different datasets are collected from different sources, they may have different quality levels and noise distributions. To better utilize different kinds of pre-train data, we propose a multi-stage pre-training framework, as shown in Figure <ref type="figure" coords="5,72.00,588.04,3.81,8.64" target="#fig_7">5</ref>. According to downstream tasks, the pre-trained model should be trained firstly using large-scale out-of-domain data followed by small-scale in-domain data, so that the model can be better converged towards the final tasks. In multi-stage pre-training, several pre-train stages (k+2 stages, for example, in Figure <ref type="figure" coords="5,413.37,609.85,4.23,8.64" target="#fig_7">5</ref>) could be applied to the same network structure to utilize different kinds of dataset sequentially. Unlike training strategy mentioned in <ref type="bibr" coords="5,495.94,620.76,16.73,8.64" target="#b26">[27]</ref> which only has a single-stage in language model pre-training, language model fine-tuning and classifier fine-tuning separately, our multi-stage framework mainly applies to the pre-training stage so as to better utilize heterogeneous out-of-domain datasets. Another work <ref type="bibr" coords="5,169.29,653.49,16.73,8.64" target="#b27">[28]</ref> which also mentioned the multi-stage concept used it to solve optimization problem in feature learning, which is very different from our multi-stage strategy here.  During ablation study, we also experiment on different fine-tuning objectives for image-text retrieval tasks. We will introduce our pre-train tasks as well as fine-tune tasks in below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pre-training tasks</head><p>During model pre-training, we design four tasks to model the linguistic information and visual content, as well as their interactions.</p><p>Task 1: Masked Language Modeling (MLM). This task is the same with the MLM task in BERT <ref type="bibr" coords="6,466.59,363.21,20.88,8.64" target="#b9">[10]</ref> training. We denote the n input sub-word tokens as w = {w 0 , • • • , w n-1 }. The input token which will be predicted afterwards is masked randomly with a probability of 15%. We denote the masked indices as m T ∈ N M and the masked word as w m T , and the non-masked word as w \m T . The masked token is replaced with a special token [MASK], a random token or remains unchanged with a probability of 80%, 10%, 10%, respectively. The prediction is made based on the surrounding text as well as the cross-attention between the textual tokens and the visual features, which are denoted as v = {v 0 , • • • , v o-1 }, by minimizing the negative log-likelihood:</p><p>L M LM (θ) = -E (v,w)∼D log P θ (w m T |w \m T , v) where θ denotes the model's parameters which are trainable and will be updated during training. The is calculated over all (v, w) pairs in the training corpus D. The similar expression is omitted in sections below.</p><p>Task 2: Masked Object Classification (MOC). This task is an expansion of the MLM task. Similar to language modeling, we also conduct masked modeling on the visual object tokens. We randomly mask each object token with a probability of 15%, then zero out the masked token or keep the original token with a probability of 90% and 10%, respectively. We denote the masked indices of the object tokens as m I ∈ N M , the masked token as v m I , and non-masked token as v \m I . The i th of the M masked input tokens is denoted as v (i) m I . To predict the M masked object tokens, we treat the labeled classification categories from Faster R-CNN model as ground truth labels l θ (v (i) m I ). Suppose the output vector corresponding to the masked token from the Transformer is f θ (v (i) m I ) ∈ R, we add a fully-connected layer to predict the correct label from K object classes, using the cross-entropy (CE) loss as final objective with the context of the linguistic features w:</p><formula xml:id="formula_1" coords="6,198.59,595.14,214.81,30.32">L M OC (θ) = -E (v,w)∼D M -1 i=0 CE(l θ (v (i) m I ), f θ (v (i) m I ))</formula><p>Task 3: Masked Region Feature Regression (MRFR). Similar to MOC, MRFR also models the visual content, but it does a more precise job on object feature prediction. This task aims to regress the embedding feature of each masked object, denoted as h θ (v</p><formula xml:id="formula_2" coords="6,164.22,656.12,17.90,13.07">(i) m I ).</formula><p>We add a fully-connected layer on top of the output feature vector to project it to the same dimension with the pooled input RoI object feature, denoted as r θ (v (i) m I ). Then an L2 loss is applied to regress the ground truth feature:</p><formula xml:id="formula_3" coords="6,196.04,695.01,219.43,30.32">L M RF R (θ) = -E (v,w)∼D M -1 i=0 h θ (v (i) m I ) -r θ (v (i) m I ) 2</formula><p>Note that we use conditional mask for all of the above three tasks, which means we only calculate the all mask losses when the input image and text is related.</p><p>Task 4: Image-Text Matching (ITM). In addition to the language modeling task and the visual content modeling tasks, we also add ITM task to learn the image-text alignment. For each training sample, We randomly sample negative sentences for each image and randomly sample negative images for each sentence, to generate negative training data. Therefore, we denote the ground truth label as y ∈ {0, 1} for each image-text pair (v, w) , indicating if the input sample pair is related or not. Similar to BERT, we add [CLS] as the first token of the input sequence to ImageBERT model, and apply a fully-connected layer on top it to obtain the image-text similarity score s θ (v, w). A binary classification loss is used for optimization:</p><formula xml:id="formula_4" coords="7,168.67,188.38,274.67,10.03">L IT M (θ) = -E (v,w)∼D [y log s θ (v, w) + (1 -y) log (1 -s θ (v, w))]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Fine-tuning tasks</head><p>After pre-training, we get a well-trained pre-trained model on vision-language joint representation. We further fine-tune and evaluate this model on Image-Text Retrieval task. This task contains two sub-tasks: image retrieval and text retrieval.</p><p>Image retrieval targets to retrieve the correct images given an input caption sentence describing the content of image.</p><p>Text retrieval does a similar task in the opposite direction. We fine-tune on MSCOCO and Flickr30k dataset after a two-stage pre-training. During fine-tuning, the format of the input sequence is the same with that in pre-training but without any masks on the object or word. We propose two fine-tuning objectives corresponding to different negative sampling methods: image-to-text (sample negative sentences for each image) and text-to-image (sample negative images for each text). Besides, we experiment on three different losses to get the best model quality:</p><p>• Binary classification Loss. This is to guarantee the prediction for negative samples is correct: the output scores of the negatives samples should not only be different from the positive sample but also be predicted with right labels. For a image-text pair (v, w) with ground truth label y ∈ 0, 1, we take the embedding of the first token t (v,w) output from the Transformer as c θ (t (v,w) ) ∈ R, then apply the binary classification loss for optimization:</p><formula xml:id="formula_5" coords="7,188.53,393.64,270.81,9.96">L BCE (θ) = -E (v,w) [y log c θ (t (v,w) ) + (1 -y) log (1 -c θ (t (v,w) ))]</formula><p>• Multi-class Classification Loss. This is the most widely-used loss to enlarge the margins between positive and negative samples. For each positive pair (v, w) + , we sample P -1 negative pairs (v, w) -from different captions or images. Then add a scoring function to predict the correct label l (v,w) for all P pairs' first tokens t</p><p>(v,w) , and apply the cross-entropy loss as final objective:</p><formula xml:id="formula_7" coords="7,233.77,476.16,180.32,30.32">L CE (θ) = -E<label>(j) (v,w) P -1 j=0 CE(s(t (j) (v,w) ), l (j) (v,w) )</label></formula><p>• Triplet Loss. We use triplet loss to maximize the margin between positive and the hardest negative sample.</p><p>As mentioned in multi-class classification loss, for each positive pair (v, w) + , we sample P -1 negative pairs (v, w) -from different captions or images with calculated similarity scores s(t (v,w) -). The hardest negative sample is given by n - h = arg max (v,w) j =(v,w) + s(t (v,w) j ). Then we apply triplet loss to the positive sample and the hardest negative sample:</p><formula xml:id="formula_8" coords="7,213.23,588.53,221.40,23.75">L T riplet (θ) = -E (j) (v,w) n -∈N max[0, s(t (v,w) + ), s(n - h )]</formula><p>We conduct ablation study on combinations of the above-mentioned three fine-tuning losses, and we will present experimental results later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>For the Image-Text retrieval task, we present zero-shot result for evaluating the quality of the pre-trained model as well as the results after further fine-tuning. We compare our model with state-of-the-art methods on image-retrieval and text-retrieval tasks in different settings on MSCOCO <ref type="bibr" coords="7,275.68,713.51,17.90,8.64" target="#b4">[5]</ref> and Flickr30k <ref type="bibr" coords="7,350.72,713.51,12.59,8.64" target="#b5">[6]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flickr30k</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSCOCO Image Retrieval</head><p>Sentence Retrieval Image Retrieval Sentence Retrieval R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 1k Test set SCAN <ref type="bibr" coords="9,100.87,126.57,19.43,7.77" target="#b28">[29]</ref> 48 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Studies</head><p>We also perform ablation experiments on different combinations of pre-train datasets, presence of global visual features, different training tasks, etc., on Flickr3k, so as to study deeply on our model structure and training strategy.</p><p>Pre-train dataset. We conduct pre-train experiments using combinations of different datasets. Results are shown in Table <ref type="table" coords="9,96.65,500.71,3.81,8.64" target="#tab_2">3</ref>. CC stands for pre-training only on Conceptual Captions dataset, SBU stands for pre-training only on SBU Captions, LAIT+CC+SBU stands for pre-training using LAIT, Conceptual Caption dataset and SBU combined dataset, LAIT → CC+SBU stands for pre-training using LAIT as stage-1, then continue pre-training using Conceptual Captions and SBU Captions as stage-2. We can see that using three different out-of-domain datasets in a multi-stage way achieves significantly better results than all the other settings.</p><p>Global image features. It is worth noticing that the detected RoIs may not include all the information of the whole image. Thus, we also try to add global image features to the visual part. We use three different Convolutional Neural Networks (CNN) models (DenseNet <ref type="bibr" coords="9,211.36,582.56,17.33,8.64" target="#b31">[32]</ref>, Resnet <ref type="bibr" coords="9,261.26,582.56,17.22,8.64" target="#b10">[11]</ref> and GoogleNet <ref type="bibr" coords="9,337.58,582.56,17.87,8.64" target="#b32">[33]</ref>) to extract global visual features from an input image, but find not all of the metrics have improvements. Results can be seen in part 1 of Table <ref type="table" coords="9,453.64,593.46,3.74,8.64" target="#tab_3">4</ref>.</p><p>Pre-train loss. We also add MRFR loss which is inspired by UNITER <ref type="bibr" coords="9,361.88,609.85,21.59,8.64" target="#b20">[21]</ref> to our pre-training, and achieve huge improvement on zero-shot results as shown in part 2 of Table <ref type="table" coords="9,329.48,620.76,3.81,8.64" target="#tab_3">4</ref>. This implies that adding a harder task for better modeling the visual content can contribute to visual-textual joint learning.</p><p>Number of objects (RoIs) from image. To understand the importance of the visual part in our model, we perform experiments on different numbers of objects. We use the setting of 100 objects in all the experiments above to provide enough context of the input image for the pre-training tasks. In our model, the 100 RoIs are extracted using a Faster R-CNN model to obtain the top-100 ranked objects ordered by confidence scores from the detection network. Since some bounding boxes of the objects may have overlaps with each other or contain duplicate information, we also conduct experiments to see the impact of different numbers of objects. As we can see in part 3 of Table <ref type="table" coords="9,493.97,702.61,3.81,8.64" target="#tab_3">4</ref>, with less objects (same number of objects with ViLBERT <ref type="bibr" coords="9,254.48,713.51,19.74,8.64" target="#b13">[14]</ref>) our model gets no better results on retrieval task. We can conclude</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Image Retrieval Sentence Retrieval R@1 R@5 R@10 R@1 R@5 R@10 Pre-train zero-shot result that more objects indeed can help the model achieve better results, because more RoIs are helpful for understanding the image content.</p><p>Fine-tune loss. For the three losses we mentioned in section 4.4, we try different combinations of them during fine-tuning. As shown in part 4 of Table <ref type="table" coords="10,237.34,352.94,3.81,8.64" target="#tab_3">4</ref>, using binary cross-entropy loss itself gives the best fine-tuned results on image-text retrieval task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we presented a new vision-language pre-trained model, ImageBERT, which is based on Transformer architecture and models vision-language joint embedding. We also collected a large-scale image-text training corpus LAIT from Web using weakly-supervised methods, which has the largest volume among current existing visionlanguage datasets, and has demonstrated its effectiveness in the first stage of our multi-stage pre-training pipeline.</p><p>We can see that large-scale out-of-domain data, though lack of precise human labels, can add value to the quality of the pre-trained model and consequentially benefit the corresponding downstream tasks. Our ImageBERT model has achieved new state-of-the-art results on both image retrieval and sentence retrieval tasks on MSCOCO and Flickr30k.</p><p>In the future, we will try to extend our pre-trained model to other cross-modal tasks such as VQA, VCR, and image captioning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,200.54,590.37,210.92,8.64;3,118.78,72.00,374.44,510.24"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Weakly-supervised data collection pipeline</figDesc><graphic coords="3,118.78,72.00,374.44,510.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,72.00,702.61,468.00,8.64;3,72.00,713.51,468.00,8.64"><head>Figure 4</head><label>4</label><figDesc>Figure 4 illustrates the overall architecture of our ImageBERT model. Similar to BERT[10], we use Transformer as basic structure, but take both image visual tokens and textual tokens as input. The image and text input are encoded into</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,219.79,217.71,172.42,8.64;4,95.40,72.00,421.20,137.59"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Data samples from LAIT dataset.</figDesc><graphic coords="4,95.40,72.00,421.20,137.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,127.27,477.71,357.46,8.64;4,95.40,240.64,421.20,228.94"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of non-eligible images that have been discarded during data cleaning.</figDesc><graphic coords="4,95.40,240.64,421.20,228.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="5,214.97,310.20,182.07,8.64"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Architecture of ImageBERT model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="5,72.00,680.79,468.00,8.64;5,72.00,691.70,468.00,8.64;5,72.00,702.61,468.17,8.64;5,72.00,713.51,469.25,8.64"><head></head><label></label><figDesc>More specifically, in our ImageBERT model, we use a two-stage pre-training strategy. The first pre-training stage uses our LAIT dataset as mentioned in section 3, and the second stage uses other public datasets, such as Conceptual Captions and SBU Captions. Both pre-training stages use the same training strategy which includes all of our four pre-training tasks. We also conduct experiments on single-stage pre-training, which trains on all datasets simultaneously,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="6,214.43,223.87,183.14,8.64;6,72.00,72.00,468.01,143.74"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Multi-stage pre-training framework.</figDesc><graphic coords="6,72.00,72.00,468.01,143.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,81.44,126.57,445.36,264.27"><head>Table 2 :</head><label>2</label><figDesc>Results of fine-tuned model on Flickr30k and MSCOCO test sets.</figDesc><table coords="9,81.44,126.57,445.36,264.27"><row><cell></cell><cell>.6</cell><cell>77.7</cell><cell>85.2</cell><cell>67.4</cell><cell>90.3</cell><cell>95.8</cell><cell>58.8</cell><cell>88.4</cell><cell>94.8</cell><cell>72.7</cell><cell>94.8</cell><cell>98.4</cell></row><row><cell>SCG[30]</cell><cell>49.3</cell><cell>76.4</cell><cell>85.6</cell><cell>71.8</cell><cell>90.8</cell><cell>94.8</cell><cell>61.4</cell><cell>88.9</cell><cell>95.1</cell><cell>76.6</cell><cell>96.3</cell><cell>99.2</cell></row><row><cell>PFAN[31]</cell><cell>50.4</cell><cell>78.7</cell><cell>86.1</cell><cell>70.0</cell><cell>91.8</cell><cell>95.0</cell><cell>61.6</cell><cell>89.6</cell><cell>95.2</cell><cell>76.5</cell><cell>96.3</cell><cell>99.0</cell></row><row><cell>ViLBERT[14]</cell><cell>58.2</cell><cell>84.9</cell><cell>91.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>UNITER[21]</cell><cell>71.5</cell><cell>91.2</cell><cell>95.2</cell><cell>84.7</cell><cell>97.1</cell><cell>99.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Unicoder-VL[18]</cell><cell>71.5</cell><cell>90.9</cell><cell>94.9</cell><cell>86.2</cell><cell>96.3</cell><cell>99.0</cell><cell>69.7</cell><cell>93.5</cell><cell>97.2</cell><cell>84.3</cell><cell>97.3</cell><cell>99.3</cell></row><row><cell>ImageBERT</cell><cell>73.1</cell><cell>92.6</cell><cell>96.0</cell><cell>87.0</cell><cell>97.6</cell><cell>99.2</cell><cell>73.6</cell><cell>94.3</cell><cell>97.2</cell><cell>85.4</cell><cell>98.7</cell><cell>99.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">5k Test set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SCAN[29]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>38.6</cell><cell>69.3</cell><cell>80.4</cell><cell>50.4</cell><cell>82.2</cell><cell>90.0</cell></row><row><cell>Unicoder-VL[18]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>46.7</cell><cell>76.0</cell><cell>85.3</cell><cell>62.3</cell><cell>87.1</cell><cell>92.8</cell></row><row><cell>UNITER[21]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>48.4</cell><cell>76.7</cell><cell>85.9</cell><cell>63.3</cell><cell>87.0</cell><cell>93.1</cell></row><row><cell>ImageBERT</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>50.5</cell><cell>78.7</cell><cell>87.1</cell><cell>66.4</cell><cell>89.8</cell><cell>94.4</cell></row><row><cell></cell><cell cols="2">Version</cell><cell></cell><cell cols="7">Image Retrieval R@1 R@5 R@10 R@1 R@5 R@10 Sentence Retrieval</cell><cell></cell><cell></cell></row><row><cell></cell><cell>LAIT</cell><cell></cell><cell></cell><cell>21.4</cell><cell>46.0</cell><cell>59.1</cell><cell>31.6</cell><cell>58.4</cell><cell>72.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CC</cell><cell></cell><cell></cell><cell>33.0</cell><cell>60.6</cell><cell>72.5</cell><cell>43.8</cell><cell>73.7</cell><cell>81.7</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">CC+SBU</cell><cell></cell><cell>48.4</cell><cell>76</cell><cell>85.2</cell><cell>64.3</cell><cell>85.8</cell><cell>92.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">LAIT+CC+SBU</cell><cell>46.4</cell><cell>73.7</cell><cell>83.4</cell><cell>63.4</cell><cell>86.8</cell><cell>92.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">LAIT → (CC+SBU) 54.3</cell><cell>79.6</cell><cell>87.5</cell><cell>70.7</cell><cell>90.2</cell><cell>94.0</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,137.28,402.60,337.14,8.64"><head>Table 3 :</head><label>3</label><figDesc>Abalation study on combinations of different datasets on Flickr30k test set.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,71.69,119.07,468.31,169.43"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on global image features, pre-train loss, number of RoIs, and fine-tune loss on Flickr30k test set.</figDesc><table coords="10,139.50,119.07,328.83,138.12"><row><cell>1</cell><cell cols="2">CC+SBU CC+SBU(w global feature) 49.2 48.4</cell><cell>76.0 75.9</cell><cell>85.2 84.9</cell><cell>64.3 64.0</cell><cell>85.8 87.4</cell><cell>92.3 92.4</cell></row><row><cell>2</cell><cell>CC CC(w MRFR loss)</cell><cell>33.0 34.7</cell><cell>60.6 61.7</cell><cell>72.5 73.4</cell><cell>43.8 45.3</cell><cell>73.7 73.8</cell><cell>81.7 84.2</cell></row><row><cell></cell><cell></cell><cell cols="2">Fine-tune result</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3</cell><cell>RoI(36) RoI(100)</cell><cell>68.7 73.0</cell><cell>88.6 92.2</cell><cell>92.3 95.8</cell><cell>84.3 87.3</cell><cell>96.5 98.0</cell><cell>97.9 99.5</cell></row><row><cell></cell><cell>Binary+CE+Triplet</cell><cell>71.9</cell><cell>91.1</cell><cell>94.1</cell><cell>86.6</cell><cell>97.6</cell><cell>98.7</cell></row><row><cell>4</cell><cell>CE only Triplet only</cell><cell>70.6 70.6</cell><cell>91.4 91.4</cell><cell>95.7 95.9</cell><cell>85.6 86.8</cell><cell>97.4 97.3</cell><cell>99.2 98.8</cell></row><row><cell></cell><cell>Binary only</cell><cell>73.1</cell><cell>92.6</cell><cell>96.0</cell><cell>87.0</cell><cell>97.6</cell><cell>99.2</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgement</head><p>This work was done with invaluable help from colleagues from <rs type="institution">Microsoft Research Asia</rs>. We thank <rs type="person">Nan Duan</rs>, <rs type="person">Gen Li</rs>, and <rs type="person">Haoyang Huang</rs> ({nanduan,haohua}@microsoft.com, ligen.li@pku.edu.cn) for their great support and contribution to the source code, training data and this paper. The authors are also grateful to their insightful ideas and helpful discussions on the model design and training.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flickr30k</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSCOCO Image Retrieval</head><p>Sentence Retrieval Image Retrieval Sentence Retrieval R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 1k Test set ViLBERT <ref type="bibr" coords="8,114.04,126.57,18.63,7.77" target="#b13">[14]</ref> 31.9 61.1 72.8 ---------Unicoder-VL <ref type="bibr" coords="8,127.69,136.54,16.82,7.77" target="#b17">[18]</ref> 48. During pre-training, we use a batch size of 48 and a learning rate of 1e-4 with Adam optimizer. We train the model for 17 epochs using 4 V100 GPUs. Also, we use conditional mask in MLM, MOC and MRFR tasks, and only calculate the masked loss when the input pair is a positive sample.</p><p>We use the same evaluation metrics R@K (K = 1, 5, 10) as other work, which measure the percentage of correctly matched pairs in the top K-ranked results. Since both Flickr30k and MSCOCO contain five captions per image, sentence retrieval task is easier and can get higher scores than image retrieval task.</p><p>Zero-shot result of pre-train model. Under this setting, we evaluate our pre-trained model on Flickr30k and MSCOCO test sets without fine-tuning, to measure the quality of the pre-trained model. Zero-shot results are shown in Table <ref type="table" coords="8,534.15,460.63,3.80,8.64">1</ref>.</p><p>We can see our pre-trained model has achieved new state-of-the-art on MSCOCO, but is worse than UNITER model. Note that here we only show the result of stage-2 pre-training, result of stage-1 pre-training can be found in Table <ref type="table" coords="8,534.92,482.44,5.08,8.64">3</ref> which will be explained in our ablation study of the pre-train datasets.</p><p>Besides our model, only a few other works reported the zero-shot results of their model, so we only include the other two works in table <ref type="table" coords="8,129.32,520.65,3.66,8.64">1</ref>. Unicoder-VL <ref type="bibr" coords="8,190.21,520.65,18.36,8.64" target="#b17">[18]</ref> only contains one pre-training stage using public out-of-domain dataset (Conceptual Caption and SBU Captions). UNITER <ref type="bibr" coords="8,221.93,531.56,21.38,8.64" target="#b20">[21]</ref> added partial in-domain datasets (VG and MSCOCO) during pre-training stage, thus getting the highest zero-shot results. But we can also get comparable results after fine-tuning, and we will present them later (refer to table 2). It is also worth noticing that comparing to latest results of other methods which only have one pre-training stage, our multi-stage pre-training strategy learns more useful knowledge during pre-training, and can consequently contribute to the fine-tuning stage on the downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation for the Fine-tuned Model</head><p>Experiment settings. After two-stage pre-training on LAIT and other public datasets (Conceptual Captions and SBU Captions), we apply our well-trained model to downstream task, Image-Text retrieval, and fine-tune it for the ITM task. Experiments are also conducted on both Flickr30k and MSCOCO. During fine-tuning, we use a batch size of 24 and a learning rate of 5e-5, and train for 130 epochs using 4 V100 GPUs.</p><p>Fine-tuned results. The final results after fine-tuning on retrieval task are shown in Table <ref type="table" coords="8,426.49,680.79,3.66,8.64">2</ref>. We can see that our model achieves new state-of-the-art on both Flickr30k and MSCOCO (both on 1k and 5k test sets) and outperforms all the other methods, which proves the effectness of our LAIT data and our multi-stage pre-training strategy for cross-modal joint learning.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,93.58,647.58,446.42,8.64;10,93.58,658.31,378.30,8.82" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m" coord="10,163.04,658.31,165.07,8.82">Attention Is All You Need. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2017-06">Jun 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.58,675.10,447.67,8.64;10,93.58,685.83,274.59,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,359.50,675.10,181.75,8.64;10,93.58,686.00,210.79,8.64">Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning</title>
		<author>
			<persName coords=""><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p18-1238</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,322.80,685.83,15.35,8.59">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.58,702.61,446.42,8.64;10,93.58,713.34,114.55,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,321.81,702.61,218.19,8.64;10,93.58,713.51,47.43,8.64">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName coords=""><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,159.70,713.34,18.82,8.59">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,93.58,75.30,446.42,8.82;11,93.58,86.21,170.75,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName coords=""><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298932</idno>
		<idno type="arXiv">arXiv:1412.2306</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,228.08,75.48,281.75,8.64">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014-12">Dec 2014</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct coords="11,93.58,101.92,448.08,8.64;11,93.58,112.65,447.66,8.82;11,93.22,123.74,40.40,8.64" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m" coord="11,116.54,112.83,262.06,8.64">Microsoft COCO Captions: Data Collection and Evaluation Server</title>
		<imprint>
			<date type="published" when="2015-04">Apr 2015</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct coords="11,93.58,139.28,447.81,8.64;11,93.58,150.01,446.42,8.82;11,93.25,160.92,171.05,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,353.08,139.28,188.31,8.64;11,93.58,150.19,289.17,8.64">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00166</idno>
	</analytic>
	<monogr>
		<title level="j" coord="11,393.75,150.01,146.25,8.59;11,93.25,160.92,104.65,8.59">Transactions of the Association for Computational Linguistics</title>
		<title level="j" type="abbrev">TACL</title>
		<idno type="ISSNe">2307-387X</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014-12">2014</date>
			<publisher>MIT Press - Journals</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,93.58,176.64,446.42,8.64;11,93.58,187.37,377.52,8.82" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00468</idno>
		<title level="m" coord="11,125.52,187.37,197.39,8.82">VQA: Visual Question Answering. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,93.58,203.08,446.42,8.64;11,93.58,213.81,249.46,8.82" xml:id="b7">
	<analytic>
		<title level="a" type="main">From Recognition to Cognition: Visual Commonsense Reasoning</title>
		<author>
			<persName coords=""><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00688</idno>
		<idno type="arXiv">arXiv:1811.10830</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,326.55,203.08,213.45,8.64;11,93.58,213.99,40.10,8.64">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-11">Nov 2018</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct coords="11,93.58,229.53,448.17,8.64;11,93.58,240.26,446.42,8.82;11,93.58,251.35,114.04,8.64" xml:id="b8">
	<analytic>
		<title level="a" type="main">Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00636</idno>
		<idno type="arXiv">arXiv:1707.07998</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,93.58,240.44,359.64,8.64">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">Jul 2017</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct coords="11,93.58,266.89,446.42,8.64;11,93.27,277.62,375.13,8.82" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="11,374.45,266.89,165.55,8.64;11,93.27,277.80,168.47,8.64">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018-10">Oct 2018</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct coords="11,93.58,293.16,446.42,8.82;11,93.41,304.07,377.65,8.82" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,334.03,293.34,178.35,8.64">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,93.41,304.07,282.86,8.59">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2016. 2015</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,93.58,319.78,447.80,8.64;11,93.58,330.51,446.42,8.82;11,93.58,341.60,22.42,8.64" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Xlnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m" coord="11,93.58,330.69,267.95,8.64">Generalized Autoregressive Pretraining for Language Understanding</title>
		<imprint>
			<date type="published" when="2019-06">Jun 2019</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct coords="11,93.58,357.14,446.42,8.64;11,93.58,367.87,446.42,8.82;11,93.58,378.78,171.32,8.82" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="11,295.29,368.05,211.39,8.64">A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019-07">Jul 2019</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct coords="11,93.58,394.50,448.07,8.64;11,93.58,405.23,264.80,8.82" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,306.37,394.50,235.28,8.64;11,93.58,405.40,138.49,8.64">12-in-1: Multi-Task Vision and Language Representation Learning</title>
		<author>
			<persName coords=""><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.01045</idno>
		<idno>ArXiv, abs/1908.02265</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,93.58,420.94,446.42,8.64;11,93.27,431.67,95.48,8.82" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,224.56,420.94,299.64,8.64">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tan</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,93.27,431.67,65.15,8.59">EMNLP/IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,93.58,447.39,446.42,8.64;11,93.58,458.12,300.76,8.82" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="11,433.66,447.39,106.34,8.64;11,93.58,458.30,173.91,8.64">Visualbert: A simple and performant baseline for vision and language</title>
		<author>
			<persName coords=""><forename type="first">Liunian</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cho-Jui</forename><surname>Da Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai-Wei</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Chang</surname></persName>
		</author>
		<idno>ArXiv, abs/1908.03557</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,93.58,473.84,446.42,8.64;11,93.58,484.57,189.55,8.82" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,360.77,473.84,179.22,8.64;11,93.58,484.75,75.35,8.64">Fusion of Detected Objects in Text for Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Reitter</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d19-1219</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,187.65,484.57,65.15,8.59">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,93.58,500.29,446.59,8.64;11,93.58,511.02,422.22,8.82" xml:id="b17">
	<analytic>
		<title level="a" type="main">Unicoder-VL: A Universal Encoder for Vision and Language by Cross-Modal Pre-Training</title>
		<author>
			<persName coords=""><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i07.6795</idno>
		<idno type="arXiv">arXiv:1908.06066</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<title level="j" type="abbrev">AAAI</title>
		<idno type="ISSN">2159-5399</idno>
		<idno type="ISSNe">2374-3468</idno>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">07</biblScope>
			<biblScope unit="page" from="11336" to="11344" />
			<date type="published" when="2019-08">Aug 2019</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct coords="11,93.58,526.73,446.42,8.64;11,93.22,537.46,342.82,8.82" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Vl-Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m" coord="11,449.33,526.73,90.67,8.64;11,93.22,537.64,133.63,8.64">Pre-training of Generic Visual-Linguistic Representations</title>
		<imprint>
			<date type="published" when="2019-08">Aug 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,93.58,553.18,448.08,8.64;11,93.58,563.91,329.81,8.82" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,479.99,553.18,61.66,8.64;11,93.58,564.09,203.01,8.64">Unified Vision-Language Pre-Training for Image Captioning and VQA</title>
		<author>
			<persName coords=""><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i07.7005</idno>
		<idno>ArXiv, abs/1909.11059</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<title level="j" type="abbrev">AAAI</title>
		<idno type="ISSN">2159-5399</idno>
		<idno type="ISSNe">2374-3468</idno>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">07</biblScope>
			<biblScope unit="page" from="13041" to="13049" />
			<date type="published" when="2019">2019</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,93.58,579.63,448.17,8.64;11,93.58,590.36,338.72,8.82" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="11,93.58,590.54,212.21,8.64">UNITER: UNiversal Image-TExt Representation Learning</title>
		<author>
			<persName coords=""><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>El Kholy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58577-8_7</idno>
		<idno>ArXiv, abs/1909.11740</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2020</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,93.58,606.07,446.42,8.64;11,93.58,616.98,446.42,8.64;11,93.58,627.71,447.67,8.82;11,92.83,638.80,70.02,8.64" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="11,424.77,616.98,115.23,8.64;11,93.58,627.89,267.14,8.64">Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations</title>
		<author>
			<persName coords=""><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
			<idno type="ORCID">0000-0001-8784-2531</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-016-0981-7</idno>
	</analytic>
	<monogr>
		<title level="j" coord="11,369.11,627.71,168.17,8.59">International Journal of Computer Vision</title>
		<title level="j" type="abbrev">Int J Comput Vis</title>
		<idno type="ISSN">0920-5691</idno>
		<idno type="ISSNe">1573-1405</idno>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2016">2016</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,93.58,654.34,448.16,8.64;11,93.22,665.25,446.77,8.64;11,93.02,675.98,326.79,8.82" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="11,93.22,665.25,430.36,8.64">Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books</title>
		<author>
			<persName coords=""><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2015.11</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,93.02,675.98,253.91,8.59">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-12">December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,93.58,691.70,446.42,8.64;11,93.58,702.43,446.42,8.82;11,93.27,713.34,107.03,8.82" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="11,138.76,702.61,134.93,8.64">Towards VQA Models That Can Read</title>
		<author>
			<persName coords=""><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00851</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,292.11,702.43,247.89,8.59;11,93.27,713.34,77.91,8.59">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.58,75.48,447.67,8.64;12,93.58,86.21,447.67,8.82;12,93.33,97.30,79.50,8.64" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="12,218.42,86.39,193.70,8.64">Towards VQA Models That Can Read</title>
		<author>
			<persName coords=""><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00851</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,430.47,86.21,66.85,8.59">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.58,112.19,446.42,8.64;12,93.58,123.10,446.42,8.64;12,93.58,134.01,446.42,8.64;12,93.58,144.92,446.42,8.64;12,93.58,155.83,446.42,8.64;12,93.58,166.56,287.72,8.82" xml:id="b25">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m" coord="12,384.93,144.92,155.07,8.64;12,93.58,155.83,446.42,8.64;12,93.58,166.73,80.91,8.64">Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google&apos;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</title>
		<meeting><address><addrLine>Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09">Sep 2016</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct coords="12,93.58,181.45,448.14,8.82" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="12,245.38,181.63,233.41,8.64">Universal Language Model Fine-tuning for Text Classification</title>
		<author>
			<persName coords=""><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p18-1031</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,496.70,181.45,15.22,8.59">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.58,196.34,446.42,8.82;12,93.58,207.25,215.13,8.82" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="12,299.33,196.52,155.15,8.64">Multi-stage multi-task feature learning</title>
		<author>
			<persName coords=""><forename type="first">Pinghua</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,462.02,196.34,77.98,8.59;12,93.58,207.25,123.56,8.59">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2979" to="3010" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.58,222.33,446.59,8.64;12,93.58,233.06,151.76,8.82" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="12,430.65,222.33,109.52,8.64;12,93.58,233.24,80.13,8.64">Stacked Cross Attention for Image-Text Matching</title>
		<author>
			<persName coords=""><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01225-0_13</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,192.48,233.06,22.36,8.59">Computer Vision – ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="212" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.58,248.13,446.59,8.64;12,93.58,258.86,150.09,8.82" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="12,335.36,248.13,204.81,8.64;12,93.58,259.04,80.13,8.64">Knowledge Aware Semantic Concept Expansion for Image-Text Matching</title>
		<author>
			<persName coords=""><forename type="first">Botian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhendong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/720</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence Organization</publisher>
			<date type="published" when="2019-08">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.58,273.93,446.42,8.64;12,93.58,284.66,199.23,8.82" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="12,434.28,273.93,105.72,8.64;12,93.58,284.84,129.27,8.64">Position Focused Attention Network for Image-Text Matching</title>
		<author>
			<persName coords=""><forename type="first">Yaxiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xueming</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Biao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Fan</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/526</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,241.62,284.66,21.90,8.59">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence Organization</publisher>
			<date type="published" when="2019-08">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.58,299.74,448.16,8.64;12,93.58,310.47,369.24,8.82" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="12,93.58,310.65,247.26,8.64">libHOG: Energy-Efficient Histogram of Oriented Gradient Computation</title>
		<author>
			<persName coords=""><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="DOI">10.1109/itsc.2015.205</idno>
		<idno>ArXiv, abs/1404.1869</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 18th International Conference on Intelligent Transportation Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.58,325.54,447.67,8.64;12,93.22,336.27,446.78,8.82;12,93.25,347.18,273.15,8.82" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="12,289.13,336.45,131.01,8.64">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><surname>Wei Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Yangqing Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298594</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,454.56,336.27,85.44,8.59;12,93.25,347.18,198.28,8.59">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2015. 2014</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
