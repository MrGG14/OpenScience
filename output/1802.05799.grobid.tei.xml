<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,116.97,101.17,378.07,15.48;1,262.91,121.09,86.23,15.48">Horovod: fast and easy distributed deep learning in TensorFlow</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,183.47,178.57,79.71,8.96"><forename type="first">Alexander</forename><surname>Sergeev</surname></persName>
							<email>asergeev@uber.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Uber Technologies, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.59,178.57,64.66,8.96"><forename type="first">Mike</forename><forename type="middle">Del</forename><surname>Balso</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Uber Technologies, Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,116.97,101.17,378.07,15.48;1,262.91,121.09,86.23,15.48">Horovod: fast and easy distributed deep learning in TensorFlow</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3BA7C99FDF38C3D1737A400F5911AD93</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-06T11:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training modern deep learning models requires large amounts of computation, often provided by GPUs. Scaling computation from one GPU to many can enable much faster training and research progress but entails two complications. First, the training library must support inter-GPU communication. Depending on the particular methods employed, this communication may entail anywhere from negligible to significant overhead. Second, the user must modify his or her training code to take advantage of inter-GPU communication. Depending on the training library's API, the modification required may be either significant or minimal.</p><p>Existing methods for enabling multi-GPU training under the TensorFlow library entail non-negligible communication overhead and require users to heavily modify their model-building code, leading many researchers to avoid the whole mess and stick with slower single-GPU training. In this paper we introduce Horovod, an open source library that improves on both obstructions to scaling: it employs efficient inter-GPU communication via ring reduction and requires only a few lines of modification to user code, enabling faster, easier distributed training in TensorFlow. Horovod is available under the Apache 2.0 license at https://github.com/uber/horovod.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past few years, advances in deep learning have driven tremendous progress in image processing, speech recognition, and forecasting. At Uber, we apply deep learning across our business; from self-driving research to trip forecasting and fraud prevention, deep learning enables our engineers and data scientists to create better experiences for our users.</p><p>TensorFlow <ref type="bibr" coords="1,158.65,566.22,11.75,8.64" target="#b0">[1]</ref> has become a preferred deep learning library at Uber for a variety of reasons. To start, the framework is one of the most widely used open source frameworks for deep learning, which makes it easy to onboard new users. It also combines high performance with an ability to tinker with low-level model details-for instance, we can use both high-level APIs, such as Keras <ref type="bibr" coords="1,473.03,598.94,10.57,8.64" target="#b1">[2]</ref>, and implement our own custom operators using NVIDIA's CUDA toolkit. Additionally, TensorFlow has end-to-end support for a wide variety of deep learning use cases, from conducting exploratory research to deploying models in production on cloud servers, mobile apps, and even self-driving vehicles.</p><p>In September 2017, Uber Engineering introduced Michelangelo <ref type="bibr" coords="1,372.94,658.97,10.71,8.64" target="#b2">[3]</ref>, an internal ML-as-a-service platform that democratizes machine learning and makes it easy to build and deploy these systems at scale. In this paper, we introduce Horovod, an open-source component of Michelangelo's deep learning toolkit which makes it easier to start-and speed up-distributed deep learning projects with TensorFlow. Horovod is available under the Apache 2.0 license at https://github.com/uber/ horovod.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1802.05799v3 [cs.LG] 21 Feb 2018</head><p>As we began training more and more machine learning models at Uber, their size and data consumption grew significantly. In a large portion of cases, the models were still small enough to fit on one or multiple GPUs within a server, but as datasets grew, so did the training times, which sometimes took a week or longer to complete. We found ourselves in need of a way to train using a lot of data while maintaining short training times. To achieve this, our team turned to distributed training.</p><p>We began by testing the standard distributed TensorFlow <ref type="bibr" coords="2,337.77,169.30,11.67,8.64" target="#b3">[4]</ref> technique. After trying it out on a few models, it became apparent that we needed to make two adjustments.</p><p>First, after following the documentation and code examples, it was not always clear which code modifications needed to be made to distribute their model training code. The standard distributed TensorFlow package introduces many new concepts: workers, parameter servers, tf.Server(), tf.ClusterSpec(), tf.train.SyncReplicasOptimizer(), and tf.train.replicas_device_setter() to name a few.</p><p>While this API may be well-suited to certain scenarios, in many cases it introduced subtle, hardto-diagnose bugs. Identifying and fixing these bugs unfortunately required users to climb a steep learning curve of concepts they almost never care about-they just want to take an existing model and make it faster, not become an expert along the way in syncronization primtivies.</p><p>The second issue dealt with the challenge of computing at Uber's scale. After running a few benchmarks, we found that we could not get the standard distributed TensorFlow to scale as well as our services required. For example, we lost about half of our resources due to communication overhead when training on 128 GPUs.</p><p>Figure <ref type="figure" coords="2,135.99,548.23,3.80,8.64">1</ref>: Multi-GPU scaling performance using TensorFlow. When comparing images processed per second while running the standard TensorFlow benchmarking suite on NVIDIA Pascal GPUs (ranging from 1 to 128) with both the Inception V3 and ResNet-101 TensorFlow models to theoretically ideal scaling (computed by multiplying the single-GPU rate by the number of GPUs), we were unable to take full advantage of our hardware resources.</p><p>When we ran the standard TensorFlow benchmarking suite [5] on 128 NVIDIA Pascal GPUs, showcased in Figure <ref type="figure" coords="2,192.78,642.58,3.77,8.64">1</ref>, we observed that both the Inception V3 and ResNet-101 models were were unable to leverage nearly half of our GPU resources.</p><p>Motivated to make the most of our GPU capacity, we became even more excited about distributed training after Facebook published a paper <ref type="bibr" coords="2,274.32,680.79,10.50,8.64" target="#b4">[6]</ref>, demonstrating training of a ResNet-50 network in one hour on 256 GPUs by combining principles of data parallelism <ref type="bibr" coords="2,362.90,691.70,11.68,8.64" target="#b5">[7]</ref> with an innovative learning rate adjustment technique. This milestone made it abundantly clear that large-scale distributed training can have an enormous impact on model developer productivity.</p><p>Figure <ref type="figure" coords="3,137.87,305.34,7.91,8.64">2:</ref> The "data parallel" approach to distributed training involves splitting up the data and training on multiple nodes in parallel. In synchronous cases, the gradients for different batches of data are calculated separately on each node but averaged across nodes to apply consistent updates to the model copy in each node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Leveraging a different type of algorithm</head><p>After this realization, we started looking for a better way to train our distributed TensorFlow models. Since our models were small enough to fit on a single GPU, or multiple GPUs in a single server, we tried using Facebook's data parallel approach to distributed training, shown on Figure <ref type="figure" coords="3,451.35,416.50,3.74,8.64">2</ref>.</p><p>Conceptually, the data-parallel distributed training paradigm is straightforward:</p><p>1. Run multiple copies of the training script and each copy: The standard distributed TensorFlow package runs with a parameter server approach to averaging gradients, shown on Figure <ref type="figure" coords="3,224.67,573.23,3.81,8.64" target="#fig_1">3</ref>. In this approach, each process has one of two potential roles: a worker or a parameter server. Workers process the training data, compute gradients, and send them to parameter servers to be averaged.</p><p>While this approach improved our performance, we encountered two challenges:</p><p>• Identifying the right ratio of worker to parameter servers: If one parameter server is used, it will likely become a networking or computational bottleneck. If multiple parameter servers are used, the communication pattern becomes "all-to-all" which may saturate network interconnects.</p><p>• Handling increased TensorFlow program complexity: During our testing, every user of distributed TensorFlow had to explicitly start each worker and parameter server, pass around service discovery information such as hosts and ports of all the workers and parameter servers, and modify the training program to construct tf.Server() with an appropriate tf.ClusterSpec(). Additionally, users had to ensure that all the operations were placed appropriately using tf.train.device_replica_setter() and code is modified to use towers to leverage multiple GPUs within the server. This often led to a steep learning curve and a significant amount of code restructuring, taking time away from the actual modeling.</p><p>In early 2017 Baidu published an article <ref type="bibr" coords="4,266.11,287.75,11.48,8.64" target="#b6">[8]</ref> evangelizing a different algorithm for averaging gradients and communicating those gradients to all nodes (Steps 2 and 3 above), called ring-allreduce, as well as a fork of TensorFlow through which they demonstrated a draft implementation of this algorithm.</p><p>The algorithm was based on the approach introduced in the 2009 paper by Patarasuk and Yuan <ref type="bibr" coords="4,486.50,320.48,10.58,8.64" target="#b7">[9]</ref>.</p><p>Figure <ref type="figure" coords="4,136.52,501.07,3.88,8.64">4</ref>: The ring-allreduce algorithm allows worker nodes to average gradients and disperse them to all nodes without the need for a parameter server.</p><p>In the ring-allreduce algorithm, shown on Figure <ref type="figure" coords="4,306.14,536.16,3.77,8.64">4</ref>, each of N nodes communicates with two of its peers 2 * (N -1) times. During this communication, a node sends and receives chunks of the data buffer. In the first N -1 iterations, received values are added to the values in the node's buffer. In the second N -1 iterations, received values replace the values held in the node's buffer. Patarasuk and Yuan in <ref type="bibr" coords="4,158.24,579.80,11.64,8.64" target="#b7">[9]</ref> suggest that this algorithm is bandwidth-optimal, meaning that if the buffer is large enough, it will optimally utilize the available network.</p><p>In addition to being network-optimal, the allreduce approach is much easier to understand and adopt. Users utilize a Message Passing Interface (MPI) <ref type="bibr" coords="4,328.50,618.01,16.55,8.64" target="#b8">[10]</ref> implementation such as Open MPI <ref type="bibr" coords="4,487.45,618.01,16.55,8.64" target="#b9">[11]</ref> to launch all copies of the TensorFlow program. MPI then transparently sets up the distributed infrastructure necessary for workers to communicate with each other. All the user needs to do is modify their program to average gradients using an allreduce() operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Introducing Horovod</head><p>The realization that a ring-allreduce approach can improve both usability and performance motivated us to work on our own implementation to address Uber's TensorFlow needs. We adopted Baidu's draft implementation <ref type="bibr" coords="5,194.73,75.48,16.63,8.64" target="#b10">[12]</ref> of the TensorFlow ring-allreduce algorithm and built upon it. We outline our process below:</p><p>1. We converted the code into a stand-alone Python package called Horovod, named after a traditional Russian folk dance in which performers dance with linked arms in a circle, much like how distributed TensorFlow processes use Horovod to communicate with each other. At any point in time, various teams at Uber may be using different releases of TensorFlow. We wanted all teams to be able to leverage the ring-allreduce algorithm without needing to upgrade to the latest version of TensorFlow, apply patches to their versions, or even spend time building out the framework. Having a stand-alone package allowed us to cut the time required to install Horovod from about an hour to a few minutes, depending on the hardware.</p><p>2. We replaced the Baidu ring-allreduce implementation with NCCL <ref type="bibr" coords="5,404.36,200.42,15.12,8.64" target="#b11">[13]</ref>. NCCL is NVIDIA's library for collective communication that provides a highly optimized version of ringallreduce. NCCL 2 introduced the ability to run ring-allreduce across multiple machines, enabling us to take advantage of its many performance boosting optimizations.</p><p>3. We added support for models that fit inside a single server, potentially on multiple GPUs, whereas the original version only supported models that fit on a single GPU.</p><p>4. Finally, we made several API improvements inspired by feedback we received from a number of initial users. In particular, we implemented a broadcast operation that enforces consistent initialization of the model on all workers. The new API allowed us to cut down the number of operations a user had to introduce to their single GPU program to four. User can then run several copies of the program across multiple servers using the mpirun command:</p><p>$ mpirun -np 16 -H server1:4,server2:4,server3:4,server4:4 python train.py</p><p>The mpirun command distributes train.py to four nodes and runs it on four GPUs per node.</p><p>Horovod can also distribute Keras programs by following the same steps. (You can find examples of scripts for both TensorFlow and Keras on the Horovod GitHub page <ref type="bibr" coords="6,381.21,378.35,14.94,8.64" target="#b12">[15]</ref>.) Horovod's ease of use, debugging efficiency, and speed makes it a highly effective sidekick for engineers and data scientists interested in distributing a single-GPU or single-server program. Next, we introduce Horovod Timeline, a means of providing a high level of understanding of the states of worker nodes during a distributed training job.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Horovod Timeline</head><p>As we onboarded users to Horovod, we realized that we needed a way for them to easily identify bugs in their code, an issue commonly faced when dealing with complex distributed systems. In particular, it was difficult to use native TensorFlow timelines or the CUDA Profiler because users are required to collect and cross-reference profiles from the various servers.</p><p>With Horovod, we wanted to created a way to provide a high-level understanding of operation timelines across nodes. To do so, we built Horovod Timeline, a Horovod-focused profiling tool compatible with Chrome's about:tracing <ref type="bibr" coords="6,283.82,548.67,16.47,8.64" target="#b13">[16]</ref> trace event profiling viewer. Users can use Horovod Timelines to view exactly what each node was doing at each time step throughout a training job. This helps identify bugs and debug performance issues. Users can enable timelines by setting a single environment variable and can view the profiling results in the browser through chrome://tracing. Figure <ref type="figure" coords="6,136.50,592.31,4.98,8.64" target="#fig_2">5</ref> shows an example of Horovod Timeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Tensor Fusion</head><p>After we analyzed the timelines of a few models, we noticed that those with a large amount of tensors, such as ResNet-101, tended to have many tiny allreduce operations. As noted earlier, ring-allreduce utilizes the network in an optimal way if the tensors are large enough, but does not work as efficiently or quickly if they are very small. We asked ourselves: what if multiple tiny tensors could be fused together before performing ring-allreduce on them?</p><p>Our answer: Tensor Fusion, an algorithm that fuses tensors together before we call Horovod's ring-allreduce. As we experimented with this approach, we observed up to 65 percent improvement in performance on models with a large number of layers running on an unoptimized transmission control protocol (TCP) network. We outline how to use Tensor Fusion, below:</p><p>1. Determine which tensors are ready to be reduced. Select the first few tensors that fit in the buffer and have the same data type.</p><p>2. Allocate a fusion buffer if it was not previously allocated. Default fusion buffer size is 64 MB.</p><p>3. Copy data of selected tensors into the fusion buffer.</p><p>4. Execute the allreduce operation on the fusion buffer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Copy data from the fusion buffer into the output tensors.</p><p>6. Repeat until there are no more tensors to reduce in the cycle.</p><p>With Horovod, Tensor Fusion, and other features built on top of Michelangelo, we can increase the efficiency, speed, and ease-of-use across our machine learning systems. In our next section, we share real world benchmarks that showcase Horovod's performance. We re-ran the official TensorFlow benchmarks modified to use Horovod <ref type="bibr" coords="8,410.09,75.48,16.73,8.64" target="#b14">[17]</ref> and compared the performance with regular distributed TensorFlow. As depicted in Figure <ref type="figure" coords="8,416.32,86.39,3.81,8.64" target="#fig_3">6</ref>, we observed large improvements in our ability to scale; we were no longer wasting half of the GPU resources-in fact, scaling using both Inception V3 and ResNet-101 models achieved an 88 percent efficiency mark. In other words, the training was about twice as fast as standard distributed TensorFlow. Since both MPI and NCCL support remote direct memory access (RDMA) <ref type="bibr" coords="8,407.31,362.73,16.55,8.64" target="#b15">[18]</ref> capable networking (e.g., via InfiniBand <ref type="bibr" coords="8,196.42,373.64,16.74,8.64" target="#b16">[19]</ref> or RDMA over Converged Ethernet <ref type="bibr" coords="8,371.96,373.64,15.10,8.64" target="#b17">[20]</ref>), we ran additional sets of benchmarking tests using RDMA network cards to determine if they helped us enhance efficiency compared to TCP networking. Figure <ref type="figure" coords="8,260.14,395.46,4.98,8.64" target="#fig_4">7</ref> shows the results of this benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Horovod Benchmarks</head><p>For the Inception V3 and ResNet-101 models, we found that RDMA did not significantly improve our performance and only achieved a three to four percent increase over TCP networking. RDMA, however, did help Horovod exceed 90 percent scaling efficiency on both models.</p><p>Meanwhile, the VGG-16 model experienced a significant 30 percent speedup when we leveraged RDMA networking. This can be explained by VGG-16's high number of model parameters, caused by the use of fully connected layers combined with its small number of layers. These characteristics shifted the critical path from GPU computation to communication and created a networking bottleneck.</p><p>These benchmarks demonstrate that Horovod scales well on both plain TCP and RDMA-capable networks, although users with RDMA networking will be able to squeeze out optimal performance and experience a significant efficiency gain when using models with a high number of model parameters, such as the VGG-16.</p><p>With Horovod, we have only scratched the surface when it comes to exploring performance optimizations in deep learning; in the future, we intend to continue leveraging the open source community to extract additional performance gains with our machine learning systems and frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Next steps</head><p>There are a few areas that we are actively working on to improve Horovod, including:</p><p>1. Making it easier to install MPI: While it is relatively easy to install MPI on a workstation, installation of MPI on a cluster typically requires some effort; for instance, there are number of workload managers available and different tweaks should be made depending on network hardware. We are developing reference designs for running Horovod on a cluster; to do so, we hope to work with the MPI community and network hardware vendors to develop instructions for installing MPI and relevant drivers.</p><p>2. Collecting and sharing learnings about adjusting model parameters for distributed deep learning: Facebook's paper <ref type="bibr" coords="9,281.12,86.39,11.70,8.64" target="#b4">[6]</ref> describes the adjustments needed to model hyperparameters to achieve the same or greater accuracy in a distributed training job compared to training the same model on a single GPU, demonstrating the feasibility of training a TensorFlow model on 256 GPUs. We believe this area of deep learning research is still in its early stages and hope to collaborate with other teams about approaches to further scale deep learning training. 3. Adding examples of very large models: Horovod currently supports models that fit into one server but may span multiple GPUs. We are eager to develop more examples for large models spanning multiple GPUs, and encourage others to test Horovod on these types of models as well.</p><p>We hope the simplicity of Horovod enables others to adopt distributed training and better leverage their compute resources for deep learning. We welcome feedback and contributions: please report any issues you encounter, share speed-ups, and send pull requests.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,147.75,469.51,115.36,8.64;3,147.19,482.51,117.33,8.64;3,147.75,495.51,160.20,8.64;3,131.41,510.61,201.10,8.64;3,131.41,525.71,83.29,8.64;3,131.41,540.80,100.71,8.64"><head></head><label></label><figDesc>(a) reads a chunk of the data (b) runs it through the model (c) computes model updates (gradients) 2. Average gradients among those multiple copies 3. Update the model 4. Repeat (from Step 1a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,108.00,189.23,396.00,8.64;4,108.00,200.14,318.11,8.64;4,121.75,75.39,368.50,102.33"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The parameter server model for distributed training jobs can be configured with different ratios of parameter servers to workers, each with different performance profiles.</figDesc><graphic coords="4,121.75,75.39,368.50,102.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,108.00,243.80,396.00,8.64;7,108.00,254.71,141.56,8.64;7,121.75,75.39,368.50,156.90"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Horovod Timeline depicts a high level timeline of events in a distributed training job in Chrome's trace event profiling tool.</figDesc><graphic coords="7,121.75,75.39,368.50,156.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,108.00,688.42,396.00,8.64;7,108.00,699.33,396.17,8.64;7,108.00,710.24,272.98,8.64;7,121.75,518.57,368.50,158.34"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: A comparison of images processed per second with standard distributed TensorFlow and Horovod when running a distributed training job over different numbers of NVIDIA Pascal GPUs for Inception V3 and ResNet-101 TensorFlow models over 25GbE TCP.</figDesc><graphic coords="7,121.75,518.57,368.50,158.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,108.00,314.94,396.00,8.64;8,108.00,325.85,396.00,8.64;8,108.00,336.76,371.55,8.64;8,121.75,145.09,368.50,158.34"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: A comparison of the images processed per second of the Horovod over plain 25GbE TCP and the Horovod with 25GbE RDMA-capable networking when running a distributed training job over different numbers of NVIDIA Pascal GPUs for Inception V3, ResNet-101 and VGG-16.</figDesc><graphic coords="8,121.75,145.09,368.50,158.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="2,121.75,378.28,368.50,158.44"><head></head><label></label><figDesc></figDesc><graphic coords="2,121.75,378.28,368.50,158.44" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors would like to thank <rs type="person">Molly Vorwerck</rs> and <rs type="person">Jason Yosinski</rs> for the help in preparing this paper.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,129.58,338.10,375.66,8.64;9,129.58,349.01,375.66,8.64;9,129.22,359.91,376.02,8.64;9,129.58,370.82,375.66,8.64;9,129.58,381.73,374.42,8.64;9,129.27,392.64,375.97,8.64;9,129.58,403.55,374.42,8.64;9,129.58,414.46,325.24,8.64" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="9,405.72,403.55,98.28,8.64;9,129.58,414.46,218.57,8.64">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName coords=""><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Derek</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernanda</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,129.58,432.75,375.25,8.70;9,129.58,443.66,115.43,8.64" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015-12">2015. December-2017</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,129.58,461.95,376.16,8.64;9,129.58,472.86,359.90,8.70" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,285.75,461.95,215.91,8.64">Meet Michelangelo: Uber&apos;s machine learning platform</title>
		<author>
			<persName coords=""><forename type="first">Jeremy</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Balso</forename></persName>
		</author>
		<ptr target="https://eng.uber.com/michelangelo/" />
		<imprint>
			<date type="published" when="2017-12">2017. -December-2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,129.58,491.15,374.42,8.64;9,129.58,502.06,375.66,8.64;9,129.58,512.97,375.67,8.64;9,129.58,523.88,374.59,8.64;9,129.58,534.79,219.26,8.64" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Derek</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Tensorflow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08695</idno>
		<title level="m" coord="9,449.32,523.88,54.84,8.64;9,129.58,534.79,112.94,8.64">A system for large-scale machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,129.58,582.28,375.67,8.64;9,129.22,593.19,374.78,8.64;9,129.58,604.10,182.26,8.64" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m" coord="9,201.16,593.19,302.84,8.64;9,129.58,604.10,75.77,8.64">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,129.58,622.39,378.08,8.70;9,129.58,633.30,376.07,8.70;9,129.58,644.21,106.58,8.64" xml:id="b5">
	<analytic>
		<title level="a" type="main">Wikipedia</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcgrady</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-32001-4_207-1</idno>
		<ptr target="https://en.wikipedia.org/w/index.php?title=Data_parallelism&amp;oldid=807618997" />
	</analytic>
	<monogr>
		<title level="m" coord="9,129.58,622.39,260.41,8.64">Encyclopedia of Big Data</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017-12">2017. December-2017</date>
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,129.58,662.50,378.08,8.70;9,129.58,673.41,376.08,8.70;9,129.58,684.31,25.73,8.64" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Gibiansky</surname></persName>
		</author>
		<ptr target="http://research.baidu.com/bringing-hpc-techniques-deep-learning" />
		<title level="m" coord="9,212.55,662.50,171.82,8.64">Bringing HPC techniques to deep learning</title>
		<imprint>
			<date type="published" when="2017-12">2017. December-2017</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,129.58,702.61,376.07,8.64;9,129.58,713.34,215.65,8.82" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,251.81,702.61,253.85,8.64;9,129.58,713.51,18.22,8.64">Bandwidth optimal all-reduce algorithms for clusters of workstations</title>
		<author>
			<persName coords=""><forename type="first">Pitch</forename><surname>Patarasuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jpdc.2008.09.002</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,155.03,713.34,108.14,8.59">Journal of Parallel and Distributed Computing</title>
		<title level="j" type="abbrev">Journal of Parallel and Distributed Computing</title>
		<idno type="ISSN">0743-7315</idno>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="124" />
			<date type="published" when="2009-02">2009</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.58,75.48,378.08,8.70;10,129.58,86.39,197.79,8.70" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,182.14,75.48,207.30,8.64">MPI</title>
		<author>
			<persName coords=""><surname>Mpi Forum</surname></persName>
		</author>
		<idno type="DOI">10.1145/169627.169855</idno>
		<ptr target="http://www.mpi-forum.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1993 ACM/IEEE conference on Supercomputing - Supercomputing &apos;93</title>
		<meeting>the 1993 ACM/IEEE conference on Supercomputing - Supercomputing &apos;93</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2017-12">2017. December-2017</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.58,105.27,376.16,8.64;10,129.58,116.17,376.16,8.64;10,129.58,127.08,375.66,8.64;10,129.58,137.81,374.42,8.82;10,129.27,148.72,346.84,8.82" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,430.00,127.08,75.24,8.64;10,129.58,137.99,242.90,8.64">Open MPI: Goals, Concept, and Design of a Next Generation MPI Implementation</title>
		<author>
			<persName coords=""><forename type="first">Edgar</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Graham</forename><forename type="middle">E</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Bosilca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thara</forename><surname>Angskun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jack</forename><forename type="middle">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Squyres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vishal</forename><surname>Sahay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prabhanjan</forename><surname>Kambadur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brian</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Lumsdaine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ralph</forename><forename type="middle">H</forename><surname>Castain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timothy</forename><forename type="middle">S</forename><surname>Woodall</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-30218-6_19</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,391.15,137.81,112.85,8.59;10,129.27,148.72,131.19,8.59">Recent Advances in Parallel Virtual Machine and Message Passing Interface</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Berlin Heidelberg</publisher>
			<date type="published" when="2004-09">September 2004</date>
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.58,167.78,378.09,8.70;10,129.58,178.69,376.08,8.70;10,129.58,189.60,25.73,8.64" xml:id="b10">
	<analytic>
		<title level="a" type="main">Beyond Baidu SEO</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<idno type="DOI">10.1002/9781119368755.ch6</idno>
		<ptr target="https://github.com/baidu-research/tensorflow-allreduce" />
	</analytic>
	<monogr>
		<title level="m" coord="10,282.13,167.78,139.52,8.64">baidu-research/tensorflow-allreduce</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2017-12">2017. December-2017</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="97" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.58,208.48,378.08,8.70;10,129.58,219.39,223.94,8.70" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,169.07,208.48,200.51,8.64">NVIDIA collective communications library (NCCL)</title>
		<author>
			<persName coords=""><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://developer.nvidia.com/nccl" />
		<imprint>
			<date type="published" when="2017-12">2017. December-2017</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.58,278.96,377.04,8.70;10,129.06,289.87,312.83,8.70" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Sergeev</surname></persName>
		</author>
		<ptr target="https://github.com/uber/horovod" />
		<title level="m" coord="10,227.95,278.96,239.59,8.64">uber/horovod: Distributed training framework for TensorFlow</title>
		<imprint>
			<date type="published" when="2017-12">2017. -December-2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.58,308.75,378.08,8.70;10,129.58,319.66,376.08,8.70;10,129.58,330.57,69.79,8.64" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Chromium</forename><surname>Authors</surname></persName>
		</author>
		<ptr target="https://www.chromium.org/developers/how-tos/trace-event-profiling-tool" />
		<title level="m" coord="10,211.25,308.75,179.66,8.64">The Trace Event Profiling Tool (about:tracing)</title>
		<imprint>
			<date type="published" when="2017-12">2017. -December-2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.58,349.45,375.47,8.70;10,129.58,360.36,260.54,8.70" xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">Tensorflow</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Benchmarks</surname></persName>
		</author>
		<ptr target="https://github.com/alsrgv/benchmark/tree/horovod_v2" />
		<imprint>
			<date type="published" when="2017-12">2017. December-2017</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.58,379.24,44.48,8.64;10,192.94,379.24,312.80,8.64;10,129.58,390.55,371.27,8.30;10,129.58,401.06,260.54,8.70" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="10,192.94,379.24,308.49,8.64">Remote direct memory access -Wikipedia, the free encyclopedia</title>
		<author>
			<persName coords=""><surname>Wikipedia</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/w/index.php?title=Remote_direct_memory_access&amp;oldid=812842097" />
		<imprint>
			<date type="published" when="2017-12">2017. -December-2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.58,419.94,375.46,8.70;10,129.58,430.84,376.07,8.70;10,129.58,441.75,25.73,8.64" xml:id="b16">
	<analytic>
		<title level="a" type="main">Embedding wikipedia title based on its wikipedia text and categories</title>
		<author>
			<persName><forename type="first">Chi-Yen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Yun</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/ialp.2017.8300566</idno>
		<ptr target="https://en.wikipedia.org/w/index.php?title=InfiniBand&amp;oldid=810171440" />
	</analytic>
	<monogr>
		<title level="m" coord="10,129.58,419.94,227.32,8.64">2017 International Conference on Asian Language Processing (IALP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-12">2017. December-2017</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.58,460.63,44.48,8.64;10,195.49,460.63,310.16,8.64;10,129.58,471.54,24.55,8.64;10,175.09,471.94,329.43,8.30;10,129.58,482.45,307.60,8.70" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><surname>Wikipedia</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/w/index.php?title=RDMA_over_Converged_Ethernet&amp;oldid=782744462" />
		<title level="m" coord="10,195.49,460.63,310.16,8.64;10,129.58,471.54,20.46,8.64">RDMA over Converged Ethernet -Wikipedia, the free encyclopedia</title>
		<imprint>
			<date type="published" when="2017-12">2017. -December-2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
