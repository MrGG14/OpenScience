<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.08,103.10,315.12,15.20;1,219.96,125.01,156.88,15.20">How good are deep models in understanding the generated images?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-08-26">August 26, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,275.29,157.53,44.69,10.56"><forename type="first">Ali</forename><surname>Borji</surname></persName>
							<email>aliborji@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Quintic AI</orgName>
								<address>
									<settlement>San Francisco</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.08,103.10,315.12,15.20;1,219.96,125.01,156.88,15.20">How good are deep models in understanding the generated images?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-08-26">August 26, 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">AA1CD957FD5DE23AE8B3E79318504123</idno>
					<idno type="arXiv">arXiv:2208.10760v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-06T16:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>My goal in this paper is twofold: to study how well deep models can understand the images generated by DALL-E 2 and Midjourney, and to quantitatively evaluate these generative models. Two sets of generated images are collected for object recognition and visual question answering (VQA) tasks. On object recognition, the best model, out of 10 state-of-the-art object recognition models, achieves about 60% and 80% top-1 and top-5 accuracy, respectively. These numbers are much lower than the best accuracy on the ImageNet dataset (91% and 99%). On VQA, the OFA model scores 77.3% on answering 241 binary questions across 50 images. This model scores 94.7% on the binary VQA-v2 dataset. Humans are able to recognize the generated images and answer questions on them easily. We conclude that a) deep models struggle to understand the generated content, and may do better after fine-tuning, and b) there is a large distribution shift between the generated images and the real photographs. The distribution shift appears to be category-dependent. Data is available at: link.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent deep generative models such as DALL-E 2 <ref type="bibr" coords="1,309.71,436.96,15.60,8.80" target="#b11">[12]</ref> and Midjourney<ref type="foot" coords="1,400.19,435.40,3.97,6.16" target="#foot_0">1</ref> have made a big splash. They are capable of synthesizing stunning photo-realistic images for a given input text (a.k.a. a prompt), and have inspired many people, in particular the artists. Some researchers have also used these tools to synthesize data for training deep models (e.g. <ref type="bibr" coords="1,356.77,472.82,10.44,8.80" target="#b3">[4]</ref>). For the most part, the images generated by these systems capture what is included in the input in terms of the objects and their relations. Some studies (e.g. <ref type="bibr" coords="1,212.97,496.73,15.55,8.80" target="#b10">[11]</ref> <ref type="foot" coords="1,228.51,495.18,3.97,6.16" target="#foot_1">2</ref> ) have anecdotally and qualitatively inspected these images and have found that they are limited in certain ways. For example, they do not understand the numbers, counting, and negation, have spelling errors, and lack common sense.</p><p>On the one hand, deep models such as ResNet <ref type="bibr" coords="1,313.52,532.60,10.63,8.80" target="#b5">[6]</ref> are believed to surpass humans in object classification. Here, we test the capability of recent best object classification models on generated images that are easily recognizable by humans. We also investigate the performance of VQA models on answering binary questions on generated images. The outcomes will inform us about the generalization power of deep models.</p><p>On the other hand, unlike the significant body of work that has quantitatively evaluated the images generated by GANs <ref type="bibr" coords="1,172.17,604.33,10.60,8.80" target="#b4">[5,</ref><ref type="bibr" coords="1,186.10,604.33,7.79,8.80" target="#b1">2,</ref><ref type="bibr" coords="1,197.21,604.33,7.07,8.80" target="#b2">3]</ref>, little effort has been spent on evaluating DALL-E 2 and Midjourney. The authors of these papers have already used measures such as FID <ref type="bibr" coords="1,387.38,616.28,10.55,8.80" target="#b6">[7]</ref> to quantitatively evaluate their systems. However, research has shown that relying on one score is usually not enough to draw strong conclusions. Here, we take a different approach and argue that if generated images are good, then deep models should be able to recognize them. We feed the synthesized images to the best object recognition models and measure the classification accuracy for each object category. We find that, although the average model performance is poor, models score very high and near perfect over some object categories. This indicates that generative models can capture some categories better than others. Visually inspecting the images from the hard categories, reveals that they are indeed hard to recognize by humans (e.g. kites). Admittedly, our results should be taken with a grain of salt, as classification accuracy may favor a generative model that sacrifices sample diversity in favor of generating high fidelity samples. Generative models are expected to generate a diverse set of high-fidelity images and this is what some scores attempt to measure (e.g. FID <ref type="bibr" coords="2,488.44,478.35,9.93,8.80" target="#b6">[7]</ref>, or Precision-Recall <ref type="bibr" coords="2,154.48,490.31,14.76,8.80" target="#b12">[13]</ref>).</p><p>We start by curating a dataset of images generated by DALL-E 2 and Midjourney by crawling images from twitter posts as well as Google search. We did not use images that clearly depict faces of people per DALL-E 2 guidelines<ref type="foot" coords="2,235.64,524.62,3.97,6.16" target="#foot_2">3</ref> . Only images that have good quality and are recognizable by humans are selected. We created two sets of images, one for object recognition and another for visual question answering <ref type="bibr" coords="2,167.55,550.08,9.96,8.80" target="#b0">[1]</ref>. Sample images from these sets are shown in Fig. <ref type="figure" coords="2,400.14,550.08,3.87,8.80" target="#fig_0">1</ref>, and Fig. <ref type="figure" coords="2,450.91,550.08,3.87,8.80" target="#fig_3">4</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Object recognition</head><p>We collected 1,862 synthetic images generated by DALL-E 2 and Midjourney across 17 categories (Fig. <ref type="figure" coords="2,104.28,616.80,3.80,8.80" target="#fig_0">1</ref>). The number of images per category is shown in the bottom-right panel of Fig. <ref type="figure" coords="2,459.18,616.80,3.80,8.80" target="#fig_1">2</ref>. We tested 10 state-of-the-art object recognition models<ref type="foot" coords="2,280.45,627.20,3.97,6.16" target="#foot_3">4</ref> , pre-trained on ImageNet, on these images. These models have been published over the past several years and have been immensely successful over the ImageNet benchmark. They include AlexNet <ref type="bibr" coords="2,283.86,652.67,10.05,8.80" target="#b8">[9]</ref>, MobileNetV2 <ref type="bibr" coords="2,362.58,652.67,14.71,8.80" target="#b13">[14]</ref>, GoogleNet <ref type="bibr" coords="2,434.28,652.67,14.71,8.80" target="#b14">[15]</ref>, DenseNet <ref type="bibr" coords="2,501.62,652.67,10.05,8.80" target="#b7">[8]</ref>, ResNext <ref type="bibr" coords="2,121.67,664.62,14.61,8.80" target="#b18">[19]</ref>, ResNet101 <ref type="bibr" coords="2,192.97,664.62,9.96,8.80" target="#b5">[6]</ref>, ResNet152 <ref type="bibr" coords="2,259.29,664.62,9.96,8.80" target="#b5">[6]</ref>, Inception_V3 <ref type="bibr" coords="2,340.07,664.62,14.61,8.80" target="#b15">[16]</ref>, Deit <ref type="bibr" coords="2,383.66,664.62,14.61,8.80" target="#b16">[17]</ref>, and ResNext_WSL <ref type="bibr" coords="2,494.38,664.62,14.61,8.80" target="#b9">[10]</ref>.</p><p>Since some of our classes cover multiple ImageNet classes<ref type="foot" coords="2,341.72,675.02,3.97,6.16" target="#foot_4">5</ref> , we had to make some adjustments for computing accuracy. For example, ImageNet has three types of clocks including 'digital clock', 'wall clock', and 'analog clock'. Here, we only have the 'clock' class, containing mostly analog clocks. We chose to give the benefit of the doubt to models. A prediction is deemed correct if the ground-truth label is in the set of the words predicted by the model. In the mentioned scenario, if a model predicts 'wall clock', then a hit is counted. If the model predicts 'wall' or anything else, then the prediction would be considered a mistake. The same is true for the top-5 accuracy computation. For example, if the top five model predictions are 'bib', 'necklace', 'toilet seat', 'pick', and 'wall clock', then the prediction is counted as a hit. In practice, first all words in the predicted labels are extracted, and then the prediction is counted as a hit if the ground-truth is in this set. In case of ground-truth having two words (e.g. 'toilet seat'), then it should happen in the set of words exactly as it is. Since 'rabbit' and 'hare' classes are very similar, we consider both of them to be true predictions. Notice that this way of accuracy measurement gives an overestimation of the model performance, but it is good enough for our purposes here. Even with this overestimation, as we will show, models still perform poorly.</p><p>Results are shown in Fig. <ref type="figure" coords="3,215.95,635.07,3.95,8.80" target="#fig_1">2</ref>. Among the models, resnext101_32x8d_ws ranks the best, and significantly better than other models. It achieves around 60% top-1 and about 80% top-5 accuracy. This model scores 85.4% top-1 and 97.6% top-5 accuracy over the ImageNet-1k validation set (singlecrop). The success of this model can be attributed to the fact that it is trained to predict hashtags on billions of social media images in a weakly supervised manner. The best performance on our data is much lower than the best available performance on the ImageNet validation set which are 91% and 99% corresponding to top-1 and top-5 accuracy<ref type="foot" coords="3,286.06,705.25,3.97,6.16" target="#foot_5">6</ref> . These results suggest that there is a big difference between the distribution of ImageNet images and the distribution of generated images. An image is considered an error if the ground-truth is not within the top 5 predictions.</p><p>According to Fig. <ref type="figure" coords="4,174.52,395.59,3.80,8.80" target="#fig_1">2</ref>, the top five most difficult categories for the resnext101_32x8d_ws model in order are kite, turtle, squirrel, sunglass, and helmet. The performance on these categories is below 40%. The kite class is often confused with parachute, balloon, and umbrella classes as shown in Fig. <ref type="figure" coords="4,144.93,431.46,3.95,8.80" target="#fig_2">3</ref>. Sample failure cases from the categories along with the predictions are shown in Appendix B. Models often fail on drawings, unusual objects, or images where the object of interest is not unique.</p><p>We also computed the fraction of images, per category, over which all models succeed, or they all fail. Results are shown in the bottom left panel of Fig. <ref type="figure" coords="4,323.15,479.28,3.86,8.80" target="#fig_1">2</ref>. We noticed that for some categories such as kite and turtle models consistently fail, while for some others such as pretzel and tractor they all do very well. When all models succeed, they are correct at best over 90% of the images (over pretzel category using top-1 acc). These results indicate that models share similar weaknesses and strengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Visual question answering</head><p>Here, we test VQA models on free-form and open-ended visual question answering. We only consider binary questions since in principle, any question can be converted to a binary one on an image. Recent VQA models are able to answer binary questions above 95% accuracy over the VQA-v2 dataset <ref type="foot" coords="4,113.23,604.01,3.97,6.16" target="#foot_6">7</ref> , which is astonishing considering the complexity of the questions.</p><p>We collected 50 images and formulated a total of 241 questions on them. There are 4.82 questions per image on average. 132 questions have positive answers and 109 have negative answers. Average number of words per question is 5.12 (i.e. question length).</p><p>To see how well the state-of-the-art VQA models perform on the generated images, we choose the OFA model <ref type="bibr" coords="4,152.62,665.35,15.52,8.80" target="#b17">[18]</ref> which is currently the leading scorer on the VQA-v2 test-std set<ref type="foot" coords="4,452.72,663.79,3.97,6.16" target="#foot_7">8</ref> . This model achieves 77.27% accuracy on generated images. To put this result in perspective, this model scores about 94.7% on the VQA-v2 test-std set. There are two reasons why OFA performs lower here a) generated images may contain semantic content that is missing in the training set of the VQA-v2 dataset (e.g. 'the astronaut riding the horse'), and b) our questions might be more challenging than </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and conclusion</head><p>We tested deep models on generated images over two tasks of object recognition and visual question answering. Models perform poorly on these data compared to their performance on real images over which they have been trained on (ImageNet and VQA-v2). We conclude that a) generative models synthesize images for some categories better than other categories, and b) there is a large distribution shift between real images and synthetic images, and this is perhaps why deep models struggle over the latter. We foresee four directions for future research in this area:</p><p>1. We did not distinguish between images generated by DALL-E 2 and Midjourney. A quantitative comparison between the two models would be interesting.</p><p>2. We tested models on a small test set of generated images. Results over a larger set of images and object classes are likely to provide more insights.</p><p>3. It is hard to tell from our results whether low performance on the generated images is due to problems with the images (e.g. low fidelity, artifacts, etc), or lack of generalization by the classification models. Visual inspection supports the latter. One way to address this shortcoming is to train and test a deep classifier on generated images. If such a classifier performs well, then it hints towards the high quality of the generated images (and vice-versa).</p><p>4. Generative models offer a unique opportunity to automatically generate large scale data for training data-hungry deep models. It would be interesting to see how well models trained on synthetic data generated by DALL-E 2 and Midjourney generalize to real-world data. See <ref type="bibr" coords="5,503.11,728.34,10.53,8.80" target="#b3">[4]</ref> as an example in the context of object detection.</p><p>A Performance of the individual models               </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,81.64,409.01,433.38,8.80;2,81.64,420.97,242.83,8.80;2,124.84,63.15,345.62,334.39"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample generated images by DALL-E 2 or Midjourney models. Categories in order are: elephant, mushroom, pizza, pretzel, rabbit, and tractor.</figDesc><graphic coords="2,124.84,63.15,345.62,334.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,81.64,422.27,433.39,8.80;3,81.64,434.23,432.00,8.80;3,81.26,446.18,432.38,8.80;3,81.64,458.14,339.64,8.80;3,101.58,263.32,194.39,147.49"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Top left: per model performance of the models averaged over 17 categories. Top right: performance of the best model per category. The dashed lines show the average performance. See Appendix A for performance of individual models. Bottom left: fraction of images over which all models fail or they all succeed. Bottom-right: number of images per category.</figDesc><graphic coords="3,101.58,263.32,194.39,147.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,81.64,351.09,433.93,8.80;4,81.26,363.05,380.56,8.80;4,124.84,63.15,345.60,276.48"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sample images from the kite category over which the resnext101_32x8d_wsl model fails.An image is considered an error if the ground-truth is not within the top 5 predictions.</figDesc><graphic coords="4,124.84,63.15,345.60,276.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,111.16,388.07,372.96,8.80;5,81.64,63.16,432.00,321.41"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Sample images and questions along with the predictions of the OFA model.</figDesc><graphic coords="5,81.64,63.16,432.00,321.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,108.13,594.59,379.01,8.80;8,92.45,427.70,194.40,155.42"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance of individual models on object detection over generated images.</figDesc><graphic coords="8,92.45,427.70,194.40,155.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,91.67,561.59,411.94,8.80;9,92.45,394.69,194.40,155.42"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance of individual models on object detection over generated images (cnt'd).</figDesc><graphic coords="9,92.45,394.69,194.40,155.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="11,117.10,621.67,225.74,8.80;11,375.76,621.67,102.43,8.80;11,81.64,178.19,432.02,432.02"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Predictions of the resnext101_32x8d_wsl over the clock category.</figDesc><graphic coords="11,81.64,178.19,432.02,432.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="12,109.63,578.46,376.02,8.80;12,81.64,221.39,432.00,345.60"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Predictions of the resnext101_32x8d_wsl model over the elephant category.</figDesc><graphic coords="12,81.64,221.39,432.00,345.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="13,110.87,578.46,373.54,8.80;13,81.64,221.39,432.00,345.60"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Predictions of the resnext101_32x8d_wsl model over the helmet category.</figDesc><graphic coords="13,81.64,221.39,432.00,345.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="15,114.47,567.66,366.34,8.80;15,81.64,232.20,432.00,324.00"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Predictions of the resnext101_32x8d_wsl model over the pizza category.</figDesc><graphic coords="15,81.64,232.20,432.00,324.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="16,100.33,578.46,394.62,8.80;16,81.64,221.39,432.00,345.60"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Predictions of the resnext101_32x8d_wsl model over the rabbit/hare category.</figDesc><graphic coords="16,81.64,221.39,432.00,345.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="17,109.29,578.46,376.69,8.80;17,81.64,221.39,432.00,345.60"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Predictions of the resnext101_32x8d_wsl model over the squirrel category.</figDesc><graphic coords="17,81.64,221.39,432.00,345.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="18,107.74,518.88,379.79,8.80;18,81.64,75.40,432.01,432.01"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Predictions of the resnext101_32x8d_wsl model over the sunglass category.</figDesc><graphic coords="18,81.64,75.40,432.01,432.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="18,113.22,724.46,368.83,8.80;18,146.44,561.79,302.41,151.20"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Predictions of the resnext101_32x8d_wsl model over the teddy category.</figDesc><graphic coords="18,146.44,561.79,302.41,151.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14" coords="19,110.57,231.60,374.15,8.80;19,226.36,77.56,142.57,142.57"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Predictions of the resnext101_32x8d_wsl model over the tractor category.</figDesc><graphic coords="19,226.36,77.56,142.57,142.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15" coords="19,113.35,722.30,368.58,8.80;19,81.64,278.83,432.00,432.00"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Predictions of the resnext101_32x8d_wsl model over the turtle category.</figDesc><graphic coords="19,81.64,278.83,432.00,432.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16" coords="20,106.57,340.68,382.14,8.80;20,81.64,113.20,432.02,216.01"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Predictions of the resnext101_32x8d_wsl model over the umbrella category.</figDesc><graphic coords="20,81.64,113.20,432.02,216.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17" coords="20,116.51,686.66,362.24,8.80;20,81.64,459.19,432.02,216.01"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Predictions of the resnext101_32x8d_wsl model over the vase category.</figDesc><graphic coords="20,81.64,459.19,432.02,216.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18" coords="22,92.21,727.57,410.85,8.80;22,81.64,394.70,432.00,321.41"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: Sample images and questions along with the predictions of the OFA model (cnt'd).</figDesc><graphic coords="22,81.64,394.70,432.00,321.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="21,81.64,97.73,432.00,321.41"><head></head><label></label><figDesc></figDesc><graphic coords="21,81.64,97.73,432.00,321.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="21,81.64,420.14,432.00,321.41"><head></head><label></label><figDesc></figDesc><graphic coords="21,81.64,420.14,432.00,321.41" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,96.88,732.02,114.29,6.64"><p>https://www.midjourney.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,96.88,740.89,416.72,7.26"><p>See https://tinyurl.com/yc7u8juf, https://tinyurl.com/2p9wku6e, and https://www.reddit.com/r/dalle2/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,96.88,720.03,118.53,6.64"><p>https://tinyurl.com/r4xeyhps</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,96.88,728.90,246.16,7.26"><p>Models are available in PyTorch hub: https://pytorch.org/hub/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="2,96.88,739.01,297.31,6.64"><p>https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="3,96.88,738.31,272.41,6.64"><p>https://paperswithcode.com/sota/image-classification-on-imagenet</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="4,96.88,732.18,78.42,7.04"><p>https://visualqa.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="4,96.88,741.66,292.71,7.04"><p>https://paperswithcode.com/sota/visual-question-answering-on-vqa-v2-test-std</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Here we show sample errors made by resnext101_32x8d_wsl model over different categories. A image is considered an error if the ground-truth is not within the top 5 predictions.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional VQA results</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="6,102.11,88.07,411.53,8.80;6,102.11,100.03,411.16,8.80;6,101.47,111.98,298.14,8.80" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,228.91,100.03,144.98,8.80">VQA: Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2015.279</idno>
	</analytic>
	<monogr>
		<title level="m" coord="6,401.60,100.03,111.67,8.80;6,101.47,111.98,190.02,8.80">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-12">2015</date>
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,102.11,131.55,412.91,8.80;6,101.62,143.51,71.38,8.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,145.25,131.55,173.64,8.80">Pros and cons of GAN evaluation measures</title>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cviu.2018.10.009</idno>
	</analytic>
	<monogr>
		<title level="j" coord="6,327.05,131.55,183.35,8.80">Computer Vision and Image Understanding</title>
		<title level="j" type="abbrev">Computer Vision and Image Understanding</title>
		<idno type="ISSN">1077-3142</idno>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="page" from="41" to="65" />
			<date type="published" when="2019-02">2019</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,102.11,163.08,411.24,8.80;6,101.57,175.04,175.52,8.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,146.84,163.08,264.61,8.80">Pros and cons of GAN evaluation measures: New developments</title>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Borji</surname></persName>
			<idno type="ORCID">0000-0001-8198-0335</idno>
		</author>
		<idno type="DOI">10.1016/j.cviu.2021.103329</idno>
	</analytic>
	<monogr>
		<title level="j" coord="6,420.50,163.08,92.85,8.80;6,101.57,175.04,91.13,8.80">Computer Vision and Image Understanding</title>
		<title level="j" type="abbrev">Computer Vision and Image Understanding</title>
		<idno type="ISSN">1077-3142</idno>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="page">103329</biblScope>
			<date type="published" when="2022-01">2022</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,102.11,194.61,412.91,8.80;6,102.11,206.56,412.91,8.80;6,101.86,218.52,22.69,8.80" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,430.03,194.61,84.99,8.80;6,102.11,206.56,261.48,8.80">Dall-e for detection: Language-driven context image synthesis for object detection</title>
		<author>
			<persName coords=""><forename type="first">Yunhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiashu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brian</forename><forename type="middle">Nlong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.09592</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,102.11,238.09,411.52,8.80;6,102.11,250.04,411.24,8.80;6,101.47,262.00,179.72,8.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,301.71,250.04,119.19,8.80">Generative adversarial networks</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1145/3422622</idno>
	</analytic>
	<monogr>
		<title level="j" coord="6,429.46,250.04,83.89,8.80;6,101.47,262.00,133.19,8.80">Communications of the ACM</title>
		<title level="j" type="abbrev">Commun. ACM</title>
		<idno type="ISSN">0001-0782</idno>
		<idno type="ISSNe">1557-7317</idno>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2014">2014</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,102.11,281.57,411.52,8.80;6,102.11,293.52,412.91,8.80;6,102.11,305.48,90.80,8.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,369.67,281.57,143.97,8.80;6,102.11,293.52,45.76,8.80">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m" coord="6,166.81,293.52,343.93,8.80">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,102.11,325.05,413.46,8.80;6,102.11,337.01,411.49,8.80;6,101.35,348.96,105.03,8.80" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="6,102.11,337.01,342.96,8.80">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08500</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,102.11,368.53,411.53,8.80;6,102.11,380.49,411.53,8.80;6,101.46,392.44,153.56,8.80" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,435.26,368.53,78.38,8.80;6,102.11,380.49,97.30,8.80">Densely Connected Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.243</idno>
	</analytic>
	<monogr>
		<title level="m" coord="6,220.83,380.49,292.81,8.80;6,101.46,392.44,45.35,8.80">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,102.11,412.01,411.52,8.80;6,102.11,423.97,411.52,8.80;6,101.62,435.92,73.59,8.80" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,364.63,412.01,149.01,8.80;6,102.11,423.97,131.34,8.80">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1145/3065386</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<title level="j" type="abbrev">Commun. ACM</title>
		<idno type="ISSN">0001-0782</idno>
		<idno type="ISSNe">1557-7317</idno>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2012">2012</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,102.11,455.49,412.91,8.80;6,101.73,467.45,411.90,8.80;6,102.11,479.40,411.52,8.80;6,101.62,491.36,63.63,8.80" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,329.60,467.45,184.04,8.80;6,102.11,479.40,47.65,8.80">Exploring the Limits of Weakly Supervised Pretraining</title>
		<author>
			<persName coords=""><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ashwin</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01216-8_12</idno>
	</analytic>
	<monogr>
		<title level="m" coord="6,174.23,479.40,305.26,8.80">Computer Vision – ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="185" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,102.11,510.93,411.41,8.80;6,102.11,522.89,143.21,8.80" xml:id="b10">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName coords=""><forename type="first">Gary</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Aaronson</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:2204.13807</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,102.11,542.46,411.52,8.80;6,102.11,554.41,398.60,8.80" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="6,460.53,542.46,53.11,8.80;6,102.11,554.41,219.10,8.80">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,102.11,573.98,411.52,8.80;6,102.11,585.94,411.48,8.80;6,101.60,597.89,141.47,8.80" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="6,473.57,573.98,40.07,8.80;6,102.11,585.94,189.70,8.80">Assessing generative models via precision and recall</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mario</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sylvain</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,317.18,585.94,196.42,8.80;6,101.60,597.89,32.71,8.80">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5228" to="5237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,102.11,617.46,413.46,8.80;6,102.11,629.42,411.52,8.80;6,101.32,641.37,293.81,8.80" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="6,102.11,629.42,236.07,8.80">MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00474</idno>
	</analytic>
	<monogr>
		<title level="m" coord="6,359.41,629.42,154.23,8.80;6,101.32,641.37,185.60,8.80">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,102.11,660.94,412.91,8.80;6,102.11,672.90,413.45,8.80;6,102.11,684.86,412.90,8.80;6,101.86,696.81,22.69,8.80" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="6,376.01,672.90,135.23,8.80">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><surname>Wei Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Yangqing Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298594</idno>
	</analytic>
	<monogr>
		<title level="m" coord="6,114.67,684.86,347.43,8.80">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-06">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,102.11,716.38,411.53,8.80;6,102.11,728.34,411.35,8.80;6,101.47,740.29,236.16,8.80" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="6,466.60,716.38,47.04,8.80;6,102.11,728.34,190.61,8.80">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.308</idno>
	</analytic>
	<monogr>
		<title level="m" coord="6,310.85,728.34,202.62,8.80;6,101.47,740.29,127.95,8.80">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,102.11,66.25,411.53,8.80;7,102.11,78.21,411.52,8.80;7,101.57,90.16,357.07,8.80" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="7,164.85,78.21,330.26,8.80">Grafit: Learning fine-grained image representations with coarse labels</title>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv48922.2021.00091</idno>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m" coord="7,101.57,90.16,202.95,8.80">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-10">2021</date>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,102.11,110.09,412.91,8.80;7,101.85,122.05,411.78,8.80;7,102.11,134.00,345.49,8.80" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="7,252.39,122.05,261.25,8.80;7,102.11,134.00,207.60,8.80">Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework</title>
		<author>
			<persName coords=""><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno>CoRR, abs/2202.03052</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,102.11,153.93,411.52,8.80;7,102.11,165.88,411.35,8.80;7,101.47,177.84,236.16,8.80" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="7,425.67,153.93,87.97,8.80;7,102.11,165.88,177.80,8.80">Aggregated Residual Transformations for Deep Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.634</idno>
	</analytic>
	<monogr>
		<title level="m" coord="7,301.38,165.88,212.08,8.80;7,101.47,177.84,127.95,8.80">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">2017</date>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
