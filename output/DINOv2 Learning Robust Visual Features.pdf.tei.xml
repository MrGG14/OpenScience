<?xml version="1.0" encoding="UTF-8"?><TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transactions on Machine Learning Research (01/2024) DINOv2: Learning Robust Visual Features without Supervision</title>
				<funder ref="#_T8udwyv">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
				<funder ref="#_qJEYEnd #_VnP2MZT">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-02-02">2 Feb 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Timothée</forename><surname>Darcet</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Théo</forename><surname>Moutakanni</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Huy</forename><forename type="middle">V</forename><surname>Vo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Szafraniec</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vasil</forename><surname>Khalidov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Fernandez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Haziza</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mahmoud</forename><surname>Assran</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wojciech</forename><surname>Galuba</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Russell</forename><surname>Howes</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shang-Wen</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Rabbat</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vasu</forename><surname>Sharma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hervé</forename><surname>Jegou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Labatut</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
						</author>
						<title level="a" type="main">Transactions on Machine Learning Research (01/2024) DINOv2: Learning Robust Visual Features without Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-02-02">2 Feb 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">CF7A4FE0F316CBA2A4727A24BA185527</idno>
					<idno type="arXiv">arXiv:2304.07193v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-05-21T17:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Curated Data Embedding Deduplication Retrieval</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing generalpurpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model <ref type="bibr" target="#b37">(Dosovitskiy et al., 2021)</ref> with 1B parameters and distill it into a series of smaller models that surpass the best available general-purpose features, OpenCLIP <ref type="bibr" target="#b65">(Ilharco et al., 2021)</ref> on most of the benchmarks at image and pixel levels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning task-agnostic pretrained representations have become the standard in Natural Language Processing (NLP) <ref type="bibr" target="#b91">(Radford et al., 2019;</ref><ref type="bibr" target="#b93">Raffel et al., 2020;</ref><ref type="bibr" target="#b27">Chowdhery et al., 2022;</ref><ref type="bibr" target="#b63">Hoffmann et al., 2022;</ref><ref type="bibr" target="#b114">Touvron et al., 2023)</ref>. One can use these features "as they are", i.e., without fine-tuning, and achieve performances on downstream tasks that are significantly better than those produced by task-specific models <ref type="bibr" target="#b14">(Brown et al., 2020)</ref>. This success has been fueled by pretraining on large quantities of raw text using pretext objectives, such as language modeling <ref type="bibr" target="#b90">(Radford et al., 2017)</ref> or word vectors <ref type="bibr" target="#b34">(Devlin et al., 2019)</ref>, that require no supervision.</p><p>Following this paradigm shift in NLP, we expect similar "foundation" models to appear in computer vision <ref type="bibr" target="#b12">(Bommasani et al., 2021)</ref>. These models should generate visual features that work out of the box on any task, both at the image level, e.g., image classification, and pixel level, e.g., segmentation. Most promising efforts towards these foundation models focus on text-guided pretraining, i.e., using a form of textual supervision to guide the training of the features <ref type="bibr" target="#b68">(Joulin et al., 2016;</ref><ref type="bibr" target="#b78">Mahajan et al., 2018;</ref><ref type="bibr">Radford et al.,</ref> All the authors are affiliated to Meta, except Julien Mairal who is affiliated to Inria. Timothée Darcet and Pierre Fernandez have a co-affiliation with Inria. Théo Moutakanni has a co-affiliation with Université Paris Saclay. Alaaeldin El-Nouby has a co-affiliation with Inria and ENS-PSL. Correspondence: {qas, timdarcet, theomoutakanni, ajoulin, bojanowski}@meta.com 2021). This form of text-guided pretraining limits the information that can be retained about the image since captions only approximate the rich information in images, and complex pixel-level information may not surface with this supervision. Furthermore, these image encoders require aligned text-image corpora and hence, do not offer the flexibility of their text counterparts, that is, to learn from raw data alone.</p><p>An alternative to text-guided pretraining is self-supervised learning <ref type="bibr" target="#b15">(Caron et al., 2018;</ref><ref type="bibr" target="#b20">Chen et al., 2020;</ref><ref type="bibr" target="#b57">He et al., 2022)</ref> where features are learned from images alone. These approaches are conceptually closer to pretext tasks such as language modeling and can capture information at the image and pixel level <ref type="bibr" target="#b18">(Caron et al., 2021)</ref>. Additionally, the features output by self-supervised models have been shown to exhibit various useful properties, and have enabled enabled a wide variety of applications <ref type="bibr" target="#b0">(Amir et al., 2022;</ref><ref type="bibr" target="#b115">Tumanyan et al., 2022;</ref><ref type="bibr" target="#b83">Ofri-Amar et al., 2023;</ref><ref type="bibr" target="#b54">Hamilton et al., 2022)</ref>. However, despite their potential to learn generalpurpose features, most of the advances in self-supervised learning were made in the context of pretraining on a small curated dataset, ImageNet-1k <ref type="bibr" target="#b98">(Russakovsky et al., 2015)</ref>. Some efforts on scaling these approaches beyond ImageNet-1k have been attempted <ref type="bibr" target="#b16">(Caron et al., 2019;</ref><ref type="bibr" target="#b48">Goyal et al., 2021;</ref><ref type="bibr">2022a)</ref>, but they focused on uncurated datasets, which typically lead to a significant drop in the quality of the features. This is explained by the lack of control over the data quality and diversity, which are essential to produce good features.</p><p>In this work, we explore if self-supervised learning has the potential to learn general-purpose visual features if pretrained on a large quantity of curated data. We revisit existing discriminative self-supervised approaches that learn features at both the image and patch level, such as iBOT <ref type="bibr">(Zhou et al., 2022a)</ref>, and we reconsider some of their design choices under the lens of a larger dataset. Most of our technical contributions are tailored toward stabilizing and accelerating discriminative self-supervised learning when scaling in model and data sizes. These improvements make our approach around 2× faster and require 3× less memory than similar discriminative self-supervised methods, allowing us to leverage longer training with larger batch sizes.</p><p>Regarding pretraining data, we have built an automatic pipeline to filter and rebalance datasets from an extensive collection of uncurated images. This pipeline is inspired by pipelines used in NLP <ref type="bibr" target="#b123">(Wenzek et al., 2020)</ref>, where data similarities are used instead of external metadata and do not require manual annotation. A major difficulty when dealing with images in the wild is to rebalance concepts and avoid overfitting on a few dominant modes. In this work, a naive clustering approach works reasonably well to resolve this issue. We gathered a small but diverse corpus of 142M images to validate our approach. Figure <ref type="figure">2</ref>: Evolution of performance when scaling in parameters. We show performance on eight types of vision tasks, as presented in Sec. 7, and average metrics with each type. Features are extracted from our self-supervised encoders, DINOv2 (dark blue), and we compare them with self-supervised methods (pale orange), as well as weakly-supervised methods (dark pink). We report the best-performing weaklysupervised model's performance as a dashed horizontal line. Our family of models drastically improves over the previous state of the art in self-supervised learning and reaches performance comparable with weaklysupervised features. See Sec. 7 for a detailed analysis.</p><p>Finally, we provide a variety of pretrained visual models, called DINOv2, trained with different Vision Transformers (ViT) <ref type="bibr" target="#b36">(Dosovitskiy et al., 2016)</ref> architectures on our data. We release all the models and the code to retrain DINOv2 on any data. We validate the quality of DINOv2 on various computer vision benchmarks at both image and pixel levels as we scale them, as summarized in Fig. <ref type="figure">2</ref>. We conclude that selfsupervised pretraining alone is a good candidate for learning transferable frozen features that are competitive with the best openly available weakly-supervised models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Intra-image self-supervised training. A first family of self-supervised methods focuses on pretext tasks built from the image, i.e., extracting a signal from the image to be predicted from the rest of the image. This idea has become prevalent with the work of <ref type="bibr" target="#b35">Doersch et al. (2015)</ref>, where they train by predicting the context of a given patch. Many other pretext tasks were introduced based on, for example, re-colorizing images <ref type="bibr" target="#b132">(Zhang et al., 2016)</ref>, predicting transformations <ref type="bibr" target="#b45">(Gidaris et al., 2018)</ref>, inpainting <ref type="bibr" target="#b85">(Pathak et al., 2016)</ref> or patch re-ordering <ref type="bibr" target="#b82">(Noroozi &amp; Favaro, 2016;</ref><ref type="bibr" target="#b80">Misra &amp; Maaten, 2020)</ref>. Recently, the emergence of patch-based architectures, like ViTs, has led to a revisit of inpainting for pre-training <ref type="bibr" target="#b57">(He et al., 2022;</ref><ref type="bibr" target="#b5">Bao et al., 2021;</ref><ref type="bibr" target="#b40">El-Nouby et al., 2021)</ref>, potentially in feature space <ref type="bibr" target="#b3">(Assran et al., 2023;</ref><ref type="bibr" target="#b4">Baevski et al., 2022)</ref>. Of particular interest, <ref type="bibr" target="#b57">He et al. (2022)</ref> show that a masked auto-encoder (MAE) learns features that provide substantial improvements when finetuned on downstream tasks. This property of MAEs has been further validated on video <ref type="bibr" target="#b111">(Tong et al., 2022)</ref>, audio <ref type="bibr" target="#b127">(Xu et al., 2022)</ref>, and across other modalities <ref type="bibr" target="#b46">(Girdhar et al., 2023)</ref>. However, their features require supervised finetuning, while our features perform well out of the box.</p><p>Discriminative self-supervised learning. The second line of work, closer to ours, is using discriminative signals between images or groups of images to learn features. This family of methods has roots in early deep learning work <ref type="bibr" target="#b53">(Hadsell et al., 2006)</ref> but became popular with the emergence of instance classification methods <ref type="bibr" target="#b36">(Dosovitskiy et al., 2016;</ref><ref type="bibr" target="#b11">Bojanowski &amp; Joulin, 2017;</ref><ref type="bibr" target="#b125">Wu et al., 2018)</ref>. Several improvements Figure <ref type="figure">3</ref>: Overview of our data processing pipeline. Images from curated and uncurated data sources are first mapped to embeddings. Uncurated images are then deduplicated before being matched to curated images. The resulting combination augments the initial dataset through a self-supervised retrieval system.</p><p>were made based either on instance-level objectives <ref type="bibr" target="#b58">(Hénaff et al., 2019;</ref><ref type="bibr" target="#b56">He et al., 2020;</ref><ref type="bibr">Chen &amp; He, 2021;</ref><ref type="bibr" target="#b20">Chen et al., 2020;</ref><ref type="bibr" target="#b52">Grill et al., 2020;</ref><ref type="bibr" target="#b18">Caron et al., 2021)</ref> or clustering <ref type="bibr" target="#b15">(Caron et al., 2018;</ref><ref type="bibr" target="#b1">Asano et al., 2020;</ref><ref type="bibr" target="#b17">Caron et al., 2020)</ref>. These methods provide performant frozen features on standard benchmarks like ImageNet <ref type="bibr" target="#b98">(Russakovsky et al., 2015)</ref>, but they are hard to scale to larger model sizes <ref type="bibr">(Chen et al., 2021)</ref>. In this work, we revisit the training of these approaches in the context of large pretraining datasets and models.</p><p>In particular, we build on top of <ref type="bibr">Zhou et al. (2022a)</ref> that we find particularly suited for scaling.</p><p>Scaling self-supervised pretraining. A growing body of work has focused on the scaling abilities of self-supervised learning in terms of data and model size <ref type="bibr" target="#b16">(Caron et al., 2019;</ref><ref type="bibr" target="#b47">Goyal et al., 2019;</ref><ref type="bibr" target="#b109">Tian et al., 2021;</ref><ref type="bibr">Goyal et al., 2022a)</ref>. Most of these works use large quantities of uncurated data to train models without supervision. They show evidence that discriminative methods scale with data, but because of the poor quality of the pretraining data, most of the results are obtained by finetuning the features. Of particular interest, <ref type="bibr" target="#b48">Goyal et al. (2021)</ref> have also shown that these methods benefit from scaling in model size given enough pretrained data. This line of work questions the ability of self-supervised methods to work on any data while we focus on producing the best pretrained encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automatic data curation.</head><p>Our dataset construction borrows from the image retrieval community <ref type="bibr" target="#b121">(Weinzaepfel et al., 2021;</ref><ref type="bibr">Radenović et al., 2018b;</ref><ref type="bibr" target="#b7">Berman et al., 2019;</ref><ref type="bibr" target="#b38">Douze et al., 2009;</ref><ref type="bibr" target="#b110">Tolias et al., 2016;</ref><ref type="bibr" target="#b96">Revaud et al., 2019)</ref>. In particular, the use of retrieval to augment the training set has been studied in the context of semi-supervised learning <ref type="bibr" target="#b128">(Yalniz et al., 2019)</ref>. Similarly, others have used hashtags or other metadata <ref type="bibr" target="#b78">(Mahajan et al., 2018;</ref><ref type="bibr" target="#b92">Radford et al., 2021)</ref> or pretrained vision encoders <ref type="bibr" target="#b100">(Schuhmann et al., 2021;</ref><ref type="bibr">2022)</ref> to filter uncurated datasets. Unlike these works, we use no pretrained encoders, metadata nor supervision to filter images and leverage visual similarity between images. Our approach is inspired by text curation pipelines <ref type="bibr" target="#b123">(Wenzek et al., 2020)</ref>, where a language model is trained on Wikipedia to score texts extracted from an uncurated source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Processing</head><p>We assemble our curated LVD-142M dataset by retrieving, from a large pool of uncurated data, images that are close to those in several curated datasets. We describe below the main components in our data pipeline including the curated/uncurated data sources, the image deduplication step and the retrieval system. Our pipeline does not require any metadata or text and directly works with images, as shown in Fig. <ref type="figure">3</ref>. We refer the reader to appendix A for more details on our approach. Data sources. Our selection of curated datasets is detailed in the appendix (Table <ref type="table" target="#tab_6">15</ref>) and contains ImageNet-22k, the train split of ImageNet-1k, Google Landmarks and several fine-grained datasets. For the uncurated data source, we collect a raw unfiltered dataset of images from a publicly available repository of crawled web data. From each web page in the repository, we extract URL links of images from &lt;img&gt; tags. We discard URLs that are unsafe or restricted by domains, and post-process the downloaded images (PCA hash deduplication, NSFW filtering, and blurring identifiable faces). This results in 1.2B unique images.</p><p>Deduplication. We apply the copy detection pipeline of <ref type="bibr" target="#b87">Pizzi et al. (2022)</ref> to the uncurated data and remove near-duplicate images. This reduces redundancy and increases diversity among images. We also remove near-duplicates of images contained in the test or validation set of any benchmark used in this work.</p><p>Self-supervised image retrieval. We build our curated pretraining dataset by retrieving images from our uncurated data source that are close to images in our curated sources. In order to do this, we first compute an image embedding using a self-supervised ViT-H/16 network pretrained on ImageNet-22k, and use cosine-similarity as a distance measure between images. Then, we perform k-means clustering of the uncurated data. Given a query dataset for retrieval, if it is large enough we retrieve N (typically 4) nearest neighbors for each query image. If it is small, we sample M images from the cluster corresponding to each query image. Although visual inspection seemed to indicate good retrieval quality for N much larger than 4, this leads to more collisions (images that are nearest-neighbor retrievals of multiple queries). We choose N = 4 as it provides a good tradeoff in that sense.</p><p>Implementation Details. The deduplication and retrieval stages of our pipeline rely on the Faiss library <ref type="bibr" target="#b67">(Johnson et al., 2019)</ref> to efficiently index and compute batch searches of nearest embeddings. In particular, we heavily leverage its support for GPU-accelerated indices, using inverted file indices with product quantization codes <ref type="bibr" target="#b66">(Jegou et al., 2010)</ref>. The whole processing is distributed on a compute cluster of 20 nodes equipped with 8 V100-32GB GPUs and takes less than two days to produce the LVD-142M dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discriminative Self-supervised Pre-training</head><p>We learn our features with a discriminative self-supervised method that can be seen as a combination of DINO and iBOT losses with the centering of SwAV <ref type="bibr" target="#b17">(Caron et al., 2020)</ref>. We also add a regularizer to spread features and a short high-resolution training phase. We rapidly introduce each of these approaches, but more details can be found in the related papers, or in our open-sourced code.</p><p>• Image-level objective <ref type="bibr" target="#b18">(Caron et al., 2021)</ref>. We consider the cross-entropy loss between the features extracted from a student and a teacher network. Both features are coming from the class token of a ViT, obtained from different crops of the same image. We pass the student class token through the student DINO head. This head is an MLP model outputting a vector of scores, that we call "prototype scores". We then apply a softmax to obtain p s . Similarly, we apply the teacher DINO head to the teacher class token to obtain teacher prototype scores. We then apply a softmax followed by a centering with moving average (or a Sinkhorn-Knopp centering as detailed thereafter) to obtain p t . The DINO loss term corresponds to:</p><formula xml:id="formula_0">L DIN O = - p t log p s</formula><p>We learn the parameters of the student and build the teacher head with an exponential moving average of past iterates <ref type="bibr" target="#b56">(He et al., 2020)</ref>.</p><p>• Patch-level objective <ref type="bibr">(Zhou et al., 2022a)</ref>. We randomly mask some of the input patches given to the student, but not to the teacher. We then apply the student iBOT head to the student mask tokens. Similarly, we apply the teacher iBOT head to the (visible) teacher patch tokens corresponding to the ones masked in the student. We then apply the softmax and centering steps as above, and obtain the iBOT loss term:</p><formula xml:id="formula_1">L iBOT = - i p ti log p si</formula><p>, where i are patch indices for masked tokens. Similarly to above, we learn the parameters of the student, and build the teacher head through exponential moving average.</p><p>• Untying head weights between both objectives. Both the DINO and the iBOT loss use a learnable MLP projection head. It is applied to the output tokens and the loss is compute atop. In <ref type="bibr">Zhou et al. (2022a)</ref>, an ablation study shows that sharing parameters between the DINO and iBOT heads leads to better performance. At scale, we observed that the opposite is true, and we therefore use two separate heads in all our experiments.</p><p>• Sinkhorn-Knopp centering <ref type="bibr" target="#b17">(Caron et al., 2020)</ref>. <ref type="bibr" target="#b97">Ruan et al. (2023)</ref> recommend to replace the teacher softmax-centering step of DINO and iBot by the Sinkhorn-Knopp (SK) batch normalization of SwAV <ref type="bibr" target="#b17">(Caron et al., 2020)</ref>. We run the Sinkhorn-Knopp algorithm steps for 3 iterations. For the student, we apply the softmax normalization.</p><p>• KoLeo regularizer <ref type="bibr" target="#b99">(Sablayrolles et al., 2019)</ref>. The KoLeo regularizer derives from the Kozachenko-Leonenko differential entropy estimator (see <ref type="bibr" target="#b6">Beirlant et al. (1997)</ref>; <ref type="bibr" target="#b32">Delattre &amp; Fournier (2017)</ref>) and encourages a uniform span of the features within a batch. Given a set of n vectors (x 1 , . . . , x n ), it is defined as</p><formula xml:id="formula_2">L koleo = - 1 n n i=1 log(d n,i ),</formula><p>where d n,i = min j̸ =i ∥x i -x j ∥ is the minimum distance between x i and any other point within the batch. We also ℓ 2 -normalize the features before computing this regularizer.</p><p>• Adapting the resolution <ref type="bibr" target="#b112">(Touvron et al., 2019)</ref>. Increasing image resolution is key to pixellevel downstream tasks such as segmentation or detection, where small objects disappear at low resolutions. However, training at high resolution is time and memory demanding, and instead, we increase the resolution of images to 518 × 518 during a short period at the end of pretraining. This is also similar to UniViT training from <ref type="bibr" target="#b76">Likhomanenko et al. (2021)</ref> and FlexiViT training from <ref type="bibr" target="#b9">Beyer et al. (2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Efficient implementation</head><p>We consider several improvements to train models at a larger scale. We train models on A100 GPUs using PyTorch 2.0. The code and pretrained models are made available under Apache 2.0 license<ref type="foot" target="#foot_0">1</ref> . The details of our models are in the appendix, Table <ref type="table" target="#tab_8">17</ref>. With the same hardware, compared to the iBOT implementation, the DINOv2 code runs around 2× faster using only 1/3 of the memory.</p><p>Fast and memory-efficient attention. We implemented our own version of FlashAttention <ref type="bibr" target="#b30">(Dao et al., 2022)</ref> to improve memory usage and speed on the self-attention layers. Our version is on par with or better than the original on all cases considered, while covering more use-cases and hardware. Due to the GPU hardware specifics, the efficiency is best when the embedding dimension per head is a multiple of 64, and the matrix operations are even better when the full embedding dimension is a multiple of 256. As a consequence, our ViT-g architecture slightly differs from the architecture proposed by <ref type="bibr" target="#b131">Zhai et al. (2022)</ref> in order to maximize compute efficiency, and we use an embedding dimension of 1536 with 24 heads (64 dim/head), rather than 1408 with 16 heads (88 dim/head). Our experiments did not show significant differences in final accuracy, and our ViT-g backbone counts 1.1B parameters.</p><p>Sequence packing. The DINO algorithm requires forwarding both large crops (at resolution 224) and small crops (resolution 98). When split into patches, these two groups are represented by token sequences of different lengths and cannot be forwarded together. In order to accelerate training, we use a trick called "sequence packing," which originates from NLP <ref type="bibr" target="#b71">(Krell et al., 2022)</ref>. The idea is simple: we concatenate the sequences we must forward through the transformers into a single long sequence. We pass this sequence through the transformer blocks as usual. However, a block-diagonal mask is applied to the self-attention matrix in attention layers, preventing attention between different sequences. This way, the forward is strictly equivalent to forwarding each sequence separately. This trick gives us significant compute efficiency gains compared to using separate forward and backward passes, as in prior implementations. The lower-level components of our setup are available in the xFormers library<ref type="foot" target="#foot_1">2</ref> (Lefaudeux et al. ( <ref type="formula">2022</ref>)).</p><p>Efficient stochastic depth. We implement an improved version of stochastic depth <ref type="bibr" target="#b64">(Huang et al., 2016)</ref> that skips the computation of the dropped residuals rather than masking the result. This saves memory and compute in proportion approximately equal to the drop rate, thanks to specific fused kernels. With high drop rates (d = 40% in this work), this allows a drastic improvement in compute efficiency and memory usage. The implementation consists of randomly shuffling the B samples over the batch dimension, and slicing the first (1 -d) × B samples for the computations in the block.</p><p>Fully-Sharded Data Parallel (FSDP). Minimizing our objective with the AdamW optimizer requires 4 model replicas in float32 precision -student, teacher, optimizer first moments, optimizer second moments. This sums to 16 GB of memory for a billion-parameter model such as our ViT-g. In order to reduce this memory footprint per GPU, we split the model replicas across GPUs, i.e., sharding 16 GB across GPUs using the PyTorch implementation of FSDP. Consequently, the model size is not bounded by the memory of a single GPU but by the total sum of GPU memory across compute nodes. The Pytorch implementation of FSDP brings a second advantage, which is to save on the cross-GPU communication costs: the weight shards are stored in float32 precision as required by the optimizer, but broadcasting weights and reducing gradients is done in float16 precision for the backbone (MLP heads gradients are reduced in float32 to avoid training instabilities). This leads to approximately 50% reduction in communication costs compared to the float32 gradient all-reduce operation used in DistributedDataParallel (DDP), which is used in other self-supervised pretraining methods <ref type="bibr" target="#b18">(Caron et al., 2021;</ref><ref type="bibr">Zhou et al., 2022a)</ref>. As a consequence, the training procedure scales more efficiently than DDP with float16 autocast when scaling the number of GPU nodes. Overall, Pytorch-FSDP mixed-precision is superior to DDP with autocast in virtually all cases we encountered.</p><p>Model distillation. Most of our technical improvements to the training loop aim at improving the training of large models over large quantities of data. For smaller models, we distill them from our largest model, the ViT-g, instead of training them from scratch. Knowledge distillation <ref type="bibr" target="#b62">(Hinton et al., 2014)</ref> aims at reproducing the output of a large model with a smaller model by minimizing some distance between both outputs for a set of given inputs. Since our objective function is a form of distillation from the teacher network to the student network, we leverage the same training loop with a few exceptions: we use a larger model as a frozen teacher, keep a spare EMA of the student that we use as our final model, remove the masking and stochastic depth, and, apply the iBOT loss on the two global crops. In our ablations, we observe that this approach achieves better performance than training from scratch, even for a ViT-L. Our distillation method ends up close to the one described by <ref type="bibr" target="#b39">Duval et al. (2023)</ref>, except we do not modify the loss terms for distillation and evaluate the EMA of the student.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Ablation Studies</head><p>We present a set of ablations to empirically validate different components of our pipeline: the technical modifications described in Sec. 4, the pretraining data and the impact of model distillation. We consider various downstream tasks that are described in Sec. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Improved Training Recipe</head><p>Our approach improves over the iBOT method by combining it with several existing components described in Sec. 4. To evaluate their importance, we train multiple models where we successively add components to a baseline iBOT model. We report the Top-1 accuracy on the validation set of ImageNet-1k with a k-NN  <ref type="table" target="#tab_0">1</ref>. Generally, we observe that each component improves the performance on either k-NN or linear probing and even both in most cases. Only LayerScale and Stochastic Depth incur a performance drop in linear probing but significantly improve the training stability in our experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Pretraining Data Source</head><p>The quality of features is directly related to the quality of the pretraining data. In this experiment, we probe the impact of LVD-142M compared to ImageNet-22k, a commonly used pretraining dataset, or using directly raw and uncurated data. For the uncurated dataset, we randomly sample 142 million images from the same data source as LVD-142M. We train a ViT-g/14 on each dataset for the same number of iterations.</p><p>We also include a variant of ImageNet-22k obtained by removing the synsets of ImageNet-1k (INet-22k \ INet-1k) for completeness. We report the comparisons in Table <ref type="table" target="#tab_1">2</ref>.</p><p>The most salient observation is that training on a curated set of images works better on most benchmarks than training on uncurated data. This confirms the benefit of curating data, even in the case of selfsupervised pretraining. When compared with models trained on ImageNet-22k, training on LVD-142M is also superior on all the benchmarks but ImageNet-1k. This confirms that training on a more diverse set of images improves the quality of the features in domains that are not covered by ImageNet-22k. We also see that training on our curated data increases the performances on domains that are not used for the curation process <ref type="bibr">(INaturalist 2018, 2021 and</ref><ref type="bibr">Places205)</ref>, proving that scale and diversity can benefit unseen domains.  Overall, the conclusion of this ablation is that our dataset provides a good balance of different types of images that leads to the best performance overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Model Size and Data</head><p>We quantify the importance of scaling data with the model size in Fig. <ref type="figure">4</ref>. As the size of models grow, training on LVD-142M becomes more beneficial than training on ImageNet-22k. For instance, a ViT-g trained on LVD-142M matches the performance on ImageNet-1k of a model trained on ImageNet-22k while significantly outperforming it on the other benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Loss Components</head><p>We validated the proposed technical improvements in Sec. 6.1 by adding them incrementally. This section analyzes the performance hit observed if we ablate specific loss terms, starting from our best-performing model. We ablate the importance of the KoLeo loss and the impact of the masked image modeling term.</p><p>For both, we report performance on ImageNet-1k using a linear classifier, ADE-20k segmentation using a linear classifier, and nearest-neighbor image retrieval on Oxford-M. Table <ref type="table" target="#tab_3">3a</ref> shows the impact of using the KoLeo loss. We see that the instance retrieval performance improves by more than 8%, confirming that this term helps spread features in the output space. At the same time, the other metrics do not suffer from this regularization. In Table <ref type="table" target="#tab_3">3b</ref>, we show the impact of using the masked image modeling term from iBOT. This term is critical for dense prediction tasks, leading to almost 3% performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Impact of Knowledge Distillation</head><p>For small architectures, we distill larger models instead of training them from scratch. We use the distillation procedure described in Sec. 5. We evaluate the effectiveness of this approach by comparing a ViT-L/14 trained from scratch with one distilled from a ViT-g/14 over 12 benchmarks in Fig. <ref type="figure" target="#fig_2">5</ref>. We also report the performance of the ViT-g/14 used for distillation as a topline. The distilled model outperforms the one trained from scratch on all 12 benchmarks, validating our pretraining approach for small models.  Comparison between a ViT-L trained from scratch or distilled from DINOv2 using ViT-g/14. For reference, we also report the performance of the ViT-g/14 teacher. We show that a ViT-L model distilled from a frozen ViT-g outperforms a the same model trained from scratch on all benchmarks, sometimes even outperforming the distillation target.  <ref type="bibr">("224" and "416")</ref> or trained at 224 then 416 for a short duration ("224→416"). We train linear classifiers on top of frozen features at different resolutions and report Top-1 accuracy on ImageNet and mIoU on ADE-20k. We observe that performing SSL training at high resolution for a short duration achieve behavior and results close to training at the same high resolution for the full training, at a fraction of the cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Impact of Resolution</head><p>We measure the impact of changing the resolution during the pretraining on the performance of image and patch-level features. We consider models trained from scratch using a fixed resolution of either 224 × 224 or 416 × 416, and a model trained from scratch at 224 × 224, then resumed for 10k more iterations at 416 × 416. High-resolution training is compute-intensive, so we conduct this ablation on a small setup: a ViT-L/16 trained on ImageNet1k. In Fig. <ref type="figure" target="#fig_3">6</ref>, we report the performance of a linear probe on ImageNet-1k and ADE-20k, evaluated at various resolutions. The model trained on high-resolution images performs best across resolutions, but this comes at a high cost: training at 416 is approximately 3 × more compute-intensive than training at 224. On the other hand, training at high resolution for only 10k iterations at the end of the training is almost as good and only requiring a fraction of the compute. As a consequence, we include this step at the end of the training rather than training at a high resolution from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>In this section, we present the empirical evaluation of our models on many image understanding tasks. We evaluate both global and local image representations, on category and instance-level recognition, semantic segmentation, monocular depth prediction, and action recognition. We detail the list of benchmarks in Appendix C. The goal of this evaluation is twofold. First, we show that our self-supervised features outperform the current state of the art by a very large margin. Second, we show that they match, or surpass the performance of weakly-supervised ones on a substantial number of tasks.</p><p>Baselines. In our comparisons, we use two kinds of models as baselines. We compare to the best performing self-supervised models that are openly available. First, we run our evaluations for MAE <ref type="bibr" target="#b57">(He et al., 2022)</ref>, DINO <ref type="bibr" target="#b18">(Caron et al., 2021)</ref>, SEERv2 <ref type="bibr">(Goyal et al., 2022a)</ref>, MSN <ref type="bibr" target="#b2">(Assran et al., 2022)</ref>, EsViT <ref type="bibr">(Li et al., 2022a)</ref>, Mugs <ref type="bibr">(Zhou et al., 2022b)</ref> and iBOT <ref type="bibr">(Zhou et al., 2022a)</ref>. When several architectural variants were proposed for a given method, we report results for the one that leads to best top-1 accuracy on ImageNet-1k. Second, we report performance of open-source weakly-supervised models such as CLIP <ref type="bibr" target="#b92">(Radford et al., 2021)</ref>, OpenCLIP <ref type="bibr" target="#b65">(Ilharco et al., 2021;</ref><ref type="bibr" target="#b26">Cherti et al., 2023)</ref>, and SWAG <ref type="bibr" target="#b104">(Singh et al., 2022)</ref>. When evaluating models on ImageNet-1k, we report the performance for each of the aforementioned methods. For all other evaluations, we report the four best-performing models amongst SSL ones. Also, for reference, we report the best performing OpenCLIP-G for weakly-supervised ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">ImageNet Classification</head><p>As a first evaluation, we probe the quality of the holistic image representation produced by the model on the ImageNet-1k classification dataset. We evaluate the quality of features by training a simple classifier over a frozen backbone, and do not perform finetuning of the backbone weights. Following previous work, we use a linear model for simplicity, ensuring a reproducible evaluation, despite the fact that classes may not be linearly separable. Because most SSL methods were developped using ImageNet-1k validation performance as a debugging signal, we also report the top-1 accuracy on ImageNet-ReaL and ImageNet-V2. In order to report this additional validation performance, for all models, we run the evaluation with our code. We compare our frozen features to the best publicly available SSL features in Table <ref type="table">4</ref>, regardless of architecture or pretraining data. We see the components proposed in this work lead to a very significant improvement (+4.2%) over the previous state of the art (iBOT ViT-L/16 trained on ImageNet-22k) on linear evaluation. At the same time, we also see that the performance increase on the alternative test sets is larger for our method, indicating stronger generalization. We describe details of our linear evaluation in Appendix B.3.</p><p>How far are we from weakly-supervised models? We also want to validate that our features are competitive with state-of-the-art open-source weakly supervised models. To this end, we compare on ImageNet-1k, using the linear evaluation, to three off-the-shelf methods with several architectural variants. For all models, we run the linear evaluation using our code, after making sure that our numbers match those reported in technical reports and papers. We show the result of this evaluation in Table <ref type="table">4</ref>. We see that our backbone, surpases the performance of OpenCLIP with a ViT-G/14 architecture (+0.3%) and EVA-CLIP with a ViT-g/14 (+0.1%). At the same time, we also observe that our performance on the ImageNet-V2 test set is significantly better (+1.1% versus EVA-CLIP), indicating better generalization. For the remainder of this section, we report OpenCLIP-G as a reference for weakly-supervised models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Can we finetune the encoders?</head><p>We question if the ability of our models to produce high quality frozen features impact their performance when finetuned with supervision on a specific dataset. While this is not core to this paper, this experiment is indicative of whether we have involuntarily specialized our models to the setting of linear evaluations of frozen features. To run this sanity check, we apply the finetuning pipeline from <ref type="bibr" target="#b113">Touvron et al. (2022)</ref>, without tweaking hyper-parameters. In Table <ref type="table" target="#tab_6">5</ref>, we show that the Top-1 accuracy on the validation set of ImageNet-1k improves by more than +2% when the backbone is finetuned. This is true both when using models at resolution 224 and 448. Further gains can be obtained by tuning the hyper-parameters of the finetuning, but this is beyond the goal of this sanity check. Nonetheless, our best finetuned performance (88.9%) is only a couple of percent below <ref type="bibr">(-2.2%)</ref>  Table <ref type="table">4</ref>: Linear evaluation on ImageNet-1k of frozen pretrained features. We report Top-1 accuracy on the validation set for publicly available models trained on public or private data, and with or without text supervision (text sup.). For reference, we also report the kNN performance on the validation set. We compare across any possible architectures (Arch.), at resolution 224 × 224 unless stated otherwise. The dataset used for training EVA-CLIP is a custom mixture, see paper for details <ref type="bibr" target="#b42">(Fang et al., 2023)</ref>.</p><p>arts (91.1%), obtained by <ref type="bibr">Chen et al. (2023a)</ref>. As DINOv2 leads to features that are strong in both the linear and finetuning settings, a strong property of our approach is that finetuning is optional. Robustness analysis. To complement our study, and probe the generalization of our features, we evaluate our ImageNet-1k models trained with linear classification heads on domain generalization benchmarks. We use the best performing linear classifier as described above and simply run inference on those benchmarks. Please note that most results in the literature are obtained with models that are finetuned end-to-end on ImageNet-1k. We show the result of this experiment in Table <ref type="table" target="#tab_7">6</ref>. When comparing with state-of-the-art SSL methods, our models shows drastically better robustness (+29.6% on A <ref type="bibr">(Hendrycks et al., 2021b)</ref>, +22.1% on R <ref type="bibr">(Hendrycks et al., 2021a)</ref> and +23.0% on Sketch <ref type="bibr" target="#b119">(Wang et al., 2019)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Arch</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Additional Image and Video classification Benchmarks</head><p>In this section we study the generalization of our features on downstream classification benchmarks. We consider two sets of evaluations in that context. On one hand, we use large and finegrained datasets such as iNaturalist and Places205. On the other, we use the 12 image classification tasks originally proposed in SimCLR <ref type="bibr" target="#b20">(Chen et al., 2020)</ref>. For iNaturalist 2018, iNaturalist 2021, and Places205, we train a linear classifier with data augmentations as in Sec. 7.1 We report top-1 accuracy for those three datasets in Table <ref type="table" target="#tab_8">7</ref>. Interestingly, our model significantly outperforms OpenCLIP ViT-G/14 on both variants of iNaturalist (+8.6% and +9.7% for 2018 and 2021 respectively), and lags slightly behind on Places 205 (-2.3%).</p><p>In a second set of evaluations, we measure the performance of our model on video action recognition even though our features were not trained on videos.. We evaluated features on three datasets, namely UCF-101 <ref type="bibr" target="#b106">(Soomro et al., 2012)</ref>, Kinetics-400 <ref type="bibr" target="#b69">(Kay et al., 2017)</ref> and Something-Something v2 <ref type="bibr" target="#b51">(Goyal et al., 2017)</ref>.</p><p>For this evaluation, we pick 8 evenly spaced frames in the video and train a linear classifier on the average of the features for UCF and K-400. For SSv2, we opt for concatenation to retain more temporal information than with feature averaging. For each dataset, we measure average accuracy and report the results in Table <ref type="table" target="#tab_8">7</ref>. We see that amongst self-supervised approaches, our model clearly sets a new state of the art. Moreover, our model matches the accuracy of the OpenCLIP features on UCF and Kinetics (+0.1% and +0.5% respectively) and clearly outperforms them on SSv2 (+2.5%). This is particularly interesting, as SSv2 requires a much richer understanding of the video frames.</p><p>Finally, in Table <ref type="table">8</ref>, we compare selected frozen features on 12 transfer classification benchmarks initially proposed by <ref type="bibr" target="#b20">Chen et al. (2020)</ref>. This benchmark covers scenes, objects (food, cars, planes), and textures.  <ref type="formula">2018</ref>) respectively -are mentioned at the top of the Table . For reference, using the Mask2Former pipeline <ref type="bibr" target="#b107">(Steiner et al., 2021)</ref> with a ViT-Adapter <ref type="bibr">(Chen et al., 2023b)</ref> on top of our frozen ViT-g/14 backbone gives 60.2 mIoU on ADE-20k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Dense Recognition Tasks</head><p>We probe the quality of patch-level features extracted from our network on several dense downstream tasks. We consider semantic image segmentation and monocular depth estimation in several settings and we conduct evaluations on multiple datasets for each.</p><p>Semantic segmentation. For our semantic segmentation evaluation, we consider two different setups. Linear: a linear layer is trained to predict class logits from a patch tokens. It is used to produce a lowresolution logit map (eg 32x32 for a model with patch size 16), which is then upsampled to full resolution (512x512) to obtain a segmentation map. This procedure is extremely simple but cannot easily produce high-resolution segmentations. +ms: a boosted version of the linear setup. We concatenate the patch tokens of the 4 last layers, use a larger image resolution of 640, and use multiscale test-time augmentations to improve the predictions. We report the performance of our model variants as well as the baselines on three datasets under the two setups in Table <ref type="table" target="#tab_9">10</ref>.</p><p>Our models show very good performance on all datasets and for all setups. Interestingly, our evaluation using +ms is on par with fully finetuning MAE with an Upernet decoder (53.0 versus 53.6 mIoU). This is surprising because we use a significantly simpler predictor. Also, our best model, when evaluated using the boosted recipe, almost matches the state of the art on Pascal VOC (86.2 versus 89.0 mIoU).</p><p>Frozen backbone in a SOTA pipeline. In a final experiment, we freeze our backbone, and plug it into a ViT-Adapter Chen et al. (2023b) with a Mask2former head <ref type="bibr" target="#b25">(Cheng et al., 2022)</ref>. We tune the weights of the adapter and head, but keep the backbone frozen, meaning 66% of the weights are frozen. This allows for a lighter segmentation training than full end-to-end fine-tuning. With this setup, we reach 60.2 mIoU on ADE20k, close to the competitive state of the art, standing at 62.9 mIoU <ref type="bibr" target="#b118">(Wang et al., 2022)</ref>. Although our setup for this experiment doesn't makes use of the optimisations described in Sec. 5, the segmentation training in this experiment took 28 hours on 16 V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth estimation.</head><p>In this experiment, we evaluate our patch-level features on three monocular depth estimation benchmarks: NYUd, KITTI and zero-shot transfer from NYUd to SUN3d. We follow the evaluation protocol of <ref type="bibr">Li et al. (2022b)</ref>. We consider three different setups for this evaluation. lin. 1: we extract the last layer of the frozen transformer and concatenate the [CLS] token to each patch token. Then we bi-linearly upsample the tokens by a factor of 4 to increase the resolution. Finally we train a simple linear layer using a classification loss by dividing the depth prediction range in 256 uniformly distributed bins and  <ref type="bibr">(2021)</ref>. lin. 4: we use the same protocol that we use with one layer, but concatenate the tokens from layers l = {3, 6, 9, 12} for ViT-S/B, l = {5, 12, 18, 24} for ViT-L, and l = {10, 20, 30, 40} for ViT-g. DPT: we use the DPT decoder <ref type="bibr" target="#b94">(Ranftl et al., 2021)</ref> on top of our frozen models and setup a regression task. We scale the size of the head following the dimension of the features for each architecture. We show results for all baselines, all datasets and all setups in Table <ref type="table" target="#tab_10">11</ref>.</p><p>From this table, we see that our features clearly surpass the best SSL and WSL features available. It is interesting to see that iBOT features extracted from a ViT-L outperform the ones of OpenCLIP with a ViT-G. This observation supports the intuition that caption-based feature learning fails to learn subtle patterns like this one. Also, our model, with the DPT decoder and frozen backbone, matches or exceeds the performance of the recent work of <ref type="bibr">Li et al. (2022b)</ref>. Finally, the out-of-domain generalization result on SUN-RGBd shows that our features allow very good transfer between domains. A depth prediction module trained on indoor scenes from NYUd generalizes pretty well to the outdoor examples of SUN-RGBd.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Qualitative Results</head><p>In this final section of the empirical evaluation of our features, we propose a few qualitative analyses.</p><p>Semantic Segmentation and Depth Estimation. We show some qualitative results for our dense prediction evaluations: segmentation on ADE20K in Fig. <ref type="figure" target="#fig_4">7</ref> and depth estimation on NYUd, KITTI and SUN RGB-D in Fig. <ref type="figure" target="#fig_4">7</ref>. We compare DINOv2 with OpenCLIP with a linear classifier on each dataset. While not perfect, the linear segmentation model using our DINOv2 backbone produces good results and behaves much better than the OpenCLIP one under this evaluation setup. Indeed, the segmentation mask produced by OpenCLIP-G shows many artifacts and disconnected components. The qualitative results on depth estimation clearly illustrate the quantitative gap between OpenCLIP and DINOv2. These results highlight that our features, as well as the features extracted from OpenCLIP, are able to linearly separate complex information such as depth, even though neither was trained with this type of information. However, our features lead to a much smoother depth estimation, with less artifacts. Some objects such as the chair on the SUN RGB-D image are completely ignored by OpenCLIP and correctly positioned using our features.</p><p>Out-of-distribution generalization. We show a few examples of applying the depth prediction and segmentation linear classifiers to out-of-distribution examples in Fig. <ref type="figure" target="#fig_5">8</ref>. The qualitative results support our claim that our features transfer between domains. The quality of the depth and segmentation predicted for pictures of animals, or paintings is very good, even though the domains are very different.  PCA of patch features. We show the results of the principal component analysis (PCA) performed on the patch features extracted by our model. We keep only patches with a positive value after we threshold the first component. This procedure turns out to separate the image's main object from the background. We compute a second PCA on the remaining patches across three images depicting the same category. We color the three first components with three different colors and present the results in Fig. <ref type="figure" target="#fig_0">1</ref> and<ref type="figure" target="#fig_6">9</ref>. There are two interesting observations: first, our unsupervised foreground / background detector, based on detecting the highest variance direction, performs very well and is capable of delineating the boundary of the main object in the picture. Second, the other components correspond to "parts" of objects and match well for images of the same category. This is an emerging property -our model was not trained to parse parts of objects. Patch matching. Finally, we explore what type of information our patch-level features contain by matching them across images. We start by detecting the foreground object using the procedure described above. Then, we compute the euclidean distance between patch features extracted from two images and map them by solving an assignment problem. In order to reduce the number of matches, we then apply a non-maximum suppression to keep only the salient ones. In Fig. <ref type="figure" target="#fig_0">10</ref>, we show some examples of such matchings.</p><p>We observe that the features seem to capture information about semantic regions that serve similar purpose in different objects or animals. For instance, the wing of a plane matches the wing of a bird. We also observe that the model is robust to style (image versus drawing), and to large variation of poses (see the elephant).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Fairness and Bias Analysis</head><p>We conduct two fairness evaluations of our models. We probe for geographical fairness and potential harmful label associations. For both evaluations, we experiment with our largest ViT-g model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Geographical Fairness</head><p>We evaluate geographical fairness on the Dollar Street dataset introduced in De Vries et al. ( <ref type="formula">2019</ref>) using the evaluation protocol of <ref type="bibr">Goyal et al. (2022b)</ref>. This benchmark compares performance across countries and income levels. It contains 16,073 images from 289 households across 54 countries. The task is to recognize 94 concepts that vary visually between households based on income or location. In Table <ref type="table" target="#tab_11">12</ref>, we compare our model with SEERv2 <ref type="bibr">(Goyal et al., 2022a)</ref>, a model trained on a geographically diverse set of images. Our model is slightly fairer across regions and incomes than the SEERv2 model and significantly better than the supervised baseline reported by <ref type="bibr">Goyal et al. (2022a)</ref>. However, we still observe a significant difference between regions, particularly in Africa, where our model performance drops by 25.7% compared to Europe. This shows that our model is still biased toward Western countries. Similarly, our model performs Figure <ref type="figure" target="#fig_0">10</ref>: Matching across images. We match patch-level features between images from different domains, poses and even objects that share similar semantic information. This exhibits the ability of our model to transfer across domains and understand relations between similar parts of different objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Income buckets Regions</head><p>Method Arch. significantly better on high-income households than low-income ones, with a difference of 31.7%. Despite improvements, we observe significant biases in our models toward wealthy households from Western countries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Gender, Skintones and Age</head><p>In a second set of evaluations, we question how our model classifies images of people of different gender, skin tone, and age (all self-reported). We follow the protocol of <ref type="bibr">Goyal et al. (2022b)</ref>, where we train a multiclass classifier on a subset of 619 classes of ImageNet-22k. We group the 619 classes into four broader categories: Human, Possibly Human, Non-Human, or Crime. Non-Human and Crime are considered harmful. Using this classifier, we run inference on 2955 images from the Casual Conversations dataset <ref type="bibr" target="#b55">(Hazirbas et al., 2021)</ref> and keep all labels in the top-5 that are assigned a probability of 0.1 or more. Because of that, we can assign multiple classes to each image. We make one modification to the original evaluation protocol: we do not backpropagate gradients to the backbone and keep it frozen. We compare our model to SEERv2 in Table <ref type="table" target="#tab_12">13</ref>. Table <ref type="table" target="#tab_13">14</ref>: Carbon footprint of reproducing DINOv2. We report the potential carbon emission of reproducing DINOv2-g when assuming a power consumption for the A100-40GB of 400W, a PUE of 1.1 and carbon intensity factor of 0.385 kg CO 2 e per KWh.</p><p>Our model often classifies images of all groups as Human without large deviations across skin tones. Neither SEERv2 nor DINOv2 predict harmful labels from the Non-Human or Crime meta-categories (except for two instances where the background contains bars visually similar to prison bars). We see that our model triggers the Possibly-Human classes often. This class is constructed from objects in ImageNet-22k that are often related to Humans, such as Scarf, Glasses, or Beard. Our model often predicts the Possibly-Human class for men because of the prevalence of the Beard class. No clear pattern indicates a bias against a particular group in this study. While this is encouraging, we also acknowledge that a more thorough evaluation of biases may reveal flaws in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Estimating the Environmental Impact of Training our Models</head><p>Training foundation models consumes a significant amount of energy, resulting in carbon dioxide emissions. <ref type="bibr" target="#b86">Patterson et al. (2021)</ref> propose a methodology to report an estimation of the carbon emitted during the training of a model based on the specifics of the data center and its power grid. This computation informs the design of the data center used for the training of models and the choice of location for data centers. This methodology requires to know the specifics of the data center used for training, which can be complex when multiple data centers are involved over time. Additionally, these specifics are most often not in the control of the AI practitioner, and hence, this methodology is less helpful when practioners make technical decisions about future trainings. Instead, in this section, we follow an alternative that reports the potential carbon emission of retraining a similar model in an average data center located in the US. This methodology was used in previous work in natural language processing <ref type="bibr" target="#b108">(Strubell et al., 2019;</ref><ref type="bibr" target="#b114">Touvron et al., 2023)</ref> to establish an apple-to-apple comparison between pretraining schemes. More precisely, we fix the value of all exogenous variables, i.e., the Power Usage Effectiveness (PUE) and carbon intensity factor of a power grid to the same values as in <ref type="bibr" target="#b114">Touvron et al. (2023)</ref>, that is, a PUE of 1.1 and the carbon intensity factor to the US average of 0.385 kg CO 2 eq/KWh. We use the same formula as in <ref type="bibr" target="#b86">Patterson et al. (2021)</ref> to estimate the potential energy consumption and the carbon emission. For the power consumption of an A100-80GB, we take the thermal design power for NVLink systems, which is 400W. We report the potential carbon emission of retraining a DINOv2 ViT-g in Carbon footprint of the whole project. Additionally, we estimate the footprint of the whole project to be between 0.5k and 1k tCO 2 eq using the same grid as presented above<ref type="foot" target="#foot_2">3</ref> . This carbon footprint represents in the order of 200k GPU-days. The primary sources of emissions are the self-supervised pre-trainings of the models. For example, a single pre-training of a ViT-g model (22k GPU-hours) emits 3.7 tons of CO 2 eq, while a finetuning on ImageNet-1k (1k GPU-hours) emits 0.2 tons. This estimate only considers the GPUs' electricity consumption and ignores other emissions, such as their manufacturing and disposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Future work and Discussion</head><p>In this work, we present DINOv2, a new series of image encoders pretrained on large curated data with no supervision. This is the first SSL work on image data that leads to visual features that close the performance gap with (weakly) supervised alternatives across a wide range of benchmarks and without the need for finetuning. We can attribute the strong performance of the DINOv2 family of models to several factors: i) an improved training recipe with better hyperparameters and regularization (Table <ref type="table" target="#tab_0">1</ref>), ii) a larger model scale with improved results regardless of the data used for training (Fig. <ref type="figure">4</ref>), iii) a larger dataset (Fig. <ref type="figure">4</ref>) and iv) the distillation process that makes smaller models benefit from the performance of the strongest ViT-g model (Fig. <ref type="figure" target="#fig_2">5</ref>). A few properties emerge from these models, such as an understanding of object parts and scene geometry regardless of the image domains. We expect that more of these properties will emerge at larger scales of models and data, akin to instruction emergence in large language models, and plan to continue scaling along these axes. This paper also demonstrates that these visual features are compatible with classifiers as simple as linear layers -meaning the underlying information is readily available. In future work, we plan to leverage this ability to train a a language-enabled AI system that can process visual features as if they were word tokens, and extract the required information to ground the system.  <ref type="table" target="#tab_6">15</ref>: Composition of our LVD-142M dataset. We report the list of datasets and associated splits used to build the dataset, how they were included (as is without retrieval or via sample-based or cluster-based retrieval). For retrievals, we indicate the actual number of retrieved images and the final number included in the dataset. We chose to include as many datasets as possible in the pretraining data in order to cover as many domains as possible. We kept a few datasets aside in order to evaluate performance outside of the pretraining domain. More details about dataset usages can be found in Table <ref type="table" target="#tab_16">18</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Visualization of the first PCA components. We compute a PCA between the patches of the images from the same column (a, b, c and d) and show their first 3 components. Each component is matched to a different color channel. Same parts are matched between related images despite changes of pose, style or even objects. Background is removed by thresholding the first PCA component.</figDesc><graphic coords="2,72.00,81.86,468.00,194.24" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Effectiveness of knowledge distillation. Comparison between a ViT-L trained from scratch or distilled from DINOv2 using ViT-g/14. For reference, we also report the performance of the ViT-g/14 teacher. We show that a ViT-L model distilled from a frozen ViT-g outperforms a the same model trained from scratch on all benchmarks, sometimes even outperforming the distillation target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Role of resolution. Performance of ViT-L/16 trained on ImageNet-1k at fixed resolution("224"  and "416")  or trained at 224 then 416 for a short duration ("224→416"). We train linear classifiers on top of frozen features at different resolutions and report Top-1 accuracy on ImageNet and mIoU on ADE-20k. We observe that performing SSL training at high resolution for a short duration achieve behavior and results close to training at the same high resolution for the full training, at a fraction of the cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Segmentation and depth estimation with linear classifiers. Examples from ADE20K, NYUd, SUN RGB-D and KITTI with a linear probe on frozen OpenCLIP-G and DINOv2-g features.</figDesc><graphic coords="17,72.00,81.86,468.02,247.11" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Examples of out-of-distribution examples with frozen DINOv2-g features and a linear probe.</figDesc><graphic coords="17,72.00,388.85,468.00,179.52" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: More visualization of the first PCA components. We compute the PCA between the patches from all of the images and show their first 3 components. Each component corresponds to a specific color channel. Same parts are matched between related images depsite changes of pose, style or even objects. Background is removed by removing patches with a negative score of the first PCA component.</figDesc><graphic coords="18,72.00,81.86,468.00,253.44" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head/><label/><figDesc/><graphic coords="19,72.00,81.86,468.00,298.08" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation study of the training differences between iBOT and DINOv2. We optimize for k-NN performance, as in our experience, the linear probe performance is lower-bounded by the k-NN performance. Some modifications, like LayerScale and a high Stochastic Depth (rate=0.4), incur a decrease in linear probe performance, but have the benefits of increasing the stability of training by avoiding NaN loss values during training<ref type="bibr" target="#b113">(Touvron et al., 2022)</ref>. Overall, these modifications allowed for the next set of improvements to be added. Experiments are run using the ViT-Large architecture on ImageNet-22k.</figDesc><table><row><cell/><cell/><cell/><cell/><cell cols="3">INet-1k k-NN INet-1k linear</cell><cell/></row><row><cell>iBOT</cell><cell/><cell/><cell/><cell>72.9</cell><cell>82.3</cell><cell/><cell/></row><row><cell cols="3">+(our reproduction)</cell><cell/><cell>74.5 ↑ 1.6</cell><cell>83.2 ↑ 0.9</cell><cell/><cell/></row><row><cell cols="5">+LayerScale, Stochastic Depth 75.4 ↑ 0.9</cell><cell>82.0 ↓ 1.2</cell><cell/><cell/></row><row><cell cols="3">+128k prototypes</cell><cell/><cell>76.6 ↑ 1.2</cell><cell>81.9 ↓ 0.1</cell><cell/><cell/></row><row><cell cols="2">+KoLeo</cell><cell/><cell/><cell>78.9 ↑ 2.3</cell><cell>82.5 ↑ 0.6</cell><cell/><cell/></row><row><cell cols="2">+SwiGLU FFN</cell><cell/><cell/><cell>78.7 ↓ 0.2</cell><cell>83.1 ↑ 0.6</cell><cell/><cell/></row><row><cell cols="2">+Patch size 14</cell><cell/><cell/><cell>78.9 ↑ 0.2</cell><cell>83.5 ↑ 0.4</cell><cell/><cell/></row><row><cell cols="4">+Teacher momentum 0.994</cell><cell>79.4 ↑ 0.5</cell><cell>83.6 ↑ 0.1</cell><cell/><cell/></row><row><cell cols="4">+Tweak warmup schedules</cell><cell>80.5 ↑ 1.1</cell><cell>83.8 ↑ 0.2</cell><cell/><cell/></row><row><cell cols="2">+Batch size 3k</cell><cell/><cell/><cell>81.7 ↑ 1.2</cell><cell>84.7 ↑ 0.9</cell><cell/><cell/></row><row><cell cols="3">+Sinkhorn-Knopp</cell><cell/><cell>81.7 =</cell><cell>84.7 =</cell><cell/><cell/></row><row><cell cols="4">+Untying heads = DINOv2</cell><cell>82.0 ↑ 0.3</cell><cell>84.5 ↓ 0.2</cell><cell/><cell/></row><row><cell>Training Data</cell><cell cols="7">INet-1k Im-A ADE-20k Oxford-M iNat2018 iNat2021 Places205</cell></row><row><cell>INet-22k</cell><cell>85.9</cell><cell>73.5</cell><cell>46.6</cell><cell>62.5</cell><cell>81.1</cell><cell>85.6</cell><cell>67.0</cell></row><row><cell>INet-22k \ INet-1k</cell><cell>85.3</cell><cell>70.3</cell><cell>46.2</cell><cell>58.7</cell><cell>80.1</cell><cell>85.1</cell><cell>66.5</cell></row><row><cell>Uncurated data</cell><cell>83.3</cell><cell>59.4</cell><cell>48.5</cell><cell>54.3</cell><cell>68.0</cell><cell>76.4</cell><cell>67.2</cell></row><row><cell>LVD-142M</cell><cell>85.8</cell><cell>73.9</cell><cell>47.7</cell><cell>64.6</cell><cell>82.3</cell><cell>86.4</cell><cell>67.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation</figDesc><table/><note><p>of the source of pretraining data. We compare the INet-22k dataset that was used in iBOT to our dataset, LVD-142M. Each model is trained for the same number of iterations, that is smaller than in our final run, without high-resolution adaptation. Pretraining on LVD-142M maintains the performance over INet-1k while leading to models that perform better in other domains. and a linear probe in Table</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc/><table/><note><p>(a) Effect of the KoLeo loss term. (b) Effect of the iBOT Masked Image Modeling (MIM) loss term. Evaluation performed on ImageNet-{1k,A} (classification with linear probe, accuracy %), ADE-20k (segmentation with linear layer, mIoU) and Oxford-M (image retrieval, mAP). Each model is trained on the same number of iterations, that is smaller than our final run. The KoLeo loss term improves nearest-neighbor search tasks (e.g. retrieval), and the MIM loss improves patch-level tasks (e.g. segmentation).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head/><label/><figDesc>Published in Transactions on Machine LearningResearch (01/2024)    </figDesc><table><row><cell>INet-1k F o o d ViT-L/14 Scratch ViT-L/14 Distill ViT-g/14 Scratch 86.3 86.5 C a rs 94.3 94.7 91.4 84.5 92.8 81.8 iNat18 90.1 77.8 80.4 81.6 83.1 85.1 85.7 iN a t2 1 67.3 66.0 47.7 IN e t-A 52.6 52.1 P la c e s 2 0 5 67.5 Oxford-H P a ri s -H INet-R K it ti 77.6 84.4 82.7 61.7 71.3 68.1 74.1 2.57 2.5 0.345 0.333 0.298 N Y U d 75.9 78.8 2.35</cell><cell>Arch ViT-g/14 Scratch Method ViT-L/14 Scratch ViT-L/14 Distill Arch Method ViT-g/14 Scratch ViT-L/14 Scratch ViT-L/14 Distill</cell><cell>INet-1k 86.5 84.5 86.3 Finegr. Retriev. ARSketch Segm. Depth↓ 73.4 1.00 72.2 1.10 73.3 1.08 78.3 75.2 77.0 75.8 71.3 69.5 77.6 76.3 74.5</cell><cell>Classif. 92.1 90.2 91.2 Video 69.3 67.3 67.5</cell></row><row><cell>(a) Comparison on individual metrics</cell><cell cols="2">(b) Averaged metrics on 8 vision tasks</cell><cell/></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Supervised finetuning on ImageNet-1k. We use the pipeline of<ref type="bibr" target="#b113">Touvron et al. (2022)</ref> to finetune our encoders on ImageNet-1k at resolutions 224 × 224 or 448 × 448. We compare with the accuracy obtained with linear probing and observe only modest improvements with fine-tuning: this suggests that DINOv2 features already perform well out-of-the-box.</figDesc><table><row><cell>.</cell><cell cols="3">Res. Linear Finetuned</cell><cell>∆</cell></row><row><cell>ViT-g/14</cell><cell>224 448</cell><cell>86.5 86.7</cell><cell>88.5 88.9</cell><cell>+2.0 +2.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Domain Generalization with a linear probe on top of frozen features at a resolution of 224. Higher numbers are better for all benchmarks except Im-C.</figDesc><table><row><cell/><cell>Method</cell><cell>Arch</cell><cell/><cell>Data</cell><cell/><cell cols="3">Im-A Im-R Im-C↓ Sketch</cell></row><row><cell/><cell cols="5">OpenCLIP ViT-G/14 LAION-2B</cell><cell>63.8</cell><cell>87.8</cell><cell>45.3</cell><cell>66.4</cell></row><row><cell/><cell>MAE</cell><cell cols="3">ViT-H/14 INet-1k</cell><cell/><cell>10.2</cell><cell>34.4</cell><cell>61.4</cell><cell>21.9</cell></row><row><cell/><cell>DINO</cell><cell cols="2">ViT-B/8</cell><cell>INet-1k</cell><cell/><cell>23.9</cell><cell>37.0</cell><cell>56.6</cell><cell>25.5</cell></row><row><cell/><cell>iBOT</cell><cell cols="4">ViT-L/16 INet-22k</cell><cell>41.5</cell><cell>51.0</cell><cell>43.9</cell><cell>38.5</cell></row><row><cell/><cell/><cell cols="4">ViT-S/14 LVD-142M</cell><cell>33.5</cell><cell>53.7</cell><cell>54.4</cell><cell>41.2</cell></row><row><cell/><cell>DINOv2</cell><cell cols="4">ViT-B/14 LVD-142M ViT-L/14 LVD-142M</cell><cell>55.1 71.3</cell><cell>63.3 74.4</cell><cell>42.7 31.5</cell><cell>50.6 59.3</cell></row><row><cell/><cell/><cell cols="4">ViT-g/14 LVD-142M</cell><cell>75.9</cell><cell>78.8</cell><cell>28.2</cell><cell>62.5</cell></row><row><cell/><cell/><cell/><cell/><cell cols="3">Image classification</cell><cell/><cell>Video classification</cell></row><row><cell>Feature</cell><cell>Arch</cell><cell/><cell cols="5">iNat2018 iNat2021 Places205</cell><cell>K400 UCF-101 SSv2</cell></row><row><cell cols="3">OpenCLIP ViT-G/14</cell><cell/><cell>73.0</cell><cell>76.0</cell><cell>69.8</cell><cell/><cell>78.3</cell><cell>90.7</cell><cell>35.8</cell></row><row><cell>MAE</cell><cell cols="2">ViT-H/14</cell><cell/><cell>31.0</cell><cell>32.3</cell><cell>52.4</cell><cell/><cell>54.2</cell><cell>70.6</cell><cell>29.2</cell></row><row><cell>DINO</cell><cell>ViT-B/8</cell><cell/><cell/><cell>59.6</cell><cell>68.3</cell><cell>60.4</cell><cell/><cell>64.5</cell><cell>85.0</cell><cell>32.6</cell></row><row><cell>iBOT</cell><cell cols="2">ViT-L/16</cell><cell/><cell>66.3</cell><cell>74.6</cell><cell>64.4</cell><cell/><cell>72.6</cell><cell>88.6</cell><cell>38.7</cell></row><row><cell/><cell cols="2">ViT-S/14</cell><cell/><cell>69.0</cell><cell>74.2</cell><cell>62.9</cell><cell/><cell>67.8</cell><cell>87.0</cell><cell>33.1</cell></row><row><cell>DINOv2</cell><cell cols="2">ViT-B/14 ViT-L/14</cell><cell/><cell>76.4 80.4</cell><cell>81.1 85.1</cell><cell>66.2 67.3</cell><cell/><cell>73.2 76.3</cell><cell>89.1 90.5</cell><cell>34.4 35.6</cell></row><row><cell/><cell cols="2">ViT-g/14</cell><cell cols="2">81.6</cell><cell>85.7</cell><cell>67.5</cell><cell/><cell>78.4</cell><cell>91.2</cell><cell>38.3</cell></row></table><note><p>compared to iBOT). Our model also improves upon the best weakly-supervised model on ImageNet-A while lagging behind on R and Sketch.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 : Linear evaluation on other image and video classification</head><label>7</label><figDesc/><table/><note><p>. The image benchmarks contain a large quantity of fine-grained examples about objects or scenes. The video benchmarks cover action classification and human-object interaction. All the features are frozen with a linear probe on top.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 : Semantic segmentation on ADE20K, CityScapes and Pascal VOC with frozen features</head><label>10</label><figDesc>and a linear classifier (lin.) and with multiscale (+ms). The absolute state of the art -from Wang et al. (2022), Liu et al. (2021) and Chen et al. (</figDesc><table><row><cell cols="5">Published in Transactions on Machine Learning Research (01/2024)</cell><cell/><cell/><cell/></row><row><cell/><cell/><cell cols="2">ADE20k</cell><cell cols="2">CityScapes</cell><cell cols="2">Pascal VOC</cell></row><row><cell/><cell/><cell cols="2">(62.9)</cell><cell cols="2">(86.9)</cell><cell cols="2">(89.0)</cell></row><row><cell>Method</cell><cell>Arch.</cell><cell>lin.</cell><cell>+ms</cell><cell>lin.</cell><cell>+ms</cell><cell>lin.</cell><cell>+ms</cell></row><row><cell cols="2">OpenCLIP ViT-G/14</cell><cell cols="2">39.3 46.0</cell><cell cols="2">60.3 70.3</cell><cell>71.4</cell><cell>79.2</cell></row><row><cell>MAE</cell><cell>ViT-H/14</cell><cell cols="2">33.3 30.7</cell><cell cols="2">58.4 61.0</cell><cell>67.6</cell><cell>63.3</cell></row><row><cell>DINO</cell><cell>ViT-B/8</cell><cell cols="2">31.8 35.2</cell><cell cols="2">56.9 66.2</cell><cell>66.4</cell><cell>75.6</cell></row><row><cell>iBOT</cell><cell>ViT-L/16</cell><cell cols="2">44.6 47.5</cell><cell cols="2">64.8 74.5</cell><cell>82.3</cell><cell>84.3</cell></row><row><cell/><cell>ViT-S/14</cell><cell cols="2">44.3 47.2</cell><cell cols="2">66.6 77.1</cell><cell>81.1</cell><cell>82.6</cell></row><row><cell>DINOv2</cell><cell>ViT-B/14 ViT-L/14</cell><cell cols="2">47.3 51.3 47.7 53.1</cell><cell cols="2">69.4 80.0 70.3 80.9</cell><cell>82.5 82.1</cell><cell>84.9 86.0</cell></row><row><cell/><cell>ViT-g/14</cell><cell cols="2">49.0 53.0</cell><cell cols="2">71.3 81.0</cell><cell>83.0</cell><cell>86.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Depth estimation with frozen features. We report performance when training a linear classifier on top of one (lin. 1) or four (lin. 4) transformer layers, as well, as the DPT decoder (DPT) of<ref type="bibr" target="#b94">Ranftl et al. (2021)</ref>. We report the RMSE metric on the 3 datasets. Lower is better. For reference, we report state-of-the-art results taken fromLi et al. (2022b)  on each benchmark on top of the Table.use a linear normalization following Bhat et al.</figDesc><table><row><cell/><cell/><cell/><cell>NYUd</cell><cell/><cell/><cell>KITTI</cell><cell/><cell cols="3">NYUd → SUN RGB-D</cell></row><row><cell/><cell/><cell/><cell>(0.330)</cell><cell/><cell/><cell>(2.10)</cell><cell/><cell/><cell>(0.421)</cell><cell/></row><row><cell>Method</cell><cell>Arch.</cell><cell cols="2">lin. 1 lin. 4</cell><cell>DPT</cell><cell cols="3">lin. 1 lin. 4 DPT</cell><cell cols="2">lin. 1 lin. 4</cell><cell>DPT</cell></row><row><cell cols="3">OpenCLIP ViT-G/14 0.541</cell><cell>0.510</cell><cell>0.414</cell><cell>3.57</cell><cell>3.21</cell><cell>2.56</cell><cell>0.537</cell><cell>0.476</cell><cell>0.408</cell></row><row><cell>MAE</cell><cell cols="2">ViT-H/14 0.517</cell><cell>0.483</cell><cell>0.415</cell><cell>3.66</cell><cell>3.26</cell><cell>2.59</cell><cell>0.545</cell><cell>0.523</cell><cell>0.506</cell></row><row><cell>DINO</cell><cell>ViT-B/8</cell><cell>0.555</cell><cell>0.539</cell><cell>0.492</cell><cell>3.81</cell><cell>3.56</cell><cell>2.74</cell><cell>0.553</cell><cell>0.541</cell><cell>0.520</cell></row><row><cell>iBOT</cell><cell>ViT-L/16</cell><cell>0.417</cell><cell>0.387</cell><cell>0.358</cell><cell>3.31</cell><cell>3.07</cell><cell>2.55</cell><cell>0.447</cell><cell>0.435</cell><cell>0.426</cell></row><row><cell/><cell>ViT-S/14</cell><cell>0.449</cell><cell>0.417</cell><cell>0.356</cell><cell>3.10</cell><cell>2.86</cell><cell>2.34</cell><cell>0.477</cell><cell>0.431</cell><cell>0.409</cell></row><row><cell>DINOv2</cell><cell cols="2">ViT-B/14 0.399 ViT-L/14 0.384</cell><cell>0.362 0.333</cell><cell>0.317 0.293</cell><cell>2.90 2.78</cell><cell>2.59 2.50</cell><cell>2.23 2.14</cell><cell>0.448 0.429</cell><cell>0.400 0.396</cell><cell>0.377 0.360</cell></row><row><cell/><cell cols="4">ViT-g/14 0.344 0.298 0.279</cell><cell cols="3">2.62 2.35 2.11</cell><cell cols="3">0.402 0.362 0.338</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 : Geographical fairness and diversity analysis across income buckets and regions.</head><label>12</label><figDesc/><table><row><cell/><cell>Data</cell><cell cols="3">low medium high</cell><cell cols="4">Africa Asia Americas Europe</cell></row><row><cell>SEERv2 RG-10B</cell><cell>IG-1B</cell><cell>59.7</cell><cell>78.5</cell><cell>86.6</cell><cell>65.9</cell><cell>76.3</cell><cell>81.1</cell><cell>85.6</cell></row><row><cell cols="2">DINOv2 ViT-g/14 LVD-142M</cell><cell>67.4</cell><cell>83.3</cell><cell>90.5</cell><cell>74.0</cell><cell>81.6</cell><cell>86.2</cell><cell>89.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 : Label association fairness evaluation across gender, skintones and age groups.</head><label>13</label><figDesc>We follow the protocol proposed byGoyal et al. (2022b)  with a slight modification. Instead of finetuning the backbone, we simply learn a linear classifier on the subset of 619 classes of ImageNet-22k.</figDesc><table><row><cell/><cell/><cell/><cell/><cell cols="2">Gender Skintone</cell><cell/><cell/><cell cols="2">Age Groups</cell></row><row><cell>Model</cell><cell cols="2">Assoc.</cell><cell>female darker</cell><cell>female lighter</cell><cell>male darker</cell><cell>male lighter</cell><cell cols="4">18-30 30-45 45-70 70+</cell></row><row><cell>SEER</cell><cell cols="2">Non-Human</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>RG-10B</cell><cell cols="2">Crime</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell/><cell cols="2">Human</cell><cell>94.9</cell><cell>95.8</cell><cell>86.6</cell><cell>79.0</cell><cell>90.5</cell><cell>88.3</cell><cell cols="2">91.9 82.3</cell></row><row><cell/><cell cols="2">Possibly-Human</cell><cell>13.6</cell><cell>6.7</cell><cell>65.0</cell><cell>60.2</cell><cell>32.8</cell><cell>37.2</cell><cell>29.4</cell><cell>6.5</cell></row><row><cell>DINOv2</cell><cell cols="2">Non-Human</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell cols="3">ViT-g/14 Crime</cell><cell>0.0</cell><cell>0.0</cell><cell>0.2</cell><cell>0.0</cell><cell>0.0</cell><cell>0.1</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell/><cell cols="2">Human</cell><cell>97.3</cell><cell>97.7</cell><cell>86.1</cell><cell>84.0</cell><cell>91.2</cell><cell>90.2</cell><cell cols="2">93.2 88.7</cell></row><row><cell/><cell cols="2">Possibly-Human</cell><cell>15.8</cell><cell>17.2</cell><cell>52.2</cell><cell>48.1</cell><cell>35.3</cell><cell>37.3</cell><cell>23.0</cell><cell>9.7</cell></row><row><cell cols="2">Model to Reproduce</cell><cell>GPU Type</cell><cell cols="4">GPU Power GPU-hours PUE consumption</cell><cell cols="4">Total power Carbon emitted consumption (tCO 2 eq)</cell></row><row><cell cols="3">DINOv2-g A100-40GB</cell><cell>400W</cell><cell/><cell>22,016</cell><cell>1.1</cell><cell>9.7 MWh</cell><cell/><cell>3.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 .</head><label>14</label><figDesc>For comparison, retraining an OpenCLIP ViT-L or OpenCLIP ViT-G would require 22.4 MWh and 118.9 MWh, respectively, if run in the same data center. This is 10× more carbon emission. Note that this comparison is not fair to them, since they also train a text encoder in parallel, and we thus do not report them in the table. However, it gives a reasonable guideline for those who are interested in training only visual features: in this context, training a self-supervised model is preferable in terms of carbon emission. Training a text-guided model still makes sense when planning to reuse the text encoder.</figDesc><table/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>.</head><label/><figDesc>Published in Transactions on Machine LearningResearch (01/2024)    </figDesc><table><row><cell>Dataset</cell><cell>Pretraining</cell><cell>Retrieving</cell><cell>Eval.</cell><cell>Task</cell><cell>Citation</cell></row><row><cell/><cell>(as is)</cell><cell>pretraining</cell><cell/><cell/><cell/></row><row><cell/><cell/><cell>data</cell><cell/><cell/><cell/></row><row><cell>ImageNet-1k</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Classif.</cell><cell>(Russakovsky et al., 2015)</cell></row><row><cell>ImageNet-22k</cell><cell>✓</cell><cell>✓</cell><cell>✗</cell><cell/><cell>(Deng et al., 2009)</cell></row><row><cell>ImageNet-V2</cell><cell>✗</cell><cell>✗</cell><cell>✓</cell><cell>Classif.</cell><cell>(Recht et al., 2019)</cell></row><row><cell>ImageNet-ReaL</cell><cell>✗</cell><cell>✗</cell><cell>✓</cell><cell>Classif.</cell><cell>(Beyer et al., 2020)</cell></row><row><cell>ImageNet-A</cell><cell>✗</cell><cell>✗</cell><cell>✓</cell><cell>Classif.</cell><cell>(Hendrycks et al., 2021b)</cell></row><row><cell>ImageNet-C</cell><cell>✗</cell><cell>✗</cell><cell>✓</cell><cell>Classif.</cell><cell>(Hendrycks &amp; Dietterich, 2019)</cell></row><row><cell>ImageNet-R</cell><cell>✗</cell><cell>✗</cell><cell>✓</cell><cell>Classif.</cell><cell>(Hendrycks et al., 2021a)</cell></row><row><cell>ImageNet-Sk.</cell><cell>✗</cell><cell>✗</cell><cell>✓</cell><cell>Classif.</cell><cell>(Wang et al., 2019)</cell></row><row><cell>Food-101</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Classif.</cell><cell>(Bossard et al., 2014)</cell></row><row><cell>CIFAR-10</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Classif.</cell><cell>(Krizhevsky et al., 2009)</cell></row><row><cell>CIFAR-100</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Classif.</cell><cell>(Krizhevsky et al., 2009)</cell></row><row><cell>SUN397</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Classif.</cell><cell>(Xiao et al., 2010)</cell></row><row><cell>StanfordCars</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Classif.</cell><cell>(Krause et al., 2013)</cell></row><row><cell>FGVC-Aircraft</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Classif.</cell><cell>(Maji et al., 2013)</cell></row><row><cell>VOC 2007</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Classif.</cell><cell>(Everingham et al., 2010)</cell></row><row><cell>DTD</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Classif.</cell><cell>(Cimpoi et al., 2014)</cell></row><row><cell>Oxford Pets</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Classif.</cell><cell>(Parkhi et al., 2012)</cell></row><row><cell>Caltech101</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Classif.</cell><cell>(Fei-Fei et al., 2004)</cell></row><row><cell>Flowers</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Classif.</cell><cell>(Nilsback &amp; Zisserman, 2008)</cell></row><row><cell>CUB200</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Classif.</cell><cell>(Welinder et al., 2010)</cell></row><row><cell>iNaturalist 2018</cell><cell>✗</cell><cell>✗</cell><cell>✓</cell><cell>Classif.</cell><cell>(Van Horn et al., 2018)</cell></row><row><cell>iNaturalist 2021</cell><cell>✗</cell><cell>✗</cell><cell>✓</cell><cell>Classif.</cell><cell>(Van Horn et al., 2021)</cell></row><row><cell>Places-205</cell><cell>✗</cell><cell>✗</cell><cell>✓</cell><cell>Classif.</cell><cell>(Zhou et al., 2014)</cell></row><row><cell>UCF101</cell><cell>✗</cell><cell>✗</cell><cell>✓</cell><cell>Video</cell><cell>(Soomro et al., 2012)</cell></row><row><cell>Kinetics-400</cell><cell>✗</cell><cell>✗</cell><cell>✓</cell><cell>Video</cell><cell>(Kay et al., 2017)</cell></row><row><cell>SSv2</cell><cell>✗</cell><cell>✗</cell><cell>✓</cell><cell>Video</cell><cell>(Goyal et al., 2017)</cell></row><row><cell>GLD v2</cell><cell>✓</cell><cell>✓</cell><cell>✗</cell><cell/><cell>(Weyand et al., 2020)</cell></row><row><cell>R-Paris</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Retrieval</cell><cell>(Radenović et al., 2018a)</cell></row><row><cell>R-Oxford</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Retrieval</cell><cell>(Radenović et al., 2018a)</cell></row><row><cell>Met</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Retrieval</cell><cell>(Ypsilantis et al., 2021)</cell></row><row><cell>Amstertime</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Retrieval</cell><cell>(Yildiz et al., 2022)</cell></row><row><cell>ADE20k</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Seg.</cell><cell>(Zhou et al., 2017)</cell></row><row><cell>Cityscapes</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Seg.</cell><cell>(Cordts et al., 2016)</cell></row><row><cell>VOC 2012</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Seg.</cell><cell>(Everingham et al., 2010)</cell></row><row><cell>Mapillary SLS</cell><cell>✓</cell><cell>✗</cell><cell>✗</cell><cell/><cell>(Warburg et al., 2020)</cell></row><row><cell>NYU-Depth V2</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Depth</cell><cell>(Silberman et al., 2012)</cell></row><row><cell>KITTI</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Depth</cell><cell>(Geiger et al., 2013)</cell></row><row><cell>SUN-RGBD</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Depth</cell><cell>(Song et al., 2015)</cell></row><row><cell>DollarStreet</cell><cell>✗</cell><cell>✗</cell><cell>✓</cell><cell>Fairness</cell><cell>(De Vries et al., 2019)</cell></row><row><cell>Casual Conv.</cell><cell>✗</cell><cell>✗</cell><cell>✓</cell><cell>Fairness</cell><cell>(Hazirbas et al., 2021)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 18 :</head><label>18</label><figDesc>List of datasets used.</figDesc><table/></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/facebookresearch/dinov2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/facebookresearch/xformers</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>For context, a full Boeing 777 return flight between London and New York corresponds to approximately 560 tCO 2 eq.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments.</head><p>We thank <rs type="person">Mathilde Caron</rs> for initial discussions that led to this work. <rs type="person">Julien Mairal</rs> was supported by the <rs type="funder">ERC</rs> grant number <rs type="grantNumber">101087696</rs> (<rs type="projectName">APHELAIA</rs> project) and by <rs type="funder">ANR</rs> <rs type="grantNumber">3IA</rs> <rs type="projectName">MIAI@Grenoble Alpes</rs> (<rs type="grantNumber">ANR-19-P3IA-0003</rs>). We thank <rs type="person">Olivia Joulin</rs> for the horse drawing used in Fig. 10. We thank <rs type="person">Madeleine and Léon</rs> for posing for Fig. 8 We also thank the rest of <rs type="funder">FAIR</rs> and <rs type="institution" subtype="infrastructure">Meta AI</rs> for feedback on this work through the entire project.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_T8udwyv">
					<idno type="grant-number">101087696</idno>
					<orgName type="project" subtype="full">APHELAIA</orgName>
				</org>
				<org type="funded-project" xml:id="_qJEYEnd">
					<idno type="grant-number">3IA</idno>
					<orgName type="project" subtype="full">MIAI@Grenoble Alpes</orgName>
				</org>
				<org type="funding" xml:id="_VnP2MZT">
					<idno type="grant-number">ANR-19-P3IA-0003</idno>
				</org>
			</listOrg>

			<listOrg type="infrastructure">
				<org type="infrastructure">					<orgName type="extracted">Meta AI</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">9</ref><p>: Evaluation of frozen features on instance-level recognition. We consider 4 different benchmarks and report their main metrics.</p><p>We replace the Birdsnap dataset with CUB because the former was not publicly available in its entirety. We follow the experimental protocol as outlined by <ref type="bibr" target="#b20">Chen et al. (2020)</ref>, namely training a logistic regression on precomputed features. Our model significantly outperforms state-of-the-art SSL models, with most notable differences on Stanford Cars (+14.8% versus DINO ViT-B/8) and FGVC Aircraft (+14.8% versus iBOT ViT-L/16). Even though these benchmarks favor text-guided pretraining, our features are still competitive with OpenCLIP on most classification benchmarks, with the exception of a few datasets, especially SUN (-5.3%) and Cars (-4.7%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Instance Recognition</head><p>In this experiment, we probe our model on the task of instance-level recognition using a non-parametric approach. Images from a database are ranked according to their cosine similarity with a query image. We evaluated our model and compare to baselines on Paris and Oxford, that are landmark recognition benchmarks. We also evaluated on Met, a dataset of artworks from the Metropolitan museum, and AmsterTime, containing street view images matched to archival images of Amsterdam. We measure performance by computing the mean average precision and report our results in Table <ref type="table">9</ref>. We see that our features significantly outperform both SSL (+41% mAP on Oxford-Hard), and weakly-supervised (+34% mAP on Oxford-Hard) ones. It is interesting to see that our features perform well across task granularities, both at the category-level and instance-level. This is a desirable property for strong off-the-shelf computer vision features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Data Processing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Data selection</head><p>Our selection of datasets for building LVD-142M is detailed in Tab. 15. This collection is intended to provide images covering well various downstream vision tasks both for image-level and dense recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Image similarity</head><p>We employ cosine similarity to compare image features (whether ours or feature generated for deduplication) with the following similarity function m:</p><p>where s and r are a pair of images to compare and f is the model generating features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Deduplication</head><p>Self-deduplication. To deduplicate our uncurated data source of 1.3B images, we compute and use the embeddings generated by <ref type="bibr" target="#b87">Pizzi et al. (2022)</ref> and retrieve the k = 64 nearest neighbors of each image (using cosine similarity). Considering only neighbors with a similarity &gt;0.6, we extract the connected components of the associated k-NN graph thanks to a scalable disjoint set data structure implementation. We then only keep one representative for each component of duplicate images. This results in a self-deduplicated data source of 1.1B images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relative deduplication</head><p>To reduce redundancy and also properly evaluate the performance of our features, we discard remaining images of our self-deduplicated data source that are too similar to train and test splits of our evaluation datasets. To achieve this, we apply a similar procedure as for self-deduplication, with a stricter similarity &gt;0.45, this time identifying the duplicate components (if any) to which each reference image belong and discarding it entirely. This results in a self-and relatively-deduplicated data source of 744M images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Retrieval</head><p>We employ two approaches to augment dataset via retrieval: sample-based and cluster-based. The first one, sample-based, applies to datasets larger than 1M images and consists in collecting a fixed number k of nearest images for each sample image of the dataset to retrieve, effectively trying to multiply by k the size of the dataset. We use k = 4 for Google Landmarks v2 and ImageNet-22k but a larger k = 32 to make this specific retrieval a core part of our LVD-142M dataset. For smaller datasets, the second approach, cluster-based, consists in first clustering our uncurated data source into 100, 000 separate clusters thanks to a distributed k-means implementation. Each cluster should capture different types of image concept and contents. We then pick 10, 000 images from each cluster associated with more than 3 images of the retrieved dataset. As this can result in a very large number of retrieved images for some dataset, we restrict such retrievals to a maximum of 1M images to maintain the balance between the different datasets within LVD-142M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Unsupervised pre-training</head><p>For unsupervised pre-training we build on the DINO and iBOT codebases. We use hyperparameters shown in Table <ref type="table">16</ref>, ViT architectures described in Table <ref type="table">17</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KoLeo regularization.</head><p>We apply the KoLeo regularizer with a weight of 0.1 between the class tokens of the first global crop, for all samples within a GPU without cross-communication for this step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Arch.</head><p>Drop-rate LR Batch size DINOv2-S (distilled) ViT-S/14 0 1e-3 2048 DINOv2-B (distilled)</p><p>ViT-B/14 0 1e-3 2048 DINOv2-L (distilled)</p><p>ViT-L/14 0 1e-3 2048 DINOv2-L (from scratch) ViT-L/14 0.4 3.5e-4 3072 DINOv2-g (from scratch)</p><p>ViT-g/14 0.4 3.5e-4 3072</p><p>Table <ref type="table">16</ref>: Training hyperparameters for DINOv2-S, DINOv2-B, DINOv2-L and DINOv2-g. All models run for 625k iterations with optimizer AdamW, an initial LayerScale value of 1e-5, a weight decay cosine schedule from 0.04 to 0.2, a learning rate warmup of 100k iterations, a teacher momentum cosine schedule from 0.994 to 1, and we train in float16 precision in all cases (except for the DINO heads where we reduce the gradients in float32).   <ref type="bibr" target="#b102">(Shazeer, 2020)</ref> when training from scratch.</p><p>EMA update for the teacher. The teacher is initialized with the same state as the student, and is an exponential moving average of the student network, with a momentum value in [0.994, 1.0] following a cosine schedule. It is updated at the end of every training step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 High-Resolution adaptation</head><p>We initialise the model with the pretrained weights then train it for 10k iterations with the same procedure as the original pretraining. All the schedules are kept the same as in the original training, but compressed to fit in 10k iterations. All the hyperparameters are kept the same as in the first pretraining, except the base learning rate which is reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Linear probing evaluation</head><p>For linear probing we define 3 evaluation parameters: the learning rate, how many output layers we use, whether we concatenate the average-pooled patch token features with the class token (or use only the class token). We train our linear layer with SGD for 12500 iterations, using random-resized-crop data augmentation, and perform the following grid search:</p><p>• learning rate in {0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.5}</p><p>• output layers in {1, 4}</p><p>• concatenate average-pooled tokens in {yes, no}</p><p>We then report the highest accuracy value obtained on the validation set as is common practice. Note that this grid search is not expensive, because at each iteration we perform inference on the backbone only once, then feed the output to all linear classifiers (each performing a single matrix multiplication).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C List of Datasets used</head><p>We show in Table <ref type="table">18</ref> the list of benchmarks and datasets used and their purposes.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the Effectiveness of ViT Features as Local Semantic Descriptors</title>
		<author>
			<persName><forename type="first">Shir</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Gandelsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-25069-9_3</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="39" to="55"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName><forename type="first">Yuki</forename><surname>Markus Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Masked Siamese Networks for Label-Efficient Learning</title>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Assran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19821-2_26</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="456" to="473"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture</title>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Assran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Duval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52729.2023.01499</idno>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023-06">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Data2vec: A general framework for self-supervised learning in speech, vision and language</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beit: Bert pre-training of image transformers</title>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Complete Statistical Ranking of Populations, with Tables and Applications</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Beirlant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Dudewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">C</forename><surname>Van Der Meulen</surname></persName>
		</author>
		<idno type="DOI">10.21236/ada114400</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Mathematical and Statistical Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="39"/>
			<date type="published" when="1997">1997</date>
			<publisher>Defense Technical Information Center</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">MultiGrain: a unified image embedding for classes and instances</title>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05509</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><forename type="middle">J</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
		<title level="m">Are we done with imagenet?</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FlexiViT: One Model for All Patch Sizes</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Alabdulmohsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Pavetic</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52729.2023.01393</idno>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023-06">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">AdaBins: Depth estimation using adaptive bins</title>
		<author>
			<persName><forename type="first">Farooq</forename><surname>Shariq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibraheem</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised learning by predicting noise</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">(2021) Volume 2, Issue 4 Cultural Implications of China Pakistan Economic Corridor (CPEC Authors: Dr. Unsa Jamshed Amar Jahangir Anbrin Khawaja Abstract: This study is an attempt to highlight the cultural implication of CPEC on Pak-China relations, how it will align two nations culturally, and what steps were taken by the governments of two states to bring the people closer. After the establishment of diplomatic relations between Pakistan and China, the cultural aspect of relations between the two states also moved forward. The flow of cultural delegations intensified after the 2010, because this year was celebrated as the ‘Pak-China Friendship Year’. This dimension of relations further cemented between the two states with the signing of CPEC in April 2015. CPEC will not only bring economic prosperity in Pakistan but it will also bring two states culturally closer. The roads and other communication link under this project will become source of cultural flow between the two states. Keyswords: China, CPEC, Culture, Exhibitions Pages: 01-11 Article: 1 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)01 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)01 Download Pdf: download pdf view article Creative Commons License Political Persona on Twittersphere: Comparing the Stardom of Prime Minister(s) of Pakistan, UK and India Authors: Maryam Waqas Mudassar Hussain Shah Saima Kausar Abstract: Political setup demands to use Twittersphere for preserving its reputation because of significant twitter audience, which follows celebrities and political figures. In this perspective, political figures frequently use twitter to highlight their political as well as personal lives worldwide. However, political figures take the stardom status among the twitter audience that follow, retweet and comment by their fans. The purpose of this study is, to analyze what kind of language, level of interest is made by political figures while communicating via twitter, text, phrases and languages used by political figures, and do their tweets contribute in their reputation. The qualitative content analysis is used for evaluation of the interests shared by PM Imran Khan, PM Boris John Son and PM Narendra Modi with the key words of tweets. A well-established coding sheet is developed for the analysis of text, phrases and words in the frames of negative, positive and neutral from March 2020 to May 2020. The results are demonstrating on the basis of content shared by Prime Ministers of three countries i.e., From Pakistan, Imran Khan, United Kingdom, Johnson Boris and India, Narendra Modi on twitter. The findings also reveal that varied issues discussed in tweets, significantly positive and neutral words are selected by these political figures. PM Imran tweeted more negative tweets than PM Boris Johnson and PM Narendra Modi. However, PM Boris Johnson and PM Narendra Modi make significant positive and neutral tweets. It is observed that political figures are conscious about their personal reputation while tweeting. It also revealed that the issues and tweets shared by these leaders contribute to their personal reputation. Keyswords: Imran Khan, Johnson Boris, Narendra Modi, Political Persona, Stardom, Twittersphere Pages: 12-23 Article: 2 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)02 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)02 Download Pdf: download pdf view article Creative Commons License An Empirical Relationship between Government Size and Economic Growth of Pakistan in the Presence of Different Budget Uncertainty Measures Authors: Sunila Jabeen Dr. Wasim Shahid Malik Abstract: Relationship between government size and economic growth has always been a debated issue all over the world since the formative work of Barro (1990). However, this relationship becomes more questionable when policy uncertainty is added in it. Hence, this paper presents evidence on the effect of government size on economic growth in the presence of budget uncertainty measured through three different approaches. Rather than relying on the traditional and complicated measures of uncertainty, a new method of measuring uncertainty based on government budget revisions of total spending is introduced and compared with the other competing approaches. Using time series annual data from 1973-2018, the short run and long run coefficients from Autoregressive Distributed Lag (ARDL) framework validate the negative effect of budget uncertainty and government size on economic growth of Pakistan regardless of the uncertainty measure used. Therefore, to attain the long run economic growth, along with the control on the share of government spending in total GDP, government should keep the revisions in the budget as close to the initial announcements as it can so that uncertainty can be reduced. Further, the uncertainty in fiscal spending calculated through the deviation method raises a big question on the credibility of fiscal policy in Pakistan. Higher will be the deviation higher will be the uncertainty and lower the fiscal policy credibility hence making fiscal policy less effective in the long run. Keyswords: Budget Uncertainty, Economic Growth, Government Size, Policy Credibility Pages: 24-38 Article: 3 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)03 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)03 Download Pdf: download pdf view article Creative Commons License Despair in The Alchemist by Ben Jonson Authors: Dr. Fatima Syeda Dr. Faiza Zaheer Numrah Mehmood Abstract: This research aims to challenge the assumption that The Alchemist by Ben Jonson is one of the greatest examples of the “explicit mirth and laughter” (Veneables 86). The paper argues that The Alchemist is a cynical and despairing play created in an atmosphere not suitable for a comedy. This is a qualitative study of the text and aims at an analysis of the theme, situations, characters, language, and the mood of the play to determine that Jonson is unable to retain the comic spirit in The Alchemist and in an attempt to “better men” (Prologue. 12) he becomes more satirical and less humorous or comic. This research is important for it contends that the play, termed as a comedy, may be read as a bitter satire on the cynical, stinky, and despairing world of the Elizabethan times. Keyswords: Comedy, Despair, Reformation Pages: 39-47 Article: 4 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)04 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)04 Download Pdf: download pdf view article Creative Commons License Analysis of Principles of Coordinated Border Management (CBM) in articulation of War-Control Strategies: An Account of Implementation Range on Pakistan and Afghanistan Authors: Dr. Sehrish Qayyum Dr. Umbreen Javaid Abstract: Currently, Border Management is crucial issue not only for Pakistan but for the entire world due to increased technological developments and security circumstances. Pakistan and Afghanistan being immediate states have inter-connected future with socio-economic and security prospects. Principles of Coordinated Border Management (CBM) approach have been extracted on the basis of in-depth interviews with security agencies and policymakers to understand the real time needs. The current research employs mixed method approach. Process Tracing is employed in this research to comprehend the causal mechanism behind the contemporary issue of border management system. A detailed statistical analysis of prospect outcomes has been given to validate the implication of CBM. Implication range of CBM has been discussed with positive and probably negative impacts due to its wide range of significance. This research gives an analysis of feasibility support to exercise CBM in best interest of the state and secure future of the region. Keyswords: Afghanistan, Coordinated Border Management, Fencing, Pakistan, Security Pages: 48-62 Article: 5 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)05 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)05 Download Pdf: download pdf view article Creative Commons License The Belt and Road Initiative (BRI) vs. Quadrilateral Security Dialogue (the Quad): A Perspective of a Game Theory Authors: Muhammad Atif Prof. Dr. Muqarrab Akbar Abstract: Containment is the central part of the U.S.'s foreign policy during the cold war. With the application of containment Policy, the U.S. achieved much success in international politics. Over time China has become more powerful and sees great power in international politics. China wants to expand and launched the Belt and Road Initiative (BRI). The primary purpose of The Belt and Road Initiative (BRI) is to achieve support from regional countries and save their interests from the U.S. In 2017, the American administration launched its Containment policy through Quadrilateral Security Dialogue (the Quad) to keep their interest from China. The Quadrilateral Security Dialogue (Quad) is comprising of Australia, the United States, Japan, and India. This Study is based on Qualitative research with theoretical application of Game theory. This research investigates both plans of China (BRI) and the U.S. (the Quad) through a Game Theory. In this study, China and the U.S. both like to act as gamers in international politics. This study recommends that Game theory can predict all developments in the long term. Keyswords: Containment, Expansionism, Quadrilateral Security Dialogue, The Belt and Road Initiative (BRI) Pages: 63-75 Article: 6 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)06 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)06 Download Pdf: download pdf view article Creative Commons License Narendra Modi a Machiavellian Prince: An Appraisal Authors: Dr. Imran Khan Dr. Karim Haider Syed Muhammad Yousaf Abstract: The comparison of Narendra Modi and Machiavellian Prince is very important as policies of Modi are creating problems within India and beyond the borders. The Prince is the book of Niccolo Machiavelli a great philosopher of his time. If Indian Prime Minister Narendra Modi qualifies as a Prince of Machiavelli is a very important question. This is answered in the light of his policies and strategies to become the undisputed political leader of India. Much of the Machiavellian Prince deals with the problem of how a layman can raise himself from abject and obscure origins to such a position that Narendra Modi has been holding in India since 2014. The basic theme of this article is revolving around the question that is following: Can Modi’s success be attributed to techniques of The Prince in important respects? This article analyzed Narendra Modi's policies and strategies to develop an analogy between Machiavellian Prince and Modi in terms of characteristics and political strategies. This research work examines, how Narendra Modi became the strongest person in India. Keyswords: Comparison, India, Machiavelli, Modus Operandi, Narendra Modi Pages: 76-84 Article: 7 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)07 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)07 Download Pdf: download pdf view article Creative Commons License Analyzing Beckett's Waiting for Godot as a Political Comedy Authors: Muhammad Umer Azim Dr. Muhammad Saleem Nargis Saleem Abstract: This study was devised to analyze Samuel Beckett’s play Waiting for Godot in the light of Jean-Francois Lyotard’s theory of postmodernism given in his book The Postmodern Condition (1984). This Lyotardian paradigm extends a subversive challenge to all the grand narratives that have been enjoying the status of an enviable complete code of life in the world for a long time. Even a cursory scan over the play under analysis creates a strong feel that Beckett very smartly, comprehensively and successfully questioned the relevance of the totalizing metanarratives to the present times. Being an imaginative writer, he was well aware of the fact that ridicule is a much more useful weapon than satire in the context of political literature. There are so many foundationalist ideologies that he ridicules in his dramatic writing. Christianity as a religion is well exposed; the gravity of philosophy is devalued; the traditional luxury that the humans get from the art of poetry is ruptured and the great ideals of struggle are punctured. He achieves his artistic and ideologically evolved authorial intentions with a ringing success. It is interesting to note that he maintains a healthy balance between art and message. Keyswords: Beckett, Lyotard, The Postmodern Condition, Waiting for Godot Pages: 85-94 Article: 8 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)08 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)08 Download Pdf: download pdf view article Creative Commons License Effect of Parenting Styles on Students’ Academic Achievement at Elementary Level Authors: Hafsa Noreen Mushtaq Ahmad Uzma Shahzadi Abstract: The study intended to find out the effect of parenting styles on students’ academic achievement. Current study was quantitative in nature. All elementary level enrolled students at government schools in the province of the Punjab made the population of the study. Multistage sampling was used to select the sample from four districts of one division (Sargodha) of the Punjab province i.e., Sargodha. A sample size i.e., n=960; students and their parents were participated in this study. Research scales i.e. Parenting Styles Dimension Questionnaire (PSDQ) was adapted to analyze and measure parents’ parenting styles and an achievement test was developed to measure the academic achievement of the elementary students. After pilot testing, reliability coefficient Cronbach Alpha values for PSDQ and achievement test were 0.67 and 0.71 Data was collected and analyzed using frequencies count, percentages, mean scores and one way ANOVA. Major findings of the study were; Majority of the parents had authoritative parental style, a handsome number of parents keep connection of warmth and support with their children, show intimacy, focus on discipline, do not grant autonomy to their children, do not indulge with their children and as well as a handsome number of students were confident during their studies and study, further, found that parental style had positive relationship with academic achievement. Recommendations were made on the basis of findings and conclusion such as arrangement of Parents Teachers Meetings (PTM‘s), parents’ training, provision of incentives and facilities to motivate families might be an inclusive component of elementary education program. Keyswords: Academic Achievement, Elementary Education, Parenting Styles Pages: 95-110 Article: 9 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)09 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)09 Download Pdf: download pdf view article Creative Commons License Kashmir Conflict and the Question of Self-Determination Authors: Izzat Raazia Saqib Ur Rehman Abstract: The objective of this paper is to explore relations between Pakistan and India since their inception in the perspective of Kashmir conundrum and its impact on the regional security. Kashmir is the unfinished agenda of partition and a stumbling block in the bilateral relations between Pakistan and India. After the partition of sub-continent in 1947, Pakistan and India got their sovereign status. Kashmir conflict, a disputed status state, is the byproduct of partition. Pakistan and India are traditional arch-foes. Any clash between Pakistan and India can bring the two nuclear states toe-to-toe and accelerate into nuclear warfare. Due to the revulsion, hostility and lack of trust between the two, the peaceful resolution of the Kashmir issue has been long overdue. Ever-increasing border spats, arms race and threat of terrorism between the two have augmented anxiety in the subcontinent along with the halt of talks between India and Pakistan at several times. Additionally, it hampers the economic and trade ties between the two. India, time and again, backtracked on Kashmir issue despite UN efforts to resolve the issue. Recently, Indian government has responded heavy-handedly to the Kashmiri agitators’ demand for sovereignty and revocation of ‘Special Status’ of Kashmir impacting the stability of the region in future. Keyswords: India, Kashmir Conundrum, Pakistan, Regional Security, Sovereignty Pages: 111-119 Article: 10 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)10 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)10 Download Pdf: download pdf view article Creative Commons License Exploring Image of China in the Diplomatic Discourse: A Critical Discourse Analysis Authors: Muhammad Afzaal Muhammad Ilyas Chishti Abstract: The present study hinges on the major objective of analyzing Pakistani and Indian diplomatic discourses employed in portrayal of image of China. Data comprises the official discourse which is used in diplomatic affairs of both the states. The extensive investigation seeks insights from the fundamentals of Critical Discourse Analysis propounded by van Dijk, Fairclough and Wodak with a special focus on Bhatia’s (2006) work. The study reveals that the image of China has always been accorded priority within Indian and Pakistani diplomatic discourse even though nature of bilateral relations among China, India and Pakistan is based on entirely different dynamics; Indian and Pakistani diplomatic discourses are reflective of sensitivities involved within the bilateral relations. Through employment of linguistic techniques of ‘positivity’, ‘evasion’ and ‘influence and power’, Indian diplomats have managed not to compromise over the fundamentals in bilateral relations with China despite Pakistan’s already strengthened and deep-rooted relations with China. While Pakistani diplomatic fronts have been equally successful in further deepening their already strengthened relations in the midst of surging controversies on CPEC, BRI and OBOR. Hence, diplomatic fronts of both the counties, through employment of ideologically loaded linguistic choices, leave no stone unturned in consolidation of the diplomatic relations with China. Keyswords: CDA, China Image, Corpus, Language of Diplomacy, Political Discourse Analysis Pages: 120-133 Article: 11 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)11 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)11 Download Pdf: download pdf view article Creative Commons License Students’ Perception about Academic Advising Satisfaction at Higher Education Level Authors: Rukhsana Sardar Zarina Akhtar Shamsa Aziz Abstract: The purpose of the study was to examine the students’ perception about academic advising satisfaction at higher education level. All the students from two years master (M.A) degree programme and four years (BS) degree programme of eight departments from International Islamic University Islamabad (IIUI), Faculty of Social Sciences were taken as a population of the study. 475 students were randomly selected as a sample of the study. The Academic Advising Inventory (AAI) was used to assess Academic Advising Style. For measuring level of the satisfaction, descriptive statistics was used. To compare the mean difference department-wise and gender-wise about academic advising satisfaction t.test was applied. It was concluded that from the major findings of the study those students who received departmental academic advising style are more satisfied as compared to those students who provided prescriptive academic advising style. Female students seemed more satisfied as compared to male students regarding the academic advising style provided to them. Students who satisfied from developmental academic advising style and they were also highly satisfied from the advising provided to them at Personalizing Education (PE) and this is the subscale of developmental academic advising whereas students who received prescriptive academic advising they were also satisfied from the advising provided to them regarding personalizing education and academic decision making but their percentage is less. It is recommended to Universities Administration to focus on Developmental Academic Advising Style and establish centers at universities/department level and nominate staff who may be responsible to provide developmental academic advising. Keyswords: Academic Advising, Higher Level, Students’ Perception Pages: 134-144 Article: 12 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)12 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)12 Download Pdf: download pdf view article Creative Commons License Perceptions of Sexual Harassment in Higher Education Institutions: A Gender Analysis Authors: Ruhina Ghassan Dr. Subha Malik Nayab Javed Abstract: Sexual harassment is a social issue which is present in every society, globally, which interferes in an individual’s social and professional life. It happens almost everywhere i.e. at workplaces, public places or institutes as well. The focus of the present study was to explore the differences of male and female students’ perception of sexual harassment. This study was a quantitative research. Sample of the study included of 400 students (200 males and 200 females) from two government and two private universities. In the present study, Sexual Harassment Perception Questionnaire (SHPQ) was used to find out these differences in perceptions as every person has his own view for different situations. The study revealed the significant differences in perception of students. Study showed that both genders perceived that female students get more harassed than male students. The factors that affect the perception frequently were gender and age. The findings recommended that regulations for sexual harassment should be implemented in universities; laws should be made for sexual harassment in higher education institutes. Students should be aware of sexual harassment through seminars, self-defense classes and awareness campaigns. And every institute should have a counseling center for the better mental health of students. Keyswords: Gender Differences, Higher Educational Institutions, Sexual Harassment Pages: 145-158 Article: 13 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)13 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)13 Download Pdf: download pdf view article Creative Commons License Role of IMF Over the Governance Structure and Economic Development of Pakistan Authors: Ali Qamar Sheikh Dr. Muhammad Imran Pasha Muhammad Shakeel Ahmad Siddiqui Abstract: Developing countries like Pakistan seeks for financial assistance in order to fulfil their deficits. IMF is one of the largest financial institution who give loans to countries who need it. This research has studied the IMF role and the effects of IMF conditions on the economy of Pakistan. To carry out this research, both quantitative data from primary sources has been gathered and qualitative analysis has been made to signify whither this borrowing creating and maintaining dependency of Pakistan on West and financial and governance structure constructed to curtail Countries like Pakistan. The results concluded that there is negative and insignificant relationship between GDP and IMF loans in the long run. The short-term dynamic shows that weak economic and Political Institutions in Pakistan. The Development dilemma constitutes dependency even today. The Current Budget Deficit Pakistan's fiscal deficit climbs to Rs 3.403 trillion in 2020-21 needs to be readdressed in such a manner that Pakistan can counter Balance of Payments and import/export imbalance. Keyswords: Dependency, Development, IMF, Loans, Debt, Pakistan, Governance structure Pages: 159-172 Article: 14 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)14 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)14 Download Pdf: download pdf view article Creative Commons License Climate Change and the Indus Basin: Prospects of Cooperation between India and Pakistan Authors: Sarah Saeed Prof. Dr. Rana Eijaz Ahmad Abstract: Climate change is transforming the global societies. The shift in average temperature is putting negative impacts on human health, food production and the natural resources. In the wake of the altered climate, water flow in the river systems is experiencing variability and uncertainty. This paper aims at studying the negative impacts of climate change on the water resources of the Indus Basin and investigate the prospects of cooperation between India and Pakistan; two major riparian nations sharing the basin. Adopting the case study approach, a theoretical framework has been built on the ‘Theory of the International Regimes’. It has been argued that institutional capacity and the dispute resolution mechanism provided in any water sharing agreement determine the extent of cooperation among the member states. Since India and Pakistan are bound by the provisions of the Indus Waters Treaty, this study tries to assess the effectiveness of this agreement in managing the negative consequences of the climate change. Keyswords: Climate Change, Cooperation, Dispute Resolution Mechanism, Institutional Capacity Pages: 173-185 Article: 15 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)15 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)15 Download Pdf: download pdf view article Creative Commons License Translation, Cultural Adaptation and Validation of Behavioral-Emotional Reactivity Index for Adolescents Authors: Saima Saeed Farah Malik Suzanne Bartle Haring Abstract: Measuring differentiation of self in terms of behavioral/emotional reactivity towards parents is important because of the complex parent-child connection. This needs a valid and reliable measure to assess the differentiation of self particularly in a relationship with parents. Behavior\Emotional Reactivity Index is such a tool that fulfills this purpose. The present study was carried out to culturaly adapt and translate BERI into the Urdu language and establish the psychometric properties of Urdu version. A sample of 303 adolescents of age (M = 16.07, SD = 1.77) was taken from different schools and colleges. Scale was split into Mother and father forms for the convenience of respondents. Findings supported the original factor structure of the BERI-original version. Higher-order factor analysis showed good fit indices with excellent alpha ranges (α= .91 to α=.80). BERI scores were compared for the adolescents who were securely attached with parents and insecurely attached with parents which showed a significant difference between the groups. BERI-Urdu version was found to be a valid and reliable measure in the Pakistani cultural context which gives researchers new directions to work with adolescents. Keyswords: Adolescence, Differentiation of Self, Behavioral, Emotional Reactivit, Index, Parental Attachment Pages: 186-200 Article: 16 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)16 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)16 Download Pdf: download pdf view article Creative Commons License Notion of Repression in Modern Society: A Comparative Analysis of Sigmund Freud and Herbert Marcuse Authors: Khadija Naz Abstract: One of the fundamental issues for modern civilized man is how to adapt a modern society without losing his individual status. Is it possible for an individual to adjust in a society where he/she loses his/her individuality and becomes part of collectivity? One point of view is that for society to flourish, man needs to be repressed. But to what extent is repression necessary for societies to rise and survive? This paper shall examine the above given questions from the standpoint of two thinkers who greatly influenced twentieth-century thought: Sigmund Freud and Herbert Marcuse. To undertake this task, first the term Repression shall be examined and then the notions of Freud and Marcuse will be discussed to determine the degree of repression required for the development of modern society. Keyswords: Modern Society, Performance Principle, Repression, Surplus-Repression, The Pleasure Principle, The Reality Principle Pages: 201-214 Article: 17 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)17 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)17 Download Pdf: download pdf view article Creative Commons License Perceptions of Teacher Educators about Integration of (ESD) in Elementary Teachers Education Program Authors: Dr. Rukhsana Durrani Dr. Fazal ur Rahman Dr. Shaista Anjum Abstract: Education and sustainable development have a close relationship as education provides sustainability to society. This study explored the perceptions of teacher educators for integration of Education for Sustainable Development (ESD) in B.Ed. 4 years’ elementary program. Four major components of ESD i.e., Education, Social &amp; Culture, Economic and Environment were included in study. 127 teacher educators from departments of education were randomly selected from public universities of Pakistan who were offering B.Ed. 4 years’ elementary program. Data was collected through questionnaires from teacher educators. The findings recommended the inclusion of the components of Education for Sustainable Development (ESD) in curriculum of B.Ed. 4 years’ elementary program. Keyswords: B.Ed. 4 Years Elementary Curriculum, Sustainable Development, Integration, Teacher Education Pages: 215-225 Article: 18 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)18 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)18 Download Pdf: download pdf view article Creative Commons License Exploring TPACK skills of prospective teachers and challenges faced in digital technology integration in Pakistan Authors: Tariq Saleem Ghayyur Dr. Nargis Abbas Mirza Abstract: The current study was aimed to explore TPACK skills of prospective teachers and challenges faced in digital technology integration in Pakistan. The study was qualitative in nature and semi structured interview schedule was developed to collect data from prospective teachers. Purposive sampling technique was employed to collect data from 20 prospective teachers of 7 public sector universities. It was concluded that majority of the prospective teachers used general technological and pedagogical practices (GTPP), technological knowledge practices (TKP), Technological Pedagogical Knowledge practices (TPKP), Technological Content Knowledge practices (TCKP). Majority of prospective teachers reported multiple challenges in integration of digital technology in teacher education programs including lack of teacher training as one of the largest hurdle in digital technology integration, lack of digital technology resources or outdated digital technology resources, inadequate computer lab, lack of learning apps (courseware), financial constraints, lack of teachers’ motivation to use digital technology, slow computers available at computer labs, and unavailability of technical support. It was recommended that digital technology infrastructure should be improved across all teacher education institution and it was further recommended that TPACK model of digital technology integration should serve digital technology integration in teacher education programs in Pakistan. Keyswords: Challenges, Digital Technology Integration, Digital Technology Resources, Digital Technology, TPACK Pages: 226-241 Article: 19 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)19 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)19 Download Pdf: download pdf view article Creative Commons License Revisiting the Linkage between Money Supply and Income: A Simultaneous Equation Model for Pakistan Authors: Zenab Faizullah Dr. Shahid Ali Muhammad Imad Khan Abstract: A reliable estimate of the money supply is an important sign of the Gross Domestic Product (GDP) and many other macroeconomic indicators. It is widely discussed that over a long period of time, there is a strong link between GDP and money supply. This link is significantly important for formation of monetary policy. The main aim of this study is to estimate the income-money supply model for Pakistan. This study estimates the income-money supply model for Pakistan over the period of 2009 to 2019. The study uses Two Stage Least Square (2SLS) econometric technique due to the presence of endogeneity problem in the model under consideration. The existence of simultaneity between money supply (M2) and income (GDP) is also clear from the results of Hausman Specification test for simultaneity between M2 and GDP. The results further show that there exists a strong money-income relationship in case of Pakistan. Keyswords: Money Supply, Income, Simultaneous Equations Pages: 242-247 Article: 20 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)20 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)20 Download Pdf: download pdf view article Creative Commons License Analyzing the Mechanism of Language Learning Process by the Use of Language Learning Strategies Authors: Shafiq Ahmad Farooqi Dr. Muhammad Shakir Sher Muhammad Awan Abstract: This analytical research study involves the use of learning strategies to know the mechanism of learning a second language. People acquire their native language (L1) without any conscious effort and they have a complete knowledge of L1 and are competent in their native language even without going to school. It is believed that language learning is a process as well as an outcome and the focus of current study is to understand the process of learning a second language. The population in this study comprised of 182 boys and Girls Govt. Higher Secondary Schools studying at intermediate level in the 11 Districts of the Southern Punjab. The sample was selected through random probability sampling and consisted of 40 subject specialists teaching the subject of English in Govt. higher secondary schools with 400 students studying English at Intermediate level. A questionnaire comprising some common and easily accessible learning strategies was designed to determine the frequency of these strategies used in the classrooms by the language learners through the specialists of the subject. The data was collected from the selected sample through the subject specialists teaching in these schools. The data was collected quantitatively and was analyzed in the statistical package for social sciences (SPSS) version 20. The most common 27 language learning strategies (LLS) were applied to analyze the process of language learning. In the light of the results of the study, it was concluded that application of the learning strategies according to the nature of the text is helpful in understanding the language functions and its application. Keyswords: Language Acquisition, Learning Strategies, Mechanism of Language Learning Pages: 249-258 Article: 21 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)21 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)21 Download Pdf: download pdf view article Creative Commons License Secondary School Science Teachers’ Practices for the Development of Critical Thinking Skills: An Observational Study Authors: Dr. Muhammad Jamil Dr. Yaar Muhammad Dr. Naima Qureshi Abstract: In the National curriculum policy documents, to produce rationale and independent critical thinkers, different pedagogical practices have been recommended like cooperative learning, questioning, discussion, etc. This qualitative case study aimed at analyzing secondary school science teachers’ practices for the development of critical thinking skills in secondary school students. There were twelve classrooms (four from each subject of Physics, Chemistry and Biology) selected as cases. Video recording was used for the observations for six lessons in each classroom. In this way, a total of 72 observations were conducted lasting for approximately 35 minutes. Qualitative content analysis was used for data analysis through Nvivo 12. The findings of the observations revealed that all the teachers used the lecture method. They used this to cover the content at a given specific time. There was not much focus on the development of critical thinking. In a few of the classrooms, the students were engaged and active during learning different specific topics. Whiteboard was used as a visual aid by most of the teachers. Furthermore, to some extent, discussion, questioning, and daily life examples were used in different classrooms. It is recommended that teachers’ professional development should be conducted to focus on the development of critical thinking skills through pedagogical practices which have been recommended by the national education policy documents. Keyswords: Analysis, Critical Thinking, Curriculum Policy, Pedagogy, Secondary Level Pages: 259-265 Article: 22 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)22 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)22 Download Pdf: download pdf view article Creative Commons License Historical Development of Clinical Psychology in Pakistan: A Critical Review-based Study Authors: Muhammad Nawaz Shahzad Dr. Mushtaq Ahmad Dr. Muhammad Waseem Tufail Abstract: Clinical Psychology is clinical and curing psychological practices in Pakistan. The present research study endeavors to examine the contemporary status of Clinical Psychology in the country and descriptively analyzes the significant contribution of various psychologists in its development. The study also elaborates the emergence of Clinical Psychology and its treatment aspects in the country. The experimental approach of the treatment psychology has also been defined. The role of different scholars to set and promote the Clinical Psychology as discipline and dealing about treatment of Human mind has also been discussed here. The study also presented the scenario of the issues of legislative acknowledgment, qualifications mandatory for practice, communal awareness of cerebral treatment, the tradition of ethnic and native practices about the clinical psychological treatments has also been discussed. Keyswords: Approaches, Clinical Psychology, Psychologist, Therapist Pages: 266-272 Article: 23 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)23 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)23 Download Pdf: download pdf view article Creative Commons License Impact of Devolution of Power on School Education Performance in Sindh after 18th Constitutional Amendment Authors: Abdul Hafeez Dr. Saima Iqbal Muhammad Imran Abstract: Devolution of the authority from central units of empowering authorities to the local level to develop and exercise policies at local or organizational level is under debate in various countries of the world. The legation in with the name of 18th constitutional amendment in constitution of 1973 of Pakistan ensures more autonomy to federal units. The difference between province and federation mostly creates misunderstanding in the belief of cooperation and universalism of education standards, expenditures and service delivery. Very currently the ministry of education and local government encoring principles and headmasters to adopt self-management skills to be updated to accept the spin of power from higher authorities to lower authorities’ pedagogical and local schools. In this qualitative research semi structured questioner were incorporated as data collection tool equally, the data was analyzed by usage of NVivo software. In this regard Government of Sindh has introduced various reforms and new trends like objectives and policy pillars, better government schools, improved learning outcomes and increased and improved funding in the education sector Sindh government has so far been unable to effectively use its resources to implement effective governance system which provides quality and sustained education in the province. To achieve this basic universal education, equally fourth objective of Sustainable Development Goal (SDG) the educational leaders must develop a comparative education setup that help to educate planers to plan and design standards for school leaders, instruction, appropriate professional development of teachers, ways to support school leaders to change in mission. Parallel, develop new program for early childhood, school and class size and ensure school enrollment. Keyswords: 18th Constitutional Amendment, Devolution of Power, Sindh Education Performance Pages: 273-285 Article: 24 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)24 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)24 Download Pdf: download pdf view article Creative Commons License Legal Aspects of Evidence Collected by Modern Devices: A Case Study Authors: Muhammad Hassan Zia Alvina Ali Abstract: This paper is a qualitative research of different case laws dealing with modern technological evidence. Courts were required to adopt new methods, techniques and devices obtained through advancement of science without affecting the original intention of law. Because of modern technology, a benefit could be taken from said technology to preserve evidences and to assist proceedings of the Court in the dispensation of justice in modern times. Owing to the scientific and technological advancements the admissibility of audio and visual proofs has grown doubtful. No doubt modern evidence assist the court in reaching out to the just decision but at the same time certain criteria need to be laid down which must be satisfied to consider such evidence admissible. Different Case laws are discussed here to show how the cases were resolved on the basis of technological evidence and when and why such evidence have been rejected by the court, if it did. Moreover, legal practices developed in various countries allow our Courts to record evidence through video conferencing. The Honorable Supreme Court of Pakistan directed that in appropriate cases statement of juvenile rape victims and other cases of sensitive nature must be recorded through video conferencing to avoid inconvenience for them to come to the Court. Nevertheless, it has some problems. The most important among them is the identification of the witness and an assurance that he is not being prompted when his statement is recorded. In this paper protocols that are necessary to follow while examining witness through video link are discussed Keyswords: DNA Profiling, Finger Prints, , Telephone Calls, Video Tape Pages: 286-297 Article: 25 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)25 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)25 Download Pdf: download pdf view article Creative Commons License The Political Economy of Terrorisms: Economic Cost of War on Terror for Pakistan Authors: Muhammad Shakeel Ahmad Siddiqui Dr. Muhammad Imran Pasha Saira Akram Abstract: Terrorism and its effect on contemporary society is one of the core and vital subjects of International Political Economy (IPE) during the last years. Despite the fact that this is not a new phenomenon, special attention has been given to this issue, specifically after the terrorist attacks of 9/11, 2001. The objective of this paper analyzes to what dimensions terrorism affects the global economy mainly the two predominant actors of the conflict i.e. Pakistan and the United States. For this purpose, this article will take a look at the financial cost of War for Pakistan and how Pakistan’s decision to become frontline State has affected its Economy, its effect on agriculture, manufacturing, tourism, FDI, increased defense costs The normative and qualitative methodology shows a significant disadvantage between terrorist activities and economic growth, social progress, and political development. The results shows that Pakistan has bear slow economic growth while facing terrorist activities more than US. In this last section, the paper suggests ways and means to satisfy people around the world not to go in the hands of fundamentals and terrorists. Keyswords: Cost of War, Economic Growth, Frontline States, Pak Us Relations, Terrorism Pages: 297-309 Article: 26 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)26 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)26 Download Pdf: download pdf view article Creative Commons License A Comparative Study of Grade 10 English Textbooks of Sindh Textbook Board and Cambridge “O Level” in the perspective of Revised Bloom’s Taxonomy Authors: Mahnoor Shaikh Dr. Shumaila Memon Abstract: The present study evaluated the cognitive levels of reading comprehension questions present in grade 10 English Textbooks namely English Textbook for grade 10 by Sindh Textbook Board and compared it to Oxford Progressive English book 10 used in Cambridge “O Level” in the perspective of Revised Bloom’s Taxonomy. Qualitative content analysis was used as a methodology to carry out the study. To collect the data, a checklist based on Revised Bloom’s taxonomy was used as an instrument. A total of 260 reading comprehension questions from both the textbooks were evaluated. The findings of the study revealed that reading comprehension questions in English textbook for grade 10 were solely based on remembering level (100%) whereas the questions in Oxford Progressive English 10 were mainly based on understanding level (75.5%) with a small percentage of remembering (12.5%), analyzing (11.1%) and evaluating level (0.74%). This suggests that the reading comprehension questions in both the textbooks are dominantly based on lower-order thinking skills. Keyswords: Bloom’s Taxonomy, Content Analysis, Reading Comprehension, Textbook Evaluation Pages: 310-320 Article: 27 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)27 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)27 Download Pdf: download pdf view article Creative Commons License Assessing the Preparedness of Government Hospitals: A Case of Quetta City, Balochiatan Authors: Sahar Arshad Syed Ainuddin Jamal ud din Abstract: Earthquake with high magnitude is often resulting in massive destruction with more causalities and high mortality rate. Timely providence of critical healthcare facilities to affected people during an emergency response is the core principle of disaster resilient communities. The main objective of this paper is assessing the hospital preparedness of government hospitals in Quetta. Primary data was collected through questionnaire survey. Total of 165 sample size chosen via simple random sampling. Relative important index (RII) is used to analyze the overall situation of hospitals preparedness in term of earthquake disaster. Findings of the study showed that the preparedness level of government hospitals in Quetta is weak to moderate level. Based on the findings this study recommends the necessary measures to minimize the risk of earthquake disaster including training and exercise programs for the staff of hospital, proper resource management to efficiently use the existing machinery and equipment in the meeting of disaster to enhance employee’s performance and preparedness of government hospitals in Quetta to deal with earthquake disaster. Keyswords: Earthquake, Preparedness, Relative Important Index Pages: 321-329 Article: 28 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)28 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)28 Download Pdf: download pdf view article Creative Commons License Development of Reasoning Skills among Prospective Teachers through Cognitive Acceleration Approach Authors: Memoona Bibi Dr. Shamsa Aziz Abstract: The main objectives of this study were to; investigate the effects of the Cognitive Acceleration approach on the reasoning skills of the prospective teachers at the university level and compare the effects of the Cognitive Acceleration approach and traditional approach concerning reasoning skills of prospective teachers’ at the university level. The study was experimental and followed a pre-test post-test control group experimental design. The sample of the study included the experimental group and control group from the BS Education program in the Department of Education at International Islamic University Islamabad. A simple random sampling technique was used to select the sample after pre-test and pairing of prospective teachers. CTSR (classroom test for scientific reasoning) developed by A.E. Lawson (2000) was used to collect the data through pre-tests and post-tests. The experimental group’s perception about different activities of the experiment was taken through a self-made rating scale. Collected data were analyzed by calculating mean scores and t-test for hypothesis testing by using SPSS. The main findings of the study revealed that the Cognitive Acceleration teaching approach has a significant positive effect on the reasoning skills development of prospective teachers at the university level. Findings also showed that participants found this teaching approach effective and learned many new concepts and skills with the help of thinking activities. Based on findings it has been concluded that the Cognitive Acceleration teaching approach might be encouraged for training prospective teachers at the university level and training sessions about the use of the Cognitive Acceleration approach must be arranged by teacher education programs and institutions. Keyswords: Cognitive Acceleration Approach, Prospective Teachers, Reasoning Skills, Traditional Approach Pages: 330-342 Article: 29 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)29 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)29 Download Pdf: download pdf view article Creative Commons License Spatial Injustice in Shamsie’s Kartography Authors: Syeda Hibba Zainab Zaidi Dr. Ali Usman Saleem Sadia Waheed Abstract: Social space under postmodernism and wave of globalization have suffered in and its idealistic representations are lost and deteriorated which ultimately led to discursiveness in the lives of postmodern man, especially Karachiites. The boundaries of geographies play a significant role in shaping fates, biographies, social superstructures and shared collective histories of its residents. Considering this, Henri Lefebvre and Edward William Soja, argue that space is something which determines the living circumstances within the particular social framework and instigates and controls various societal happenings. City space of Karachi suffers from appalling distortions as a part of postmodern, globalized and capitalist world. By employing Lefebvre’s idea of spatial triad and Soja’s views of the trialectrics of spaciality, this paper foregrounds how social space enforces spatial injustice and serves for the inculcation of spatial cleansing in the lives of inhabitants of urban space. Using Shamsie’s Kartography as an interpretive tool for contemporary urban environment, this paper inquires the engrafting of spatial cleansing in the lives of Karachiites resulting in multiple standardization and segregation on the basis of living standards among different social strata. This research substantiates how in Kartography, Materialism nibbles the roots of social values and norms while sequentially administering Spatial Injustice in the lives of Karachiites. This paper proclaims the scarcity of execution of Spatial Justice in the lives of common people in this postmodern globalized capitalist era. This paper urges the possibility of a utopian urban space with enforced spatial justice where people can be saved from dilemmas of injustice and segregation, especially Karachiites. Keyswords: Capitalistic Hegemony, City Space, Globalization, Spatial Cleansing, Spatial Injustice Pages: 343-352 Article: 30 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)30 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)30 Download Pdf: download pdf view article Creative Commons License A Quasi-Experimental Study on the Performance and Attitudes of Pakistani Undergraduate Students towards Hello English Language Learning Application Authors: Wafa Pirzada Dr. Shumaila Memon Dr. Habibullah Pathan Abstract: With the advancement of technology, more and more avenues of bringing creativity and innovation in language learning have opened up. These exciting advances have given rise to a new field of study within linguistics, termed Mobile Assisted Language Learning (MALL). This paper aims to fill the gap of MALL research in the area of grammar teaching in the Pakistan. Two BS Part 1 classes from University of Sindh, Jamshoro, were chosen for this quasi-experimental study. In total, 62 out of 101 students volunteered to use the Hello English application for 2 months, making up the experiment group, and the remaining 39 students were put in a control group. Paired Samples T-Test was run on pretest and posttest results which revealed no significant difference in both groups’ performances, proving that Hello English application could not significantly improve students’ grammar performance. However, in spite of the lack of a significant difference between the test results, the data gathered through the attitudinal survey showed that students still found mobile application very easy to use and effective in language learning. Keyswords: Attitudes, Grammar Learning, Hello English, Mobile Language Learning, Technology In Language Learning Pages: 353-367 Article: 31 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)31 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)31 Download Pdf: download pdf view article Creative Commons License Impact of Determinants on the Profile Elevation of Secondary School Teachers in Pakistan Authors: Zahida Aziz Sial Dr. Farah Latif Naz Humaira Saadia Abstract: The foremost purpose of this research paper was to interrogate the effects of determinants on the educational and social profile of secondary school teachers in Pakistan. The key question taken was related to determinants that affect teachers’ profile. The Population of the study was secondary school teachers of Punjab province. A questionnaire was used as research instrument. The researcher personally visited the schools to administer the questionnaire. E-Views software was used for data analysis. Moreover, OLS regression model and LOGIT regression model were carried out. It was found that the variable years of teaching experience (EXPYR) (*** 0.03) can have a vital concrete effect upon the societal figuration of teachers as the experience of teachers grows, so does their social interactions with officials, colleagues, students and friends increases. The said variable is significant at 10 percent level. The variable, Residence (RESIDE) (** 0.53) have a significant impact upon civic links. This obviously associated with less community connection of country side teachers than the teachers residing in urban areas. Keyswords: Determinants, Elevation, Educational Profile, Social Profile, Secondary School Teacher Pages: 368-372 Article: 32 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)32 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)32 Download Pdf: download pdf view article Creative Commons License Impact of War on Terror on the Tourism Industry in Swat, Pakistan Authors: Sabir Ihsan Prof. Dr. Anwar Alam Aman Ullah Abstract: The present study was designed to ascertain the status of tourism before insurgency, during insurgency and after insurgency in District Swat-KP Pakistan. The study is quantitative and descriptive in nature. A diverse sample size of 370 out of 9014 was selected through convenient sampling strategy. Notwithstanding, the objectives of the study was achieved through structured questionnaire. Data was analysed through chi-square at Bi Variate level. Findings of the study revealed that earning livelihood in swat was significantly associated (P=0.016), (P=0.003) with tourism industry prior 2009 and present time respective, but the same statement was observed non-significant (P=0.075) at the time of insurgency. Arranging different festivals in the study area and establishment of different showrooms for local handcrafts, artificial jewellery and woollen shawl are some of the recommendations of the study. Keyswords: Business, Insurgency, Swat, Tourism Pages: 373-385 Article: 33 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)33 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)33 Download Pdf: download pdf view article Creative Commons License Challenges and Prospects of Pak-China Economic Corridor Authors: Muhammad Mudabbir Malik Prof. Dr. Muqarrab Akbar Abstract: Pak-China has historic relationships from the emergence of both states, and were proved long-lasting in every thick and thin times. In initial times they supported each other in foreign policies and regional issues. Pakistan and China have border disputes with India, which forced them to come close to counter India, letter on the economic interests strengthened these relations. In order to maximize the economic benefits, China announced economic corridor with the name China Pakistan Economic Corridor (CEPC). It was thought it will boost the economic growth of China, and as a prime partner Pakistan will also get economic benefits. In order to completely understand how Pakistan and China came on the same page and decided to put CPEC into reality we have to understand the Geo-political Importance of Pakistan, Strategic and economic importance of CPEC for China and Pakistan, Influence and concerns of West and neighboring countries including India. Domestic limitations and all the possible benefits and risks involved in this project for both Pakistan and China, this research acknowledges all these questions. Keyswords: Challenges, China, CPEC, Domestic Limitations Economic Growth, Pakistan, Western and Regional Concerns Pages: 386-404 Article: 34 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)34 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)34 Download Pdf: download pdf view article Creative Commons License An Analysis of Learning Practices and Habits of Children at Early Childhood Education: Students’ Perspective Authors: Masood Ahmad Sabiha Iqbal Shaista Noreen Abstract: The study was designed to analysis learning practices and habits of children at early childhood education. The major objective of the study was to find out the learning practices and habits of children. Problem was related to current situation, so survey method was exercised, 220 students were selected with the help of convenient sampling technique. Self-constructed questionnaire were exercised. The collected data was analyzed and calculate frequency, percentage, mean score, standard deviation and t-test of independent variable. The major findings of the study were; students learn from the pictures, cartoons and funny face; student’s eyes get tired of reading. When student read context continuously then they feel that their eyes get tired. There was a significance difference between male and female student about learning practices and habits of children. Keyswords: Early Childhood Education, Learning Practices and Habits, Pre-School Students Pages: 405-416 Article: 35 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)35 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)35 Download Pdf: download pdf view article Creative Commons License Gender Identity Construction in Akhtar’s Melody of a Tear Authors: Dr. Amna Saeed Hina Quddus Abstract: This study aims to discuss the notion of gender in terms of performativity and social construction. It also draws upon the idea of gender identity construction and how it relates to the society, performativity and biology. As its theoretical framework, the study relies upon the Performative Theory of Gender and Sex (1990) presented by Judith Butler and studies the gender identity construction in the female protagonist of Akhtar’s Melody of a Tear. Zara is a girl who is raised as a boy from his father and there is a kind of dilemma in Zara’s personality related to being masculine and feminine. The cultural norms of a particular gender are also a cause of this dilemma. Throughout the novel, she is in a conflicting state whether she should behave feminine or masculine. She is being depicted as an incomplete person until she finds and resolves this issue of gender identity. The paper discusses the gender performativity, social construction, cultural norms and identity as these are all contributing to the confusion and construction of the protagonist’s identity. Character analysis is used as the methodology of analysis. Keyswords: Cultural Norms, Femininity And Identity Confusion, Gender, Performativity, Masculinity, Social Construction Pages: 417-427 Article: 36 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)36 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)36 Download Pdf: download pdf view article Creative Commons License The Level of Impulsivity and Aggression among Crystal Meth and Cannabis Users Authors: Dr. Umbreen Khizar Muhammad Shafique Sana Nawab Abstract: Cannabis and crystal meth use is pervading in our society. Present study was conducted to explore the relationship between level of impulsivity and aggression among crystal meth and cannabis users. The sample of the present study was comprised of 100 participants. There were 50 cannabis and 50 crystal meth users who were diagnosed on the basis of DSM-V without any comorbidity. The sample were taken from all age range of population. The minimum education level was primary and maximum education level was graduation and above. The sample was selected from different drug rehabilitation centers of Rawalpindi and Islamabad, Pakistan. Demographic Performa was used to collect the initial important information, The “Barratt Impulsiveness Scale was used to measure the impulsivity and “Aggression Questionnaire” were used to measure the level of aggression. Finding of the study showed that there are significant differences among crystal meth and cannabis users on level of aggression. The calculated mean value for crystal meth user and for cannabis users indicates that crystal meth users have higher level of aggression as compared to the cannabis user. Over all analysis indicates a significant positive correlation of impulsivity with the variable aggression. The alpha coefficient value for all scale is acceptable. Keyswords: Aggression, Cannabis Users, Crystal Meth, Impulsivity Pages: 428-439 Article: 37 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)37 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)37 Download Pdf: download pdf view article Creative Commons License Impact of Social Factors on the Status of Tribal Women: A Case Study of the (Erstwhile) Mohmand Agency Authors: Sadia Jabeen Prof. Dr. Anwar Alam Muhammad Jawad Abstract: This study investigates the impact of socio-economic and cultural factors on the status of tribal women in the erstwhile Mohmand agency of the Ex-Federally Administered Tribal Area (FATA), Pakistan. Cultural practices and illiteracy impede the role of women in socio-economic development. The respondents were randomly selected from tehsil Ekka Ghund and Pindialai with a sample size of 370, through stratified random sampling. Data collected through structured interview schedule, FGD and observation technique. The study reveals that tribal practices early marriages, joint family system, tradition of forced marriages, compensation/Swara, exchange, purchase marriages, hampers women’s socioeconomic status. The illiteracy rate is high among the tribal women and it further undermines their role and negatively affects their socio-economic status. However, improvement in women status needs peace and stability, reforms in the constitution for women empowerment and active participation, improvement in the quality and quantity of education, women employability, skills development and women entrepreneurship Keyswords: Empowerment and Education, Marriage Types, Tribal Women Role, Tribal Women Status, Violence against Women Pages: 440-455 Article: 38 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)38 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)38 Download Pdf: download pdf view article Creative Commons License Effects of Heavy School Bags on Students’ Health at Primary Level in District Haveli (Kahutta) Azad Jammu and Kashmir Authors: Dr. Muhammad Mushtaq Shamsa Rathore Mishbah Saba Abstract: Heavy school bags is a very serious issue for the health of the primary level students throughout the world particularly in Azad Jammu and Kashmir. This study intends to explore the effect of heavy school bags on students’ health at primary level in district Kahuta. Naturally the study was descriptive and survey method was used, the population consists of one hundred ninety teachers and a sample of one hundred twenty seven teachers was selected using non probability sampling technique. A likert scale questionnaire was developed validated and distributed among the sampled respondents. The researcher personally visited the schools and collected the filled questionnaire. The data was coded and fed to the SPSS to analyze and interpret. The Chi Square test was applied to see the effect of heavy school bags on student’s health and academic achievement. The study found that heavy bags have negative effect on their health as well as their academic achievement. Students were found complaining their sickness, body and back pain. They were also found improper in their gait and their body postures. The researcher recommended the policy makers to take and develop strategies to decrease the heavy school bags. The school administration needs to make alternate days’ time tables of the subjects. Keyswords: Health, Primary Level, School, Bags, Students Heavy Pages: 456-466 Article: 39 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)39 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)39 Download Pdf: download pdf view article Creative Commons License Exploring the ‘Civil Repair’ Function of Media: A Case Study of The Christchurch Mosques Shootings Authors: Ayaz Khan Dr. Muhammad Junaid Ghauri Riffat Alam Abstract: This research endeavor is an attempt to explore and analyze the discourse produced by The New Zealand Herald; a newspaper from New Zealand and by The News International; a Pakistani newspaper. The researchers intend to determine whether and to what extent both the newspapers have the role of ‘civil repair’ played after the Christchurch mosques shootings. The researchers have incorporated the ‘lexicalization’ and the ‘ideological square’ techniques proposed by Tuen A. van Dijk within the scope of Critical Discourse Analysis. The findings of this study show that both the selected newspapers assuming the social status of ‘vital center’ performed the role of ‘civil repair’ in the aftermath of the shootings by producing the ‘solidarity discourse’. The ‘solidarity discourse’ has been produced in terms of the ‘we-ness’, harmony, understanding, and by mitigating the conflicting opinions. Keyswords: Christchurch Mosque Shootings, Civil Repair, Civil Sphere Theory, Lexicalization, Solidarity Discourse Pages: 467-484 Article: 40 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)40 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)40 Download Pdf: download pdf view article Creative Commons License China Pakistan Economic Corridor: Regional Dominance into Peace and Economic Development Authors: Tayba Anwar Asia Saif Alvi Abstract: The purpose of this qualitative study was to investigate the true motivations behind CPEC idea and the advantages it delivers to Pakistan and China. It also recognizes the Corridor's potential for mixing regional economies while dissolving geographical borders. The study is deductive in character, since it examines financial, political, and military elements of Pakistan and China's positions and situations. Enhancing geographical linkages through improved road, train, and air transport systems with regular and free exchanges of development and individual’s interaction, boosting through educational, social, and regional civilization and wisdom, activity of larger quantity of investment and commerce flow, generating and moving energy to provide more optimal businesses for the region. Keyswords: Geographical Linkages, Globalized World, Landlocked, Regional Connectivity, Regionalization Pages: 485-497 Article: 41 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)41 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)41 Download Pdf: download pdf view article Creative Commons License China’s New Great Game in Central Asia: Its Interest and Development Authors: Bushra Fatima Rana Eijaz Ahmad Abstract: Central Asia is rich in hydrocarbon resources. It’s geostrategic, geopolitical, and geo-economic significance has grasped the attention of multiple actors such as China, the USA, Russia, Turkey, the European Union, Pakistan, Afghanistan, and India. Due to its location, the Central Asian region appeared as a strategic hub. In the present scenario, China’s strategy is massive economic development, energy interest, peace, and stability. This article highlights China’s interest, political and economic development, and its role as a major player in the New Great Game in Central Asia. Shanghai Cooperation Organization (SCO) which presents as a platform where China is playing an active role in political, economic, and security concerns for achieving its objectives in Central Asia. The new step of the Belt and Road Initiative (BRI) sheds light on China’s progressive move in this region via land and sea routes, which creates opportunities for globalization. Keyswords: Belt and Road Initiative, Central Asia, China, New Great Game Pages: 498-509 Article: 42 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)42 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)42 Download Pdf: download pdf view article Creative Commons License Personality Traits as Predictors of Self-Esteem and Death Anxiety among Drug Addicts Authors: Umbreen Khizar Saira Irfan Iram Ramzan Abstract: This study seeks to investigate whether personality traits predict self-esteem and death anxiety among drug addicts. The sample consisted of 100 drug addicts taken from the two hospitals in Multan city. Only men between the ages of 20 and 65 were included in the study. Data was collected through reliable and valid questionnaires. Results revealed positive relationship between conscientiousness, openness to experience and self-esteem. Moreover, findings showed positive relationship between extraversion and death anxiety, and negative correlation between neuroticism and death anxiety. Findings also showed that self-esteem and death anxiety are significantly and negatively correlated. Additionally, findings revealed that conscientiousness positively predicted self-esteem and neuroticism negatively predicted death anxiety. Furthermore, significant differences were observed in self-esteem, and death anxiety based on age. Significant differences were also found in extraversion, agreeableness, openness to experience, and death anxiety based on location. Understanding how personality traits affect behavior can help drug addicts get the support they need to live a better life and reduce their risk of death anxiety and premature death. Keyswords: Death Anxiety, Drug Users, Personality Traits, Self- Esteem Pages: 510-524 Article: 43 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)43 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)43 Download Pdf: download pdf view article Creative Commons License Middle East: A Regional Instability Prototype Provoking Third Party Interventions Authors: Waseem Din Prof. Dr. Iram Khalid Abstract: Third party interventions always prolong the interstate or civil wars with unending sufferings and devastations. The entire Middle East region is fraught with tensions, conflicts, civil wars and rivalries. From strategic interests to power grabbing, sectarian divisions, flaws in the civil and social structure of the state and society, ethnic insurrections, and many other shapes of instability syndromes can be diagnosed in this region. In the post-Arab Spring, 2011, the emerging new regional hierarchical order for power/dominance, in addition to the weakening/declining dominant US power in the region, changed the entire shape of already conflict-ridden region. New weak or collapsing states and bifurcation of the ‘status quo’ and ‘counter-hegemonic’ states along with their respective allies, made this region a prototype of instability in the regional security complex of the Middle East, as a direct result of these developments. The perpetuation of these abnormalities would not recede this instability conundrum from the region, provoking third party intervention, if not contained. Keyswords: Conflicts/Civil Wars, Dominant Power, Instability, Intervention, Middle East, Middle Powers, Regional Hierarchy, Regional Powers, Security Complex, Weak State Pages: 525-542 Article: 44 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)44 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)44 Download Pdf: download pdf view article Creative Commons License Impact of Classroom Environment on Second Language Learning Anxiety Authors: Zohaib Zahid Abstract: Second language learning anxiety has attained the attention of the researchers in almost every part of the world. Pakistan is a country where English is taught as a second language from the very beginning of school education. Second Language learning anxiety is a phenomenon which has been prominently found among the learners because of their less proficiency in learning English language. This study has been conducted to investigate the effect of anxiety in learning and using English language in classroom, university and outside the classroom. There are variables that affect language learning performance of the learners but this paper has solely investigated the effect of anxiety. The paper has concluded that anxiety is a variable which has a striking affect in second language learning and its use inside classrooms. Keyswords: Effect of Anxiety, Proficiency, Second Language Learning Anxiety, Striking Affect Pages: 485-497 Article: 45 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)45 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)45 Download Pdf: download pdf view article Creative Commons License Struggling for Democracy: A Case of Democratization in Pakistan Authors: Ammara Tariq Cheema Dr. Rehana Saeed Hashmi Abstract: The objective of this research paper is to review the challenges for democratization in Pakistan. The problem of democratization and consolidation refers to the structure of democracy following the collapse of non-democratic regime. Ten factors as given by Michael J. Sodaro are considered effective in helping a democratically unstable state to stabilize its system in other words helps in the democratic consolidation. It is argued in this research that the ten factors of democratization as given by Michael J. Sodaro have been absent in the political system of Pakistan and working on these factors can lead Pakistan to the road of democratization. This study uses qualitative method of research and proposes a novel framework for the deed of parliament, because the effectiveness of parliament can contribute positively to democratization/consolidated democracy. Keyswords: Electoral Politics, General Elections, Political Participation, Women Empowerment Pages: 554-562 Article: 46 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)46 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)46 Download Pdf: download pdf view article Creative Commons License Impact of Dependency Ratio on Economic Growth among Most Populated Asian Countries Authors: Dilshad Ahmad Salyha Zulfiqar Ali Shah Abstract: Demographic transition through different channels significantly influences economic growth. Malthusian view postulated as dependency ratio adversely affects economic growth while Julian Simon's view is quite different, highlighted the long-run benefits of the population in the range of 5 to15 years on economic growth. This study can be a valuable addition in research to analyzing the association of dependency ratio and economic growth of the five most populated Asian countries (Bangladesh, China, Indonesia, India, and Pakistan). Empirical findings of the study indicated that a total dependency and younger dependency ratio has a positive and significant influence on economic growth in both short-run and long-run scenarios while the old dependency ratio shows a negative influence on economic growth in the long run while short-run results are unpredictable. There is a need for state-based proper policy measures in focusing the higher financing in human capital development specifically in education and health. Keyswords: Economic Growth, Gross Saving, Old Dependency Ratio, Young Dependency Ratio Pages: 563-579 Article: 47 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)47 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)47 Download Pdf: download pdf view article Creative Commons License Chinese Geo-Strategic Objectives and Economic Interests in Afghanistan under President Xi Jinping Authors: Farooq Ahmed Prof. Dr. Iram Khalid Abstract: China has its own distinctive interests, concerns and strategies with respect to the changing security dynamics in Afghanistan. China has taken an active interest, though retaining a low profile and avoiding direct military interaction. China has exclusively relished on economic engagement actively and provided numerous financial aid and financial support in the rebuilding of Afghanistan's economy. The aim of this research study is to analyze the geo-strategic objectives and economic interests of China under the leadership of President Xi Jinping. This study looks at the actual diplomatic, economic and protection commitments of both countries as well as the basis of the geopolitical complexities – core variables that form China's current foreign policy to Afghanistan. Keyswords: Afghanistan, BRI, China, NATO Withdrawal Pages: 580-592 Article: 48 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)48 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)48 Download Pdf: download pdf view article Creative Commons License The Argument Structure of Intransitive Verbs in Pashto Authors: Abdul Hamid Nadeem Haider Bukhari Ghani Rehman Abstract: This study focuses on the description and categorization of intransitive verbs in terms of its argument structure. The study concludes that the unaccusative verbs only project an internal argument. It does not require the event argument. However, the said verb can be causativised by adding external argument and at the same time the event argument gets included in the valency of the derived causative of the unaccusative root. The unergative, on the other hand, requires an external argument as an obligatory argument while the internal argument is not the obligatory argument of the verb. The event argument is also a part of the valency of the verb. The APFs require one argument which is the internal argument of the verb. However, since the external argument is not available, the internal argument of the verb gets realized as the subject of the verb. The verb does not project event argument. The ergative predicates are derived by the suppression of the external argument and by the externalization of the internal argument. Keyswords: Argument Structure, Ergative Case, Event Argument, External Argument, Internal Argument, Valency Pages: 593-610 Article: 49 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)49 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)49 Download Pdf: download pdf view article Creative Commons License Positive, Negative and Criminal Orientation of Beggars in Okara: Perspective of Students Authors: Shahzad Farid Saif-Ur-Rehman Saif Abbasi Hassan Raza Abstract: This study aimed to measure the perspective of students about the criminal orientation of beggars. The sample size of the study (i.e., 100 students) was explored using Taro Yamane’ equation from the university of Okara, Punjab, Pakistan. The respondents were approached using simple random sampling and interviewed using face to face interview schedule. The data was collected using a structured questionnaire. The analysis was administered through SPSS-20.The study explored that parental illiteracy is associated with the high criminal and negative orientation of students towards beggars. It was also explored that females and respondents from rural background have low negative orientation towards beggars. However, males and respondents from urban background have medium criminal orientation and low positive orientation towards beggars, respectively. The study is useful for the government of Punjab, Pakistan campaign and policy for anti-begging. The study introduced the geometrical model of youth’s orientation toward begging. The study also contributed to the literature on begging by extending its domain from Law and Criminology to sociology as it incorporated social variables e.g., parents’ education, gender, etc., to explore their association with the youth’s socialization about begging. Keyswords: Begging, Crime, Education, Gender, Students Pages: 611-621 Article: 50 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)50 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)50 Download Pdf: download pdf view article Creative Commons License Relationship between Entrepreneurial Export Orientation and Export Entrepreneurship through Mediation of Entrepreneurial Capabilities Authors: Muhammad Saqib Nawaz Masood ul Hassan Abstract: Export led growth is prominent paradigm in developing world since decades. Exports play vital role in the economy by improving the level of balance of payments, economic growth and employment. Due to strategic importance of exports, organizational researchers focused on finding antecedents of export performance of the organizations. To line with this, current study aims to find the impact of entrepreneurial export orientation on export entrepreneurship through mediation of entrepreneurial capabilities in the Pakistani context. For this purpose, data was collected from 221 exporting firms of Pakistan by using questionnaire. Collected data was analyzed with the help of Smart PLS. In findings, measurement model confirmed the validity and reliability of measures of variables. Additionally, structural model provides the positive impact of entrepreneurial export orientation on export entrepreneurship. Similarly, entrepreneurial capabilities mediate the relationship between entrepreneurial export orientation on export entrepreneurship. The findings provide important implications for the managers of exporting firms to improve export performance. Keyswords: Entrepreneurial Capabilities, Entrepreneurial Export Orientation, Export Entrepreneurship Pages: 622-636 Article: 51 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)51 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)51 Download Pdf: download pdf view article Creative Commons License China Pakistan Economic Corridor: Explaining U.S-India Strategic Concerns Authors: Nasreen Akhtar Dilshad Bano Abstract: Regional and International political and economic landscape is being changed owing to China Pakistan Economic Corridor (CEPEC)-the new security paradigm has taken place-that has increased the strategic concerns of the U.S. and India. This research paper attempts to re-examine China-Pakistan relations in the new emerging geo-political compass. This paper has investigated the question that how regional, and global developments have impacted the China-Pakistan relationship? And why China – Pakistan have become partners of CPEC? In the global context, this paper assesses the emerging International Order, Indo-U. S strategic narrative vis-à-vis CPEC, and the containment of China through the new alliances and their impacts on China -Pakistan vis-à-vis the Belt Road Initiative (BRI). Quadrilateral (Quad) alliances is shaping the new strategic political and security paradigms in the world politics. Keyswords: BRI, China, CPEC, India, Pakistan, Silk Road, Strategic Concerns Pages: 637-649 Article: 52 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)52 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)52 Download Pdf: download pdf view article Creative Commons License The Structure of Domestic Politics and 1973 Constitution of Pakistan Authors: Dr. Fida Bazai Dr. Ruqia Rehman Amjad Rashid Abstract: Pakistan is located in a pivotal region. Its geo-strategic location affects its national identity as a nation state. Unlike Europe in South Asia security dilemma, proxy warfare and nuclear arms race are consistent features of the regional politics. The identity of Pakistan as security-centric state gives its army disproportional power, which created institutional imbalance that directly affected constitutionalism in the country. The constitution of Pakistan is based on principles of civilian supremacy and separation of power but in reality Pakistan’s army is the most powerful institution in country. This paper argues that the structure of Pakistani politics; created institutional imbalances by the disproportionate distribution of resources is the key variable in creating dichotomy. The structure of domestic politics is based upon the principles of hostility to India, use of Islam for national unity and strategic alliances with major powers to finance defense against the neighboring countries. Keyswords: Constitutionalism, Identity, Islam, South Asia Pages: 650-661 Article: 53 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)53 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)53 Download Pdf: download pdf view article Creative Commons License National Integration and Regionalism in Pakistan: Government’s Strategy and Response toward Regionalist Demands 1947-77 Authors: Najeeb ur Rehman Mohammad Dilshad Mohabbat Muhammad Wahid Abstract: The countries of South Asian region have pluralistic societies with different language, religious, and ethnic identities. Pakistan is no exception who is facing the challenge of regionalism since its inception. Different ethnic groups have been consistently raising their voices for separatism or autonomy within the frame work of an existing territorial state. The issues of provincialism, ethnicity, and regionalism is posing a serious challenge to the integrity of the country. This paper aims to explore the causes of the regionalism in Pakistan and intends to analyze the policies and strategies of different political governments which they launched to tackle this all important issue. The paper follows the historical method of research and analyzes different types of qualitative data to conclude the finding of the research. The paper develops the theory of “Regionalists Demand and Government Response” which shows how different regionalist forces put their demands and how the governments react on these demands. It recommends the grant of greater regional autonomy to the regionalists to enhance internal security and to protect the country from disintegration. Keyswords: Demands, Ethnicity, Government Strategy, National Integrity, Nationalism, Regionalism Pages: 662-678 Article: 54 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)54 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)54 Download Pdf: download pdf view article Creative Commons License Fostering Entrepreneurial Mindset through Entrepreneurial Education: A Qualitative Study Authors: Saira Maqbool Dr. Qaisara Parveen Dr. Muhammad Hanif Abstract: Research on entrepreneurial mindset has flourished in these recent years. Its significance lies in a critical suspicion and its matters for inventive behavior. Entrepreneurship joined with innovative abilities, seen as one of the most wanted in this day and age. This study aims to determine the perceptions about entrepreneurial mindset, its importance, and the role of entrepreneurship education and Training in developing the entrepreneurial mindset. This is a qualitative study based on interviews conducted by professors of Pakistan and Germany. The analysis was determined through content analysis. The results determine that 'Making Entrepreneurial Mindset' assists with seeing better all parts of business venture, which will undoubtedly influence their view of business venture, pioneering abilities, and mentalities. Keyswords: Entrepreneurship Education, Entrepreneurial Mindset Pages: 679-691 Article: 55 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)55 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)55 Download Pdf: download pdf view article Creative Commons License Benefits of Implementing Single National Curriculum in Special Schools of Lahore city for Children with Intellectual Disability: Teachers’ Perception Authors: Dr. Hina Fazil Khurram Rameez Sidra Ansar Abstract: Single national curriculum (SNC) is an important issue across the Punjab Province of Pakistan. Making and implementing SNC is not only focusing the education of normal pupils, but also focusing students with disabilities (SWD). The field of special education experienced an increased discussion of curriculum for students with intellectual disabilities (SID). The present research aimed to know the benefits to implement first stage of single national curriculum for students with Intellectual disability and to know the differences about the benefits between public and private schools regarding SNC for students with ID based on demographic characteristics. Likert type researchers-made questionnaire with reliability) Cronbach alpha .922) was used. 90 special educationists from public and private schools were chosen through random sampling technique. The findings raised some benefits such as: SNC will bridge the social and economic disparities which will increase the acceptance of ID students. It was recommended that SNC should include areas of adaptive skills, motor, and vocational skills to get involved in work activities. Keyswords: Benefits, Children with Intellectual Disability, Single National Curriculum Pages: 692-703 Article: 56 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)56 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)56 Download Pdf: download pdf view article Creative Commons License Last Rituals and Problems Faced by the Hindu Community in Punjab: A Case Study of Lahore Authors: Sabir Naz Abstract: Lahore is the provincial capital of Punjab, where a sizeable population of the Hindus has been residing there since the inception of Pakistan. There had been many crematoriums in the city but with the passage of time, one after another, disappeared from the land after partition of the Sub-continent. Those places were replaced by commercial or residential sites. There is also a graveyard in the city which is in the use of Hindu Valmik Sect. However, it was encroached by some Muslims due to very small size of population and indolence of the Hindus. Later on, the encroachments were removed by the District Government Lahore in compliance of order of the Supreme Court of Pakistan. Presently, there is a graveyard as well as a crematorium in the city. The community remained deprived of a place to dispose of a dead body according to their faith for a long period which is contravention with the guidelines of the Quaid-e-Azam, founder of the nation Keyswords: Crematorium, Graveyard, Hindu community, Last Rituals Pages: 704-713 Article: 57 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)57 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)57 Download Pdf: download pdf view article Creative Commons License Estimating Growth Model by Non-Nested Encompassing: A Cross Country Analysis Authors: Benish Rashid Dr. Shahid Razzaque Dr. Atiq ur Rehman Abstract: Whether models are nested or non-nested it is important to be able to compare them and evaluate their comparative results. In this study six growth models have been used for analyzing the main determinants of economic growth in case of cross countries, therefore by using these six models we have tested them for non-nested and nested encompassing through Cox test and F-test respectively. Data from 1980 to 2020 were used to analyze the cross country growth factors so therefore, the current study looked at about forty four countries with modelling these different comparative studies based on growth modelling. So, we can make these six individual models and we can estimate the General Unrestricted Model with the use of econometric technique of Non-Nested Encompassing. By evaluating the data using the Non-Nested Encompassing econometric technique, different sets of economic variables has been used to evaluate which sets of the economic variables are important to boost up the growth level of the country. And found that in case of nested model or full model it is concluded that model with lag value of GDP, trade openness, population, real export, and gross fix capital formation are the main and potential determinants to boost up the Economic Growth in most of the countries. Keyswords: Cross Country, Economic Growth, Encompassing, Nested, Non-nested Pages: 714-727 Article: 58 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)58 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)58 Download Pdf: download pdf view article Creative Commons License Assessment of Youth Buying Behaviour for Organic Food Products in Southern Punjab: Perceptions and Hindrances Authors: Ayousha Rahman Asif Yaseen Muhammad Arif Nawaz Abstract: This research examined the cognitive antecedental effects on organic food purchase behaviour for understanding the perceptions and hindrances associated with purchasing organic food products. Theory of Planned Behaviour (TPB) was adopted as a theoretical framework. A total of 250 young consumers in the two cities of Southern Punjab, Pakistan was randomly sampled and data were collected via a face-to-face survey method. Partial least square technique was employed to test the model. The results showed that attitude towards organic food purchasing motivated when moral norms were activated to consume organic food products. Further, environmental knowledge moderated the relationship of organic food purchase intentions and behaviour significantly. The findings highlighted the importance of moral norms as a meaningful antecedent that could increase the TP-based psychosocial processes if consumers have sufficient environmental knowledge. Therefore, farmers, organic products marketers, government administrators, and food retailers should take initiatives not only to highlight the norms and values but also when promoting organic food production and consumption. Keyswords: Environmental Knowledge, Organic Food Purchase Behaviour, Personal Attitude, PLS-SEM, Subjective &amp; Moral Norms Pages: 728-748 Article: 59 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)59 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)59 Download Pdf: download pdf view article Creative Commons License An Analysis on Students Ideas about English and Urdu as Medium of Instructions in the Subjects of Social Sciences studying in the Colleges of the Punjab, Pakistan Authors: Ashiq Hussain Asma Amanat Abstract: The worth and usefulness of English education as a foreign language is of great concern to language rule and planning (LRP) researchers compared to teaching their native language globally in higher education. The study under research examines the perspectives of two similar groups of the final year students of at Higher Education Institutions of Pakistan. The first group consists of art students who received the Urdu medium of instruction (UMI), and the second group received the English medium of instruction (EMI). An empirical methodology was carried out in the present year, students answered questionnaires to find out the benefits and challenges of learning subject-based knowledge, what subject-based knowledge means to them, and their understanding of language as a teaching language. Interviews were conducted with the selected group of students who wished to participate in research. Additional information is available from the tests and results obtained in the two equivalent courses. Although many similarities have been identified between the two groups, the overall knowledge of disciplinary knowledge of English medium instruction students was not very effective, while that of UMI students was very effective. It explains the implications of the findings to continue the language rule as policy experience for teaching in higher education institutions. Keyswords: English as Medium of Instruction (EMI), Higher Education Institutions (HEIs), Urdu as Medium of Instruction (UMI) Pages: 749-760 Article: 60 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)60 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)60 Download Pdf: download pdf view article Creative Commons License Environment and Women in Kurt Vonnegut’s ‘Happy Birthday Wanda Juny’: An Eco- Critical and Feminist Analysis Authors: Dr. Muhammad Asif Safana Hashmat Khan Muhammad Afzal Khan Janjua Abstract: This is an Eco-feminist study of Vonnegut’s ‘Happy Birthday Wanda Juny’ and focuses on how both women and environment are exploited by patriarchy. Ecofeminism critiques masculine dominance highlighting its role in creating and perpetuating gender discrimination, social inequity and environmental degradation. Women suffer more because of power disparity in society. Environmental crises affect women more than men because of their already precarious existence and subaltern position. There is affinity between women and nature are victims of climate change and other environmental hazards. Cheryl Glotfelty introduced interdisciplinary approach to the study of literature and environment. Literary ecology as an emerging discipline explores the intriguing relationship between environment and literature. Ecofeminism draws on feminist critique of gender inequality showing how gender categories inscribed in power structure exploit both women and nature. Francoise d‘Eaubonne coined the term ecofeminism to critique the prevalent exploitation of both women and environment. Ecofeminism asserts that exploitation of women and degradation of the environment are the direct result of male dominance and capitalism. Ecofeminism argues for redressing the plight of women and protection of environment. Vonnegut’s play ‘Happy Birthday Wanda June’ was written at a time when the movement for the right of women and protection of environment were gaining momentum. The play shows how toxic masculinity rooted in power and capitalism exploit both women and environment. Keyswords: Eco-Feminism, Eco-Criticism, Ecology, Environment, Exploitation Pages: 761-773 Article: 61 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)61 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)61 Download Pdf: download pdf view article Creative Commons License Critical Analysis of Social Equity and Economic Opportunities in the Light of Quranic Message Authors: Prof. Dr. Muhammad Yousuf Sharjeel Mahnaz Aslam Zahida Shah Abstract: This study critically evaluated the key verses of Surah Al-Baqarah -the second chapter of Quran, a sacred scripture of Islam- which specifically relates to social equity opportunities and a code of conduct in the context of economics. The Quran claims that it is a book which explains every situation; therefore, the aim of this study remained to extract those verses of Surah Al-Baqarah which can guide us in Economics. The authentic and approved Islamic clerics and their translations were consulted for the interpretations of the Holy verses. The researchers chiefly focused and studied Surah Baqarah with regards to social equity and economic opportunities. The translations were primarily in the regional language Urdu so the interpretations must not be related exactly equitable in English. The study engaged the document analysis research strategy. This study is only an endeavour to decipher Holy Quran’s message from Allah for the mankind so it must not be considered as the full and complete solution to the all the economic issues, challenges and opportunities. Ahadees and the saying of the Holy prophet were referred to where ever required and available. The researcher also considered the Tafasir (detail intellectual interpretations) of the Quran done by the well-known scholars of Islam for the verses studied therein and any statements and/or material - such as ideas, studies, articles, documentation, data, reports, facts, statistics etc. For the study, data was collected and analyzed qualitatively. On the basis of the study, recommendations were also primed. Keyswords: Economic Issues and Challenges, Social Equity, Surah Al-Baqarah, Al Quran Pages: 774-790 Article: 62 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)62 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)62 Download Pdf: download pdf view article Creative Commons License A Critical Discourse Analysis of Dastak by Mirza Adeeb Authors: Muhammad Afzal Dr. Syed Kazim Shah Umar Hayat Abstract: The present research aims to explore ideology in Pakistani drama. The drama, “Dastak”, written by Mirza Adeeb, has been taken for exploration ideologically. Fairclough’s (1992) three-dimensional model has been used for analyzing the text of the above-mentioned drama which includes textual, discursive practice and social practice analyses. The linguistic and social analyses of the drama reveal the writer’s ideology about socio-cultural, conventional and professional aspects of life. The study has also explored the past and present states of mind of Dr. Zaidi, the central and principal character of the drama, Dastak. The text implies that the writer has conveyed personal as well as social aspects of his times through the drama of Dastak. Keyswords: Dastak, Drama, Ideology, Semiotics Pages: 791-807 Article: 63 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)63 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)63 Download Pdf: download pdf view article Creative Commons License Linking Job Satisfaction to Employee Performance: The Moderating Role of Islamic Work Ethics Authors: Dr. Shakira Huma Siddiqui Dr. Hira Salah ud din Khan Dr. Nabeel Younus Ansari Abstract: The most pervasive concern in public sector organizations is declining employee performance and workforce of these organizations are less satisfied with their jobs. The aim of this study is to investigate the impact of Job Satisfaction on employee’s performance and how Islamic work ethics moderates the above mentioned direct relationship in the public sector organizations of Pakistan. The data were collected from the sample of 193 permanent employees working in public sector organizations through stratified sampling technique. The results revealed that employees Job satisfaction is significantly related to higher performance. Further, the findings indicated that Islamic work ethics moderates the relationship between job satisfaction and employee performance. The present research has some theoretical and empirical implications for academicians, policymakers, especially of public sector organizations, for the improvement of performance of their workforce. Keyswords: Employee Performance, Islamic Work Ethics, Job Satisfaction, Person-Environment Fit Theory Pages: 808-821 Article: 64 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)64 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)64 Download Pdf: download pdf view article Creative Commons License Semantics of Qawwali: Poetry, Perception, and Cultural Consumption Authors: Rao Nadeem Alam Tayyaba Khalid Abstract: Semantics is about meanings and meanings are arbitrary and shared. Understanding qawwali context requires comprehension of semantics or process of meaning creation and meaning sharing among the qawwal party and the audience. This interactive activity might frequently be hindered when interrupted by subjective meanings creation during cultural consumption. Qawwali is a cultural tradition, its semantics are conditioned by axiological premises of poetry and perceptions which are transforming. The previous researches revealed that qawwali is associated with religion which provides the religious message by singing hamd and naat. It was a means to experience Divine; therefore, semantics are multi-layered and often crossroad with values and subjective experiences. It is novel due to its ritual of Sama. It has the therapeutic power that helps mentally disturbed people and they find refuge. This study is exploratory having a small sample size of twenty purposively selected audiences. This phenomenological inquiry used ethnographic method of conversational interviews at selected shrines and cultural spaces in Islamabad. The results indicate that qawwali is a strong refuge for people facing miseries of life and they attend Sama with a belief that attending and listening will consequently resolve their issues, either psychological or physiological. They participate in Sama which teaches them how to be optimistic in a negative situation; this paper brings forth this nodal phenomenon using the verbatim explanations by the interlocutors. Semantics of Qawwali are conditioned and some of these elements are highlighted including poetry and axiology based perceptions and cultural consumption of a cultural realm. Keyswords: Cognition, Culture, Poetry, Qawwal, Qawwali, Semantics Pages: 822-834 Article: 65 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)65 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)65 Download Pdf: download pdf view article Creative Commons License Political Economy of Smuggling: The Living Source for the Natives (A Case Study of Jiwani-Iran Border, Baluchistan) Authors: Abdul Raheem Dr. Ikram Badshah Wasia Arshed Abstract: This study explores the political economy of smuggling on Jiwani-Iran border. The natives are majorly involved in illegal transportation of goods and objects, therefore; the study sets to explain how significant smuggling for the local people is. It describes the kinship role in reciprocity of their trade and transportation. The qualitative methods such as purposive sampling and interview guide were employed for data collection. The research findings revealed that local people were satisfied with their illegal trading which is depended largely on their expertise and know-how of smuggling at borders. They disclosed that their total economy was predominantly based on smuggling of stuff like drugs, diesel, oil, gas, petrol, ration food from Iran, and human trafficking. They also enjoyed the privilege of possessing Sajjil (Iranian identity card), thus; the dual nationality helped them in their daily business and rahdari (border crossing agreement), enabling them to travel to Iran for multiple purposes. Keyswords: Drugs, Human, Navigation, Political Economy, Reciprocity, Smuggling, Trafficking Pages: 835-848 Article: 66 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)66 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)66 Download Pdf: download pdf view article Creative Commons License The Vicious Circles of System: A Kafkaesque Study of Kobo Abe’s The Woman in the Dunes Authors: Imran Aslam Kainat Azhar Abstract: This paper analyses the Kafkaesque/Kafkan features of Kobo Abe’s novel The Woman in the as formulated by Kundera in “Kafka’s World.” For Kundera, in a Kafkaesque work human existence is bleakly represented through intermingling of tragedy and comedy in an indifferent world dominated by hegemonic systems. The Kafkaesque is characterised by the following: World is a huge forking labyrinthine institution where the man has been thrown to suffer its complexities, confrontation with the labyrinth makes his existence meaningless because freedom is a taboo in no man’s land, he is punished for an unknown sin for which he seeks justification from the superior authorities, but his efforts are viewed as ludicrous or comic despite the underlying sense of tragedy. (5) The Kafkaesque tendency to present tragic situation comically is also explored in Abe’s novel. The paper studies the effect of higher authorities exercising their power over man and the inscrutability of cosmic structures continuously undermining human freedom in nightmarish conditions. The paper establishes Kobo Abe in the literary world as a writer who portrays the hollowness and futility of human lives with a Kafkaesque touch. Keyswords: Authority, Institutions, Kafka, Kafkaesque, Kafkan, Kobo Abe, Kundera, The Trial, The Woman in the Dune Pages: 849-861 Article: 67 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)67 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)67 Download Pdf: download pdf view article Creative Commons License Subjectivity and Ideological Interpellation: An Investigation of Omar Shahid Hamid’s The Spinner’s Tale Authors: Hina Iqbal Dr. Muhammad Asif Asia Saeed Abstract: Louis Althusser’s concept of interpellation is a process in which individuals internalize cultural values and ideology and becomes subject. Althusser believes that ideology is a belief system of a society in which ideological agencies establish hierarchies in society through reinforcement and discrimination for cultural conditioning. These agencies function through ideological state apparatuses. These ideological agencies help to construct individual identity in society. The undesirable ideologies promote repressive political agendas. The non-repressive ideologies are inhaled by the individuals as a natural way of looking at the culture and society. This research seeks to investigate Omar Shahid Hamid’s novel The Spinners Tales through the lens of Althusser’s ideology and interpellation. This study examines how the characters of Shahid’s novel inhaled ideology and became its subjects. This research also depicts the alarming effects of cultural hegemony that creates cultural infidelity and hierarchies between the bourgeoisie and proletariat classes. Keyswords: Cultural Hegemony, Ideological State Apparatus, Ideology, Interpellation, Repressive Factors Pages: 862-872 Article: 68 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)68 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)68 Download Pdf: download pdf view article Creative Commons License Blessing in Disguise: Recommendations of Indian Education Commission (1882) and Christian Missionaries’ Educational Policy in the Colonial Punjab Authors: Mohammad Dilshad Mohabbat Muhammad Hassan Muhammad Ayaz Rafi Abstract: Woods Education Despatch is considered to be the Magna Carta of Indian Education. It controlled the Indian education field till the establishment of Indian Education Commission, 1882. The Despatch provided space to Christian missionaries by promising government’s gradual withdrawal from the education in favour of missionaries. It also facilitated the missionaries by offering system of ‘grants on aid’ to the private bodies. Consequently, the missionaries fancied to replace the government institutions in the Punjab and initiated their efforts to increase the number of their educational institutions. They tried to occupy the educational field by establishing more and more educational institutions. But after the Recommendations of the Indian Education Commission 1882, a change in their policy of numeric increase of educational institutions is quite visible. With the turn of the century, they are found to be eager to establish a few institutions with good quality of education. This paper intends to analyse different factors behind the change of their policy of quantitative dominance to qualitative improvement. It also attempts to evaluate how their change of policy worked and what steps were taken to improve the quality of their educational institutions. Following the historical method qualitative data comprising educational reports, missionaries’ autobiographies, Reports of missionaries’ conferences, and the other relevant primary and secondary sources has been collected from different repositories. The analysis of the data suggests that the attitude of the administration of the education department and the recommendations of Indian Education Commission were the major driving forces behind the change of missionaries’ educational policy in the 20th century. The missionaries, after adopting the new policy, worked on the quality of education in their institutions and became successful. Keyswords: Christian Missionaries, Indian Education Commission, Missionary Schools, Numeric Increase, Quality of Education. The Punjab, Woods Education Despatch Pages: 873-887 Article: 69 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)69 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)69 Download Pdf: download pdf view article Creative Commons License Basic Life Values of Prospective Special Education Teachers Authors: Dr. Maria Sohaib Qureshi Dr. Syeda Samina Tahira Dr. Muhammad Irfan Arif Abstract: Future teachers' preconceived values about how to live their lives and how that affects the lives of their students were the focus of this study. Descriptive research was used by the researchers. The study was carried out by using Morris's Ways to Live Scale. Researchers used this scale to study prospective special education teachers' gender, social status, personal relationships, aesthetics and mental approach using purposive sampling method. Descriptive and inferential stats were used to analyse the data collected from those who participated in the study on basic life values of prospective teachers. Results indicated that being social and sympathetic are the most important values among prospective special education teachers. It was also found that male and female prospective special education teachers living in urban and rural areas had no significant differences in their basic life values. Keyswords: Special Education, Teacher, Values Pages: 888-896 Article: 70 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)70 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)70 Download Pdf: download pdf view article Creative Commons License Perception of Dowry: Effects on Women Rights in Punjab Authors: Dr. Bushra Yasmeen Dr. Muhammad Ramzan Dr. Asma Seemi Malik Abstract: Dowry is a common tradition in south Asian countries, especially in Pakistan and India. Daughters became curses and liability for parents causing serious consequences. For control, there are legal ban/restrictions (Dowry and Wedding Gifts (Restriction) Act, 1976; Amendment in Act, 1993) on its practice in Pakistan. Despite the legal cover, the custom has been extended. Dowry amount seems to be increasing due to changing lifestyle and trends of society. To understand males’ and females’ perceptions about dowry; impacts of dowry; why dowry is essential; and how it is affecting women’s rights and eventually affecting women’s autonomy. A qualitative study was conducted. Data was collected by using unstructured interviews from males and females including social activists, economists, and married couples about wedding expenses, demands, society pressure, men’s support, and perception against dowry especially with regards to women’s rights and autonomy. The study concluded heavy dowry especially in terms of furniture, electronics, kitchenware, car, furnished houses, and cash highly associated with women’s development and their rights. General people’s perception showed that dowry is no longer remained a custom or tradition in Asian countries. It is just a trend and people follow it as a symbol of respect for parents and women as well. Keyswords: Dowry, Effects, Impacts Of Dowry, Perceptions, Women Autonomy, Women Rights Pages: 897-909 Article: 71 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)71 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)71 Download Pdf: download pdf view article Creative Commons License NCOC-An Emblem of Effective Governance: An analysis of Pakistan’s Counter Strategy for Covid-19 as a Non-Traditional Security Challenge Authors: Dr. Iram Khalid Abstract: COVID -19 affected the world unprecedentedly. Lack of capacity and poor standards of governance caused nontraditional security challenges to Pakistan too. The NCOC is the central nerve center to guide the national response to COVID-19 by Pakistan and can be best analyzed in the light of the decision-making theory of Naturalist Decision Making (NDM). The study points out the effective role performed by NCOC at policy formation through a more prosaic combination of science, data, decision making and execution of decisions at the level of federalism. The study highlights the changing patterns of government’s approach during the pandemic at various levels. Pakistan faced economic, political and social crisis during this phase. This study uses a survey and key informant interviews as the source of analysis for qualitative data collection. By applying the decision- making theory, the paper extends that there is a need to use a model to balance the existing gap within the system, to meet challenges. The study suggests a coordinating approach among various units and center; that might raise the level of performance to meet the nontraditional security challenges with innovation, creativity and boldness. Keyswords: COVID-19, Decision Making Theory, Governance, Nontraditional Threats, Strategy Pages: 910-930 Article: 72 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)72 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)72 Download Pdf: download pdf view article Creative Commons License Comparative Implications of Wednesbury Principle in England and Pakistan Authors: Safarat Ahmad Ali Shah Dr. Sara Qayum Arzoo Farhad Abstract: Wednesbury principle is one of the most important and useful grounds of the Judicial Review. Judicial review is a remedy provided by the public law and is exercised by the superior and higher courts to supervise administrative authorities' powers and functions. The main objective of the judicial review is to ensure the fair and transparent treatment of individuals by public authorities. The ground of the judicial review, i.e., Unreasonableness or irrationality or popularly known as Wednesbury Unreasonableness was introduced by lord Greene in the Wednesbury Corporation case in 1948. Initially, the scope of this ground of judicial review was very narrow and was allowed only in rare cases. However, with the development of administrative law and Human rights, it also developed. Its development resulted in different controversies and issues about the application of this ground. The main issue is about its encroachment in the jurisdiction of other branches of the government i.e., the parliament and executive. The free and loose application of this principle results in confusion and conflict between different organs of the government. The present paper is based on the implications of the limitations on the ground of Wednesbury Unreasonableness both on the judicial and administrative bodies in Pakistan to avoid the chaos and confusion that results in the criticisms on this ground of judicial review. Keyswords: Administrative Authorities, Critical Analysis, Illegality, Judicial Review, Pakistan, Wednesbury Unreasonableness Pages: 931-946 Article: 73 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)73 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)73 Download Pdf: download pdf view article Creative Commons License Water Sharing Issues in Pakistan: Impacts on Inter-Provincial Relations</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><surname>Michael S Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><surname>Brunskill</surname></persName>
		</author>
		<idno type="DOI">10.47205/jdss.2021(2-iv)74</idno>
		<idno type="arXiv">arXiv:2108.07258</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Development and Social Sciences</title>
		<title level="j" type="abbrev">JDSS</title>
		<idno type="ISSN">2709-6254</idno>
		<idno type="ISSNe">2709-6262</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">IV</biblScope>
			<date type="published" when="2021-12-31">2021</date>
			<publisher>Pakistan Social Sciences Research Institute (PSSRI)</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Food-101 – Mining Discriminative Components with Random Forests</title>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10599-4_29</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2014</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="446" to="461"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Clustering for Unsupervised Learning of Visual Features</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01264-9_9</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="139" to="156"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised Pre-Training of Image Features on Non-Curated Data</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2019.00305</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Emerging Properties in Self-Supervised Vision Transformers</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv48922.2021.00951</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-10">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</title>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01234-2_49</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="833" to="851"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Symbolic discovery of optimization algorithms</title>
		<author>
			<persName><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.06675</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploring Simple Siamese Representation Learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr46437.2021.01549</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-06">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An Empirical Study of Training Self-Supervised Vision Transformers</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv48922.2021.00950</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-10">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vision transformer adapter for dense predictions</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Masked-attention Mask Transformer for Universal Image Segmentation</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52688.2022.00135</idno>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reproducible Scaling Laws for Contrastive Language-Image Learning</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Cherti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cade</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenia</forename><surname>Jitsev</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52729.2023.00276</idno>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023-06">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Palm: Scaling language modeling with pathways</title>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Describing Textures in the Wild</title>
		<author>
			<persName><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sammy</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2014.461</idno>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014-06">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.350</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Flashattention: Fast and memoryefficient exact attention with io-awareness</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Does object recognition work for everyone?</title>
		<author>
			<persName><forename type="first">Terrance</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshops</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the Kozachenko–Leonenko entropy estimator</title>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Delattre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Fournier</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jspi.2017.01.004</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<title level="j" type="abbrev">Journal of Statistical Planning and Inference</title>
		<idno type="ISSN">0378-3758</idno>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="page" from="69" to="93"/>
			<date type="published" when="2017-06">2017</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised Visual Representation Learning by Context Prediction</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2015.167</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-12">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2015.2496141</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<title level="j" type="abbrev">IEEE Trans. Pattern Anal. Mach. Intell.</title>
		<idno type="ISSN">0162-8828</idno>
		<idno type="ISSNe">1939-3539</idno>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1734" to="1747"/>
			<date type="published" when="2016-09-01">2016</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Evaluation of GIST descriptors for web-scale image search</title>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsimrat</forename><surname>Sandhawalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Amsaleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="DOI">10.1145/1646396.1646421</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Image and Video Retrieval</title>
		<meeting>the ACM International Conference on Image and Video Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009-07-08">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Duval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.09451</idno>
		<title level="m">A simple recipe for competitive low-compute self supervised vision models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10740</idno>
		<title level="m">Are large-scale datasets necessary for self-supervised pre-training?</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The Pascal Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-009-0275-4</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<title level="j" type="abbrev">Int J Comput Vis</title>
		<idno type="ISSN">0920-5691</idno>
		<idno type="ISSNe">1573-1405</idno>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338"/>
			<date type="published" when="2010">2010</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">EVA: Exploring the Limits of Masked Visual Representation Learning at Scale</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binhui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52729.2023.01855</idno>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023-06">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories</title>
		<author>
			<persName><surname>Li Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2004.383</idno>
	</analytic>
	<monogr>
		<title level="m">2004 Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The KITTI dataset</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="DOI">10.1177/0278364913491297</idno>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<title level="j" type="abbrev">The International Journal of Robotics Research</title>
		<idno type="ISSN">0278-3649</idno>
		<idno type="ISSNe">1741-3176</idno>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237"/>
			<date type="published" when="2013-08-23">2013</date>
			<publisher>SAGE Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dynamic Few-Shot Visual Learning Without Forgetting</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00459</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">OmniMAE: Single Model Masked Pretraining on Images and Videos</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><forename type="middle">Vasudev</forename><surname>Alwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52729.2023.01003</idno>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023-06">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scaling and Benchmarking Self-Supervised Visual Representation Learning</title>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2019.00649</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Self-supervised pretraining of visual features in the wild</title>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Lefaudeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01988</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Duval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Seessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08360</idno>
		<title level="m">Vision models are more robust and fair when pretrained on uncurated images without supervision</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fairness Indicators for Systematic Assessments of Visual Feature Extractors</title>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><forename type="middle">Romero</forename><surname>Soriano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<idno type="DOI">10.1145/3531146.3533074</idno>
	</analytic>
	<monogr>
		<title level="m">2022 ACM Conference on Fairness, Accountability, and Transparency</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-06-20">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The “Something Something” Video Database for Learning and Evaluating Visual Common Sense</title>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingo</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.622</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to selfsupervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Pierre H Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Daniel Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dimensionality Reduction by Learning an Invariant Mapping</title>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2006.100</idno>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2 (CVPR'06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unsupervised semantic segmentation by distilling feature correspondences</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhoutong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Towards Measuring Fairness in AI: The Casual Conversations Dataset</title>
		<author>
			<persName><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
			<idno type="ORCID">0000-0003-1980-5768</idno>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Dolhansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacqueline</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><forename type="middle">Canton</forename><surname>Ferrer</surname></persName>
		</author>
		<idno type="DOI">10.1109/tbiom.2021.3132237</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biometrics, Behavior, and Identity Science</title>
		<title level="j" type="abbrev">IEEE Trans. Biom. Behav. Identity Sci.</title>
		<idno type="ISSNe">2637-6407</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="324" to="332"/>
			<date type="published" when="2021">2021</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Momentum Contrast for Unsupervised Visual Representation Learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00975</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Masked Autoencoders Are Scalable Vision Learners</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52688.2022.01553</idno>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PMLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv48922.2021.00823</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-10">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Natural Adversarial Examples</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr46437.2021.01501</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-06">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Figure 2: The distinction between the conventional logits-based knowledge distillation (Hinton, Vinyals &amp; Dean, 2015) and TSKD proposed in this article.</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerjcs.1650/fig-2</idno>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Deep Learning Workshop</title>
		<imprint>
			<publisher>PeerJ</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deep Networks with Stochastic Depth</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46493-0_39</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2016</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="646" to="661"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Openclip</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cade</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Product Quantization for Nearest Neighbor Search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2010.57</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<title level="j" type="abbrev">IEEE Trans. Pattern Anal. Mach. Intell.</title>
		<idno type="ISSN">0162-8828</idno>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="128"/>
			<date type="published" when="2010">2010</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Billion-Scale Similarity Search with GPUs</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
			<idno type="ORCID">0000-0003-2743-0521</idno>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<idno type="DOI">10.1109/tbdata.2019.2921572</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<title level="j" type="abbrev">IEEE Trans. Big Data</title>
		<idno type="ISSNe">2372-2096</idno>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="535" to="547"/>
			<date type="published" when="2019">2019</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning Visual Features from Large Weakly Supervised Data</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46478-7_5</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2016</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="67" to="84"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Author response image 1. Comparison of our in vivo vastus lateralis rhythmic dataset with mouse skeletal muscle rhythmic data (Dyar et al., 2014; Zhang et al., 2014).</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="DOI">10.7554/elife.34114.029</idno>
		<idno type="arXiv">arXiv:1705.06950</idno>
	</analytic>
	<monogr>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<publisher>eLife Sciences Publications, Ltd</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">3D Object Representations for Fine-Grained Categorization</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccvw.2013.77</idno>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-12">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance</title>
		<author>
			<persName><forename type="first">Mario</forename><surname>Michael Krell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matej</forename><surname>Kosec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><forename type="middle">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Figure 5: Initial cross entropy values of samples from CIFAR-100 (Krizhevsky, 2009) on SqueezeNet (Iandola et al., 2016) when learning 10 classes at a time using the same learning settings as shown in Table 1.</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerjcs.633/fig-5</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>PeerJ</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Lefaudeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Caggiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Naren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieru</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Tintore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Labatut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Haziza</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/xformers"/>
		<title level="m">xformers: A modular and hackable transformer modelling library</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Efficient self-supervised vision transformers for representation learning</title>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Binsformer: Revisiting adaptive bins for monocular depth estimation</title>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.00987</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">slimIPL: Language-Model-Free Iterative Pseudo-Labeling</title>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="DOI">10.21437/interspeech.2021-740</idno>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2021</title>
		<imprint>
			<publisher>ISCA</publisher>
			<date type="published" when="2021-08-30">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Polarized self-attention: Towards high-quality pixel-wise mapping</title>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuqiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2022.07.054</idno>
		<idno type="arXiv">arXiv:2107.00782</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<title level="j" type="abbrev">Neurocomputing</title>
		<idno type="ISSN">0925-2312</idno>
		<imprint>
			<biblScope unit="volume">506</biblScope>
			<biblScope unit="page" from="158" to="167"/>
			<date type="published" when="2021">2021</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Exploring the Limits of Weakly Supervised Pretraining</title>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01216-8_12</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="185" to="201"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Self-Supervised Learning of Pretext-Invariant Representations</title>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00674</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Automated Flower Classification over a Large Number of Classes</title>
		<author>
			<persName><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1109/icvgip.2008.47</idno>
	</analytic>
	<monogr>
		<title level="m">2008 Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008-12">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46466-4_5</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2016</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="69" to="84"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Neural Congealing: Aligning Images to a Joint Semantic Atlas</title>
		<author>
			<persName><forename type="first">Dolev</forename><surname>Ofri-Amar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoni</forename><surname>Kasten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52729.2023.01859</idno>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023-06">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Context Encoders: Feature Learning by Inpainting</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.278</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink</title>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urs</forename><surname>Hölzle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">Hung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluis-Miquel</forename><surname>Munguia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rothchild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maud</forename><surname>Texier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="DOI">10.36227/techrxiv.19139645.v2</idno>
		<idno type="arXiv">arXiv:2104.10350</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">A Self-Supervised Descriptor for Image Copy Detection</title>
		<author>
			<persName><forename type="first">Ed</forename><surname>Pizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreya</forename><forename type="middle">Dutta</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sugosh</forename><forename type="middle">Nagavara</forename><surname>Ravindra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52688.2022.01413</idno>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Revisiting Oxford and Paris: Large-Scale Image Retrieval Benchmarking</title>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00598</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples</title>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radenović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Chum</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46448-0_1</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2016</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="20"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:1704.01444</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Vision Transformers for Dense Prediction</title>
		<author>
			<persName><forename type="first">Rene</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv48922.2021.01196</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-10">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet?</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Learning With Average Precision: Training Image Retrieval With a Listwise Loss</title>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cesar</forename><forename type="middle">De</forename><surname>Souza</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2019.00521</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Configuration Selection via Self-Supervised, Performance-Weighted Generative Neural Networks</title>
		<author>
			<persName><forename type="first">Yangjun</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warren</forename><surname>Morningstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<idno type="DOI">10.2514/6.2023-0669.vid</idno>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<publisher>American Institute of Aeronautics and Astronautics (AIAA)</publisher>
			<date type="published" when="2023-01-23">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<title level="j" type="abbrev">Int J Comput Vis</title>
		<idno type="ISSN">0920-5691</idno>
		<idno type="ISSNe">1573-1405</idno>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252"/>
			<date type="published" when="2015-04-11">2015</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Spreading vectors for similarity search</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Laion-400m: Open dataset of clip-filtered 400 million image-text pairs</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kaczmarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarush</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenia</forename><surname>Jitsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Data Centric AI Workshop</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Laion-5b: An open large-scale dataset for training next generation image-text models</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cade</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Cherti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarush</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:2002.05202</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Indoor Segmentation and Support Inference from RGBD Images</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-33715-4_54</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2012</title>
		<imprint>
			<publisher>Springer Berlin Heidelberg</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="746" to="760"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Revisiting Weakly Supervised Pre-Training of Visual Perception Models</title>
		<author>
			<persName><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Adcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>De Freitas Reis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bugra</forename><surname>Gedik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52688.2022.00088</idno>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">SUN RGB-D: A RGB-D scene understanding benchmark suite</title>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298655</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-06">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">How to train your vit? data, augmentation, and regularization in vision transformers</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>TMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Energy and Policy Considerations for Deep Learning in NLP</title>
		<author>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1355</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Divide and Contrast: Self-supervised Learning from Uncurated Data</title>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><forename type="middle">J</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv48922.2021.00991</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-10">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Particular object retrieval with integral max-pooling of cnn activations</title>
		<author>
			<persName><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Sicre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training</title>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">DeiT III: Revenge of the ViT</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-20053-3_30</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="516" to="533"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Splicing ViT Features for Semantic Appearance Transfer</title>
		<author>
			<persName><forename type="first">Narek</forename><surname>Tumanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Bar-Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52688.2022.01048</idno>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">The iNaturalist Species Classification and Detection Dataset</title>
		<author>
			<persName><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00914</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Benchmarking Representation Learning for Natural World Image Collections</title>
		<author>
			<persName><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elijah</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimberly</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oisin</forename><surname>Macaodha</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr46437.2021.01269</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-06">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions</title>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52729.2023.01385</idno>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Learning Correspondence From the Cycle-Consistency of Time</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00267</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Mapillary Street-Level Sequences: A Dataset for Lifelong Place Recognition</title>
		<author>
			<persName><forename type="first">Frederik</forename><surname>Warburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soren</forename><surname>Hauberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Lopez-Antequera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pau</forename><surname>Gargallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubin</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Civera</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00270</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Learning super-features for image retrieval</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Ccnet: Extracting high quality monolingual datasets from web crawl data</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Google Landmarks Dataset v2 – A Large-Scale Benchmark for Instance-Level Recognition and Retrieval</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingyi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Sim</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00265</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Unsupervised Feature Learning via Non-parametric Instance Discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00393</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">SUN database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krista</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2010.5539970</idno>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010-06">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Galuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.06405</idno>
		<title level="m">Masked autoencoders that listen</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">Billion-scale semi-supervised learning for image classification</title>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Zeki Yalniz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kan</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><surname>Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">AmsterTime: A Visual Place Recognition Benchmark Dataset for Severe Domain Shift</title>
		<author>
			<persName><forename type="first">Burak</forename><surname>Yildiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyran</forename><surname>Khademi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">Maria</forename><surname>Siebes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Van Gemert</surname></persName>
		</author>
		<idno type="DOI">10.1109/icpr56361.2022.9956049</idno>
	</analytic>
	<monogr>
		<title level="m">2022 26th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-08-21">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">The met dataset: Instance-level recognition for artworks</title>
		<author>
			<persName><forename type="first">Nikolaos-Antonios</forename><surname>Ypsilantis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangxing</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Ibrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanne</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Scaling Vision Transformers</title>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52688.2022.01179</idno>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Colorful Image Colorization</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46487-9_40</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2016</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="649" to="666"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Places: An Image Database for Deep Scene Understanding</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<idno type="DOI">10.1167/17.10.296</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<title level="j" type="abbrev">Journal of Vision</title>
		<idno type="ISSN">1534-7362</idno>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">296</biblScope>
			<date type="published" when="2014">2014</date>
			<publisher>Association for Research in Vision and Ophthalmology (ARVO)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Scene Parsing through ADE20K Dataset</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.544</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">ibot: Image bert pre-training with online tokenizer</title>
		<author>
			<persName><forename type="first">Jinghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">Mugs: A multigranular self-supervised learning framework</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teck Khim</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.14415</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>