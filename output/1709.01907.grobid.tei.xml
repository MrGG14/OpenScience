<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.94,97.84,328.12,12.59">Deep and Confident Prediction for Time Series at Uber</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-09-06">6 Sep 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,167.21,141.14,58.23,9.54;1,225.45,138.90,1.41,6.99"><forename type="first">Lingxue</forename><surname>Zhu</surname></persName>
							<email>lzhu@cmu.edu</email>
						</author>
						<author>
							<persName coords="1,375.93,141.14,69.45,9.54"><forename type="first">Nikolay</forename><surname>Laptev</surname></persName>
							<email>nlaptev@uber.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>Pennsylvania</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Uber Technologies San Francisco</orgName>
								<address>
									<postCode>94103</postCode>
									<region>California</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.94,97.84,328.12,12.59">Deep and Confident Prediction for Time Series at Uber</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-09-06">6 Sep 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">894FE675FB722482B94D363A80F2C8E4</idno>
					<idno type="arXiv">arXiv:1709.01907v1[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-06T16:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bayesian neural networks</term>
					<term>predictive uncertainty</term>
					<term>time series</term>
					<term>anomaly detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reliable uncertainty estimation for time series prediction is critical in many fields, including physics, biology, and manufacturing. At Uber, probabilistic time series forecasting is used for robust prediction of number of trips during special events, driver incentive allocation, as well as real-time anomaly detection across millions of metrics. Classical time series models are often used in conjunction with a probabilistic formulation for uncertainty estimation. However, such models are hard to tune, scale, and add exogenous variables to. Motivated by the recent resurgence of Long Short Term Memory networks, we propose a novel end-to-end Bayesian deep model that provides time series prediction along with uncertainty estimation. We provide detailed experiments of the proposed solution on completed trips data, and successfully apply it to large-scale time series anomaly detection at Uber.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Accurate time series forecasting and reliable estimation of the prediction uncertainty are critical for anomaly detection, optimal resource allocation, budget planning, and other related tasks. This problem is challenging, especially during high variance segments (e.g., holidays, sporting events), because extreme event prediction depends on numerous external factors that can include weather, city population growth, or marketing changes (e.g., driver incentives) <ref type="bibr" coords="1,285.34,565.91,11.66,8.67" target="#b0">[1]</ref> that all contribute to the uncertainty of the forecast. These exogenous variables, however, are difficult to incorporate in many classical time series models, such as those found in the standard R forecast <ref type="bibr" coords="1,138.61,610.71,11.66,8.67" target="#b1">[2]</ref> package. In addition, these models usually require manual tuning to set model and uncertainty parameters.</p><p>Relatively recently, time series modeling based on the Long Short Term Memory (LSTM) model <ref type="bibr" coords="1,236.68,656.06,11.66,8.67" target="#b2">[3]</ref> has gained popularity due to its end-to-end modeling, ease of incorporating exogenous variables, and automatic feature extraction abilities <ref type="bibr" coords="1,90.40,689.66,10.62,8.67" target="#b3">[4]</ref>. By providing a large amount of data across numerous dimensions, it has been shown that an LSTM network can model complex nonlinear feature interactions <ref type="bibr" coords="1,543.84,251.75,10.62,8.67" target="#b4">[5]</ref>, which is critical for modeling complex extreme events. A recent paper <ref type="bibr" coords="1,366.97,274.15,11.66,8.67" target="#b5">[6]</ref> has shown that a neural network forecasting model is able to outperform classical time series methods in cases with long, interdependent time series.</p><p>However, the problem of estimating the uncertainty in time-series predictions using neural networks remains an open question. The prediction uncertainty is important for assessing how much to trust the forecast produced by the model, and has profound impact in anomaly detection. The previous model proposed in <ref type="bibr" coords="1,432.32,363.75,11.66,8.67" target="#b5">[6]</ref> had no information regarding the uncertainty. Specifically, this resulted in a large false anomaly rates during holidays where the model prediction has large variance.</p><p>In this paper, we propose a novel end-to-end model architecture for time series prediction, and quantify the prediction uncertainty using Bayesian Neural Network, which is further used for large-scale anomaly detection.</p><p>Recently, Bayesian neural networks (BNNs) have garnered increasing attention as a principled framework to provide uncertainty estimation for deep models. Under this framework, the prediction uncertainty can be decomposed into three types: model uncertainty, inherent noise, and model misspecification. Model uncertainty, also referred to as epistemic uncertainty, captures our ignorance of the model parameters, and can be reduced as more samples being collected. Inherent noise, on the other hand, captures the uncertainty in the data generation process and is irreducible. These two sources have been previously recognized with successful application in computer visions <ref type="bibr" coords="1,491.93,576.55,10.62,8.67" target="#b6">[7]</ref>.</p><p>The third uncertainty from model misspecification, however, has been long-overlooked. This captures the scenario where the testing samples come from a different population than the training set, which is often the case in time series anomaly detection. Similar ideas have gained attention in deep learning under the concept of adversarial examples in computer vision <ref type="bibr" coords="1,383.56,654.95,10.62,8.67" target="#b7">[8]</ref>, but its implication in prediction uncertainty remains unexplored. Here, we propose a principled solution to incorporate this uncertainty using an encoderdecoder framework. To the best of our knowledge, this is the first time that misspecification uncertainty has been successfully applied to prediction and anomaly detection in a principled way.</p><p>In summary, this paper makes the following contributions:</p><p>• Provides a generic and scalable uncertainty estimation implementation for deep prediction models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Quantifies the prediction uncertainty from three sources: (i) model uncertainty, (ii) inherent noise, and (iii) model misspecification. The third uncertainty has been previously overlooked, and we propose a potential solution with an encoder-decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Motivates a real-world anomaly detection use-case at Uber that uses Bayesian Neural Networks with uncertainty estimation to improve performance at scale.</p><p>The rest of this paper is organized as follows: Section 2 gives an overview of previous work on time series prediction for both classical and deep learning models, as well as the various approaches for uncertainty estimation in neural networks. The approach of Monte Carlo dropout (MC dropout) is used in this paper due to its simplicity, strong generalization ability, and scalability. In Section 3, we present our uncertainty estimation algorithm that accounts for the three different sources of uncertainty. Section 4 provides detailed experiments to evaluate the model performance on Uber trip data, and lays out a successful application to large-scale anomaly detection for millions of metrics at Uber. Finally, Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Time Series Prediction</head><p>Classical time series models, such as those found in the standard R forecast <ref type="bibr" coords="2,157.85,464.10,11.66,8.67" target="#b1">[2]</ref> package are popular methods to provide an univariate base-level forecast. These models usually require manual tuning to set seasonality and other parameters. Furthermore, while there are time series models that can incorporate exogenous variables <ref type="bibr" coords="2,232.47,508.90,10.62,8.67" target="#b8">[9]</ref>, they suffer from the curse of dimensionality and require frequent retraining. To more effectively deal with exogenous variables, a combination of univariate modeling and a machine learning model to handle residuals was introduced in <ref type="bibr" coords="2,258.45,553.70,15.33,8.67" target="#b9">[10]</ref>. The resulting two-stage model, however, is hard to tune, requires manual feature extraction and frequent retraining, which is prohibitive to millions of time series.</p><p>Relatively recently, time series modeling based on LSTM <ref type="bibr" coords="2,85.36,609.93,11.66,8.67" target="#b2">[3]</ref> technique gained popularity due to its end-toend modeling, ease of incorporating exogenous variables, and automatic feature extraction abilities <ref type="bibr" coords="2,225.21,632.33,10.62,8.67" target="#b3">[4]</ref>. By providing a large amount of data across numerous dimensions, it has been shown that an LSTM approach can model complex extreme events by allowing nonlinear feature interactions <ref type="bibr" coords="2,54.00,677.13,10.62,8.67" target="#b4">[5]</ref>, <ref type="bibr" coords="2,71.66,677.13,10.62,8.67" target="#b5">[6]</ref>.</p><p>While uncertainty estimation for classical forecasting models has been widely studied <ref type="bibr" coords="2,192.86,699.75,15.33,8.67" target="#b10">[11]</ref>, this is not the case for neural networks. Approaches such as a modified loss function or using a collection of heterogenous networks <ref type="bibr" coords="2,315.00,83.75,16.66,8.67" target="#b11">[12]</ref> were proposed, however they require changes to the underlying model architecture. A more detailed review is given in the next section.</p><p>In this work, we use a simple and scalable approach for deep model uncertainty estimation that builds on <ref type="bibr" coords="2,517.78,129.22,15.33,8.67" target="#b12">[13]</ref>. This framework provides a generic error estimator that runs in production at Uber-scale to mitigate against bad decisions (e.g., false anomaly alerts) resulting from poor forecasts due to high prediction variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Bayesian Neural Networks</head><p>Bayesian Neural Networks (BNNs) introduce uncertainty to deep learning models from a Bayesian perspective. By giving a prior to the network parameters W , the network aims to find the posterior distribution of W , instead of a point estimation.</p><p>This procedure is usually referred to as posterior inference in traditional Bayesian models. Unfortunately, due to the complicated non-linearity and non-conjugacy in deep models, exact posterior inference is rarely available; in addition, most traditional algorithms for approximate Bayesian inference cannot scale to the large number of parameters in most neural networks.</p><p>Recently, several approximate inference methods are proposed for Bayesian Neural Networks. Most approaches are based on variational inference that optimizes the variational lower bound, including stochastic search <ref type="bibr" coords="2,512.17,396.00,15.33,8.67" target="#b13">[14]</ref>, variational Bayes <ref type="bibr" coords="2,367.69,407.20,15.33,8.67" target="#b14">[15]</ref>, probabilistic backpropagation <ref type="bibr" coords="2,511.66,407.20,15.33,8.67" target="#b15">[16]</ref>, Bayes by BackProp <ref type="bibr" coords="2,370.27,418.40,16.66,8.67" target="#b16">[17]</ref> and its extension <ref type="bibr" coords="2,460.65,418.40,15.33,8.67" target="#b17">[18]</ref>. Several algorithms further extend the approximation framework to α-divergence optimization, including <ref type="bibr" coords="2,414.09,440.80,15.33,8.67" target="#b18">[19]</ref>, <ref type="bibr" coords="2,437.38,440.80,15.33,8.67" target="#b19">[20]</ref>. We refer the readers to <ref type="bibr" coords="2,315.00,452.00,16.66,8.67" target="#b20">[21]</ref> for a more detailed and complete review of these methods.</p><p>All of the aforementioned algorithms require different training methods for the neural network. Specifically, the loss function must be adjusted to different optimization problems, and the training algorithm has to be modified in a usually non-trivial sense. In practice, however, an out-ofthe-box solution is often preferred, without changing the neural network architecture and can be directly applied to the previously trained model. In addition, most existing inference algorithms introduce extra model parameters, sometimes even double, which is difficult to scale given the large amount of parameters used in practice.</p><p>This paper is inspired by the Monte Carlo dropout (MC dropout) framework proposed in <ref type="bibr" coords="2,473.17,610.15,16.66,8.67" target="#b12">[13]</ref> and <ref type="bibr" coords="2,511.02,610.15,15.33,8.67" target="#b21">[22]</ref>, which requires no change of the existing model architecture and provides uncertainty estimation almost for free. Specifically, stochastic dropouts are applied after each hidden layer, and the model output can be approximately viewed as a random sample generated from the posterior predictive distribution <ref type="bibr" coords="2,315.00,677.35,15.33,8.67" target="#b20">[21]</ref>. As a result, the model uncertainty can be estimated by the sample variance of the model predictions in a few repetitions. Details of this algorithm will be reviewed in the next section.</p><p>The MC dropout framework is particularly appealing to practitioners because it is generic, easy to implement, and directly applicable to any existing neural networks. However, the exploration of its application to real-world problems remains extremely limited. This paper takes an important step forward by successfully adapting this framework to conduct time series prediction and anomaly detection at large scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Given a trained neural network f Ŵ (•) where Ŵ represents the fitted parameters, as well as a new sample x * , our goal is to evaluate the uncertainty of the model prediction, ŷ * = f Ŵ (x * ). Specifically, we would like to quantify the prediction standard error, η, so that an approximate α-level prediction interval can be constructed by</p><formula xml:id="formula_0" coords="3,123.10,272.70,173.90,12.03">[ŷ * -z α/2 η, ŷ * + z α/2 η]<label>(1)</label></formula><p>where z α/2 is the upper α/2 quantile of a standard Normal. This prediction interval is critical for various tasks. For example, in anomaly detection, anomaly alerts will be fired when the observed value falls outside the constructed 95% interval. As a result, underestimating η will lead to high false positive rates.</p><p>In the rest of this section, we will present our uncertainty estimation algorithm in Section 3.1, which accounts for three different sources of prediction uncertainties. This framework can be generalized to any neural network architectures. Then, in Section 3.2, we will present our neural network design for predicting time series at Uber.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Prediction Uncertainty</head><p>We denote a neural network as function f W (•), where f captures the network architecture, and W is the collection of model parameters. In a Bayesian neural network, a prior is introduced for the weight parameters, and the model aims to fit the optimal posterior distribution. For example, a Gaussian prior is commonly assumed:</p><formula xml:id="formula_1" coords="3,147.76,538.95,55.49,8.74">W ∼ N (0, I)</formula><p>We further specify the data generating distribution p(y | f W (x)). In regression, we often assume</p><formula xml:id="formula_2" coords="3,127.68,585.12,95.65,10.81">y | W ∼ N (f W (x), σ 2 )</formula><p>with some noise level σ. In classification, the softmax likelihood is often used. For time series prediction, we will focus on the regression setting in this paper.</p><p>Given a set of N observations X = {x 1 , ..., x N } and Y = {y 1 , ..., y N }, Bayesian inference aims at finding the posterior distribution over model parameters</p><formula xml:id="formula_3" coords="3,243.95,661.73,53.05,8.74">p(W | X, Y ).</formula><p>With a new data point x * , the prediction distribution is obtained by marginalizing out the posterior distribution:</p><formula xml:id="formula_4" coords="3,76.30,705.06,197.02,19.35">p(y * | x * ) = W p(y * | f W (x * ))p(W | X, Y ) dW</formula><p>In particular, the variance of the prediction distribution quantifies the prediction uncertainty, which can be further decomposed using law of total variance:</p><formula xml:id="formula_5" coords="3,324.63,110.16,233.37,37.28">Var(y * | x * ) = Var [E(y * | W, x * )] + E [Var(y * | W, x * )] = Var(f W (x * )) + σ 2<label>(2)</label></formula><p>Immediately, we see that the variance is decomposed into two terms: (i) Var(f W (x * )), which reflects our ignorance over model parameter W , referred to as the model uncertainty; and (ii) σ 2 which is the noise level during data generating process, referred to as the inherent noise.</p><p>An underlying assumption for ( <ref type="formula" coords="3,458.45,212.89,3.89,8.67" target="#formula_5">2</ref>) is that y * is generated by the same procedure. However, this is not always the case in practice. In anomaly detection, in particular, it is expected that certain time series will have unusual patterns, which can be very different from the trained model. Therefore, we propose that a complete measurement of prediction uncertainty should be a combination from three sources: (i) model uncertainty, (ii) model misspecification, and (iii) inherent noise level. The following sections provide details on how we handle these three terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Model uncertainty.</head><p>The key to estimating model uncertainty is the posterior distribution p(W | X, Y ), also referred to as Bayesian inference. This is particularly challenging in neural networks because the non-conjugacy due to nonlinearities. There have been various research efforts on approximate inference in deep learning (see Section 2.2 for a review). Here, we follow the idea in <ref type="bibr" coords="3,491.78,404.06,16.66,8.67" target="#b12">[13]</ref> and <ref type="bibr" coords="3,530.00,404.06,16.66,8.67" target="#b21">[22]</ref> to approximate model uncertainty using Monte Carlo dropout (MC dropout).</p><p>The algorithm proceeds as follows: given a new input x * , we compute the neural network output with stochastic dropouts at each layer. That is, randomly dropout each hidden unit with certain probability p. This stochastic feedforward is repeated B times, and we obtain {ŷ *</p><p>(1) , ..., ŷ * (B) }. Then the model uncertainty can be approximated by the sample variance:</p><formula xml:id="formula_6" coords="3,361.66,522.87,196.34,30.63">Var(f W (x * )) = 1 B B b=1 ŷ * (b) - ŷ * 2<label>(3)</label></formula><p>where</p><formula xml:id="formula_7" coords="3,346.04,560.72,65.93,14.58">ŷ * = 1 B B b=1</formula><p>ŷ * (b) <ref type="bibr" coords="3,435.17,564.01,15.33,8.67" target="#b12">[13]</ref>. There has been recent work done on choosing the optimal dropout probability p adaptively by treating it as part of the model parameter, but this approach requires modifying the training phase <ref type="bibr" coords="3,527.46,598.18,15.33,8.67" target="#b11">[12]</ref>. In practice, we find that the uncertainty estimation is usually robust within a reasonable range of p.</p><p>3.1.2. Model misspecification. Next, we address the problem of capturing potential model misspecification. In particular, we would like to capture the uncertainty when predicting unseen samples with very different patterns from the training data set. We propose to account for this source of uncertainty by introducing an encoder-decoder to the model framework. The idea is to train an encoder that extracts the representative features from a time series, in the sense that a decoder can reconstruct the time series from the encoded space. At test time, the quality of encoding of each sample will provide insight on how close it is to the training set. Another way to think of this approach is that we first fit a latent embedding space for all training time series using an encoder-decoder framework. Then, we measure the distance between test cases and training samples in the embedded space.</p><p>The next question is how to incorporate this uncertainty in the variance calculation. Here, we take a principled approach by connecting the encoder, g(•), with a prediction network, h(•), and treat them as one large network f = h(g(•)) during inference. Figure <ref type="figure" coords="4,223.75,217.97,5.00,8.67" target="#fig_1">1</ref> illustrates such an inference network, and Algorithm 1 presents the MC dropout algorithm. Specifically, given an input time series x = {x 1 , ..., x T }, the encoder g(•) constructs the learned embedding e = g(x), which is further concatenated with external features, and the final vector is fed to the final prediction network h. During this feedforward pass, MC dropout is applied to all layers in both the encoder g and the prediction network h. As a result, the random dropout in the encoder perturbs the input intelligently in the embedding space, which accounts for potential model misspecification and gets further propagated through the prediction network. Here, variational dropout for recurrent neural networks <ref type="bibr" coords="4,280.34,352.37,16.66,8.67" target="#b21">[22]</ref> is applied to the LSTM layers in the encoder, and regular dropout <ref type="bibr" coords="4,88.61,374.77,16.66,8.67" target="#b12">[13]</ref> is applied to the prediction network. // model uncertainty and misspecification 7:</p><formula xml:id="formula_8" coords="4,54.00,535.96,126.16,49.36">η 2 1 ← 1 B B b=1 (ŷ * (b) -ŷ * ) 2 8: return ŷ * mc , η 1 3.1.3. Inherent noise.</formula><p>Finally, we estimate the inherent noise level σ 2 . In the original MC dropout algorithm <ref type="bibr" coords="4,277.84,587.92,15.33,8.67" target="#b12">[13]</ref>, this parameter is implicitly determined by a prior over the smoothness of W . As a result, the model could end up with drastically different estimations of the uncertainty level depending on this pre-specified smoothness (see <ref type="bibr" coords="4,245.08,632.72,15.33,8.67" target="#b20">[21]</ref>, chapter 4). This dependency is undesirable in anomaly detection, because we want the uncertainty estimation to also have robust frequentist coverage, but it is rarely the case that we would know the correct noise level a priori.</p><p>Here, we propose a simple and adaptive approach that estimates the noise level via the residual sum of squares, evaluated on an independent held-out validation set. Specif-ically, let f Ŵ (•) be the fitted model on training data, and X = {x 1 , ..., x V }, Y = {y 1 , ..., y V } be an independent validation set, then we estimate</p><formula xml:id="formula_9" coords="4,372.30,93.08,185.70,48.47">σ 2 via σ2 = 1 V V v=1 y v -f Ŵ (x v ) 2 .<label>(4)</label></formula><p>Note that (X , Y ) are independent from f Ŵ (•), and if we further assume that f Ŵ (x v ) is an unbiased estimation of the true model, we have</p><formula xml:id="formula_10" coords="4,341.55,192.54,216.45,47.26">E(σ 2 ) = σ 2 + 1 V V v=1 E f Ŵ (x v ) -f W (x v ) 2 = σ 2 + Var TRN (f Ŵ (x v ))<label>(5)</label></formula><p>where </p><formula xml:id="formula_11" coords="4,320.78,445.80,131.91,66.03">ŷ v ← h(g(x v )) 4: end for 5: η 2 2 ← 1 V V v=1 ŷ v -y v 2 // total prediction uncertainty 6: η ← η 2 1 + η 2<label>2</label></formula><p>7: return ŷ * , η</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model Design</head><p>The complete architecture of the neural network is shown in Figure <ref type="figure" coords="4,386.56,575.95,3.75,8.67" target="#fig_1">1</ref>. The network contains two major components: (i) an encoder-decoder framework that captures the inherent pattern in the time series, which is learned during pre-training step, and (ii) a prediction network that takes input from both the learned embedding from encoderdecoder, as well as any potential external features to guide the prediction. We discuss the two components in more details below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Encoder-decoder.</head><p>Prior to fitting the prediction model, we first conduct a pre-training step to fit an encoder that can extract useful and representative embeddings from a time series. The goals are to ensure that (i) the learned embedding provides useful features for prediction and (ii) unusual input can be captured in the embedded space, which will get further propagated to the prediction network in the next step. Here, we use an encoder-decoder framework with two-layer LSTM cells. Specifically, given a univariate time series {x t } t , the encoder reads in the first T timestamps {x 1 , ..., x T }, and constructs a fixed-dimensional embedding state. After then, from this embedding state, the decoder constructs the following F timestamps {x T +1 , ..., x T +F } with guidance from {x T -F +1 , ..., x T } (Figure <ref type="figure" coords="5,166.04,423.26,3.75,8.67" target="#fig_1">1</ref>, bottom panel). The intuition is that in order to construct the next few timestamps, the embedding state must extract representative and meaningful features from the input time series. This design is inspired from the success of video representation learning using a similar architecture <ref type="bibr" coords="5,135.98,479.26,15.33,8.67" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Prediction network.</head><p>After the encoder-decoder is pre-trained, it is treated as an intelligent feature-extraction blackbox. Specifically, the last LSTM cell states of the encoder are extracted as learned embedding. Then, a prediction network is trained to forecast the next one or more timestamps using the learned embedding as features. In the scenario where external features are available, these can be concatenated to the embedding vector and passed together to the final prediction network.</p><p>Here, we use a multi-layer perceptron as the prediction network. We will show in Section 4.1 that the learned embedding from the encoder successfully captures interesting patterns from the input time series. In addition, including external features significantly improves the prediction accuracy during holidays and special events (see Section 4) 3.2.3. Inference. After the full model is trained, the inference stage involves only the encoder and the prediction network (Figure <ref type="figure" coords="5,124.26,699.75,3.75,8.67" target="#fig_1">1</ref>, left panel). The complete inference algorithm is presented in Algorithm 2, where the prediction uncertainty, η, contains two terms: (i) the model and misspecification uncertainty, estimated by applying MC dropout to both the encoder and the prediction network, as presented in Algorithm 1; and (ii) the inherent noise level, estimated by the residuals on a held-out validation set. Finally, an approximate α-level prediction interval is constructed by [ŷ * -z α/2 η, ŷ * + z α/2 η], where z α/2 is the upper α/2 quantile of a standard Normal.</p><p>Two hyper-parameters need to be specified in Algorithm 2: the dropout probability, p, and the number of iterations, B. As for the dropout probability, we find in our experiments that the uncertainty estimation is relatively stable across a range of p, and we choose the one that achieves the best performance on the validation set. As for the number of iterations, the standard error of the estimated prediction uncertainty is proportional to 1/ √ B. We measure the standard error across different repetitions, and find that a few hundreds of iterations are usually suffice to achieve a stable estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>This section contains two sets of results. We first evaluate the model performance on a moderately sized data set of daily trips processed by the Uber platform. We will evaluate the prediction accuracy and the quality of uncertain estimation during both holidays and non-holidays. We will also present how the encoder recognizes the day of the week pattern in the embedding space. Next, we will illustrate the application of this model to real-time large-scale anomaly detection for millions of metrics at Uber.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results on Uber Trip Data</head><p>4.1.1. Experimental settings. In this section, we illustrate the model performance using the daily completed trips over four years across eight representative large cities in U.S. and Canada, including Atlanta, Boston, Chicago, Los Angeles, New York City, San Francisco, Toronto, and Washington D.C. We use three years of data as the training set, the following four months as the validation set, and the final eight months as the testing set. The encoder-decoder is constructed with two-layer LSTM cells, with 128 and 32 hidden states, respectively. The prediction network has three fully connected layers with tanh activation, with 128, 64, and 16 hidden units, respectively.</p><p>Samples are constructed using a sliding window with step size one, where each sliding window contains the previous 28 days as input, and aims to forecast the upcoming day. The raw data are log-transformed to alleviate exponential effects. Next, within each sliding window, the first day is subtracted from all values, so that trends are removed and the neural network is trained for the incremental value. At test time, it is straightforward to revert these transformations to obtain predictions at the original scale.</p><p>4.1.2. Prediction performance. We compare the prediction accuracy among four different models: 1) Last-Day: A naive model that uses the last day's completed trips as the prediction for the next day. 2) QRF: Based on the naive last-day prediction, a quantile random forest (QRF) is further trained to estimate the holiday lifts, i.e., the ratio to adjust the forecast during holidays. The final prediction is calculated from the last-day forecast multiplied by the estimated ratio. 3) LSTM: A vanilla LSTM model with similar size as our model. Specifically, a two-layer sacked LSTM is constructed, with 128 and 32 hidden states, respectively, followed by a fully connected layer for the final output. This neural network also takes 28 days as input, and predicts the next day. 4) Our Model: Our model that combines an encoderdecoder and a prediction network, as described in Figure <ref type="figure" coords="6,118.94,418.15,3.75,8.67" target="#fig_1">1</ref>.</p><p>Table <ref type="table" coords="6,94.53,433.22,5.00,8.67" target="#tab_1">1</ref> reports the Symmetric Mean Absolute Percentage Error (SMAPE) of the four models, evaluated on the testing set. We see that using a QRF to adjust for holiday lifts is only slightly better than the naive prediction. On the other hand, a vanilla LSTM neural network provides an average of 26% improvement across the eight cities. As we further incorporate the encoder-decoder framework and introduce external features for holidays to the prediction network (Figure <ref type="figure" coords="6,124.98,522.82,3.61,8.67" target="#fig_1">1</ref>), our proposed model achieves another 36% improvement in prediction accuracy. Note that when using LSTM and our model, only one generic model is trained, where the neural network is not tuned for any cityspecific patterns; nevertheless, we still observe significant improvement on SMAPE across all cities when compared to traditional approaches.</p><p>Finally, Figure <ref type="figure" coords="6,136.91,600.94,5.00,8.67" target="#fig_2">2</ref> visualizes the true values and our predictions during the testing period in San Francisco as an example. We observe that accurate predictions are achieved not only in regular days, but also during holiday seasons. 4.1.3. Uncertainty estimation. Next, we evaluate the quality of our uncertainty estimation by calibrating the empirical coverage of the prediction intervals. Here, the dropout probability is set to be 5% at each layer, and Table <ref type="table" coords="6,245.84,688.55,5.00,8.67" target="#tab_2">2</ref> reports the empirical coverage of the 95% predictive intervals under three different scenarios:  By comparing PredNet with Enc+Pred, it is clear that introducing MC dropout to the encoder network is critical, which significantly improves the empirical coverage from 78% to 90% by capturing potential model misspecification. In addition, by further accounting for the inherent noise level, the empirical coverage of the final uncertainty estimation, Enc+Pred+Noise, nicely centers around 95% as desired.</p><p>One important use-case of the uncertainty estimation is to provide insight for unusual patterns in the time series. Figure <ref type="figure" coords="6,344.90,620.75,5.00,8.67" target="#fig_3">3</ref> shows the estimated predictive uncertainty on six U.S. holidays in the testing data. We see that New Year's Eve has significantly higher uncertainty than all other holidays. This pattern is consistent with our previous experience, where New Year's Eve is usually the most difficult day to predict.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">Embedding features.</head><p>As illustrated previously, the encoder is critical for both improving prediction accuracy,  as well as for estimating prediction uncertainty. One natural follow-up question is whether we can interpret the embedding features extracted by the encoder. This can also provide valuable insights for model selection and anomaly detection. Here, we visualize our training data, each being a 28-day time series segment, in the embedding space. We use the last LSTM cell in the encoder, and project its cell states to 2D for visualization using PCA (Figure <ref type="figure" coords="7,225.77,536.76,3.61,8.67" target="#fig_4">4</ref>). The strongest pattern we observe is day of the week, where weekdays and weekends form different clusters, with Fridays usually sitting in between. We do not observe city-level clusters, which is probably due to the fact all cities in this data set are large cities in North America, where riders and drivers tend to have similar behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Application to Anomaly Detection at Uber</head><p>At Uber, we track millions of metrics each day to monitor the status of various services across the company. One important application of uncertainty estimation is to provide real-time anomaly detection and deploy alerts for potential outages and unusual behaviors. A natural approach is to trigger an alarm when the observed value falls outside of the 95% predictive interval. There are two main challenges we need to address in this application:</p><p>• Scalability: In order to provide real-time anomaly detection at the current scale, each predictive interval must be calculated within a few milliseconds during inference stage.</p><p>• Performance: With highly imbalanced data, we aim to reduce the false positive rate as much as possible to avoid unnecessary on-call duties, while making sure the false negative rate is properly controlled so that real outages will be captured. 4.2.1. Scalability. Our model inference is implemented in Go. Our implementation involves efficient matrix manipulation operations, as well as stochastic dropout by randomly setting hidden units to zero with pre-specified probability. A few hundred stochastic passes are executed to calculate the prediction uncertainty, which is updated every few minutes for each metric. We find that the uncertainty estimation step adds only a small amount of computation overhead and can be conducted within ten milliseconds per metric. 4.2.2. Performance. Here, we illustrate the precision and recall of this framework on an example data set containing 100 metrics with manual annotation available, where 17 of them are true anomalies. Note that the neural network was previously trained on a separate and much larger data set. By adding MC dropout layers in the neural network, the estimated predictive intervals achieved 100% recall rate and a 80.95% precision rate. Figure <ref type="figure" coords="7,465.89,398.29,5.00,8.67" target="#fig_5">5</ref> visualizes the neural network predictive intervals on four representative metrics, where alerts are correctly fired for two of them. When applying this framework to all metrics, we observe a 4% improvement in precision compared to the previous ad-hoc solution, which is substantial at Uber's scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented an end-to-end neural network architecture for uncertainty estimation used at Uber. Using the MC dropout technique and model misspecification distribution, we showed a simple way to provide uncertainty estimation for a neural network forecast at scale while providing a 95% uncertainty coverage. A critical feature about our framework is its applicability to any neural network without modifying the underlying architecture.</p><p>We have used the proposed uncertainty estimate to measure special event (e.g., holiday) uncertainty and to improve anomaly detection accuracy. For special event uncertainty estimation, we found New Year's Eve to be the most uncertain time. Using the uncertainty information, we adjusted the confidence bands of an internal anomaly detection model to improve precision during high uncertainty events, resulting in a 4% accuracy improvement, which is large given the number of metrics we track at Uber.</p><p>Our future work will be focused on utilizing the uncertainty information for neural network debugging during high error periods. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,123.94,392.41,103.12,8.67;4,54.00,401.66,61.18,9.06;4,115.18,400.18,4.08,6.12;4,119.76,401.75,177.24,8.97;4,71.00,412.95,187.17,8.97;4,54.00,424.06,92.82,9.06;4,142.38,428.66,10.63,6.12;4,153.51,424.15,63.41,9.65;4,59.78,438.25,86.68,9.06;4,59.78,451.14,6.22,6.94;4,81.00,449.54,4.64,8.74;4,85.64,447.97,4.08,6.12;4,85.64,454.80,9.73,6.12;4,98.64,449.54,110.46,8.74;4,209.10,447.97,4.08,6.12;4,213.68,449.54,17.19,8.74;4,59.78,463.57,6.22,6.94;4,81.00,461.97,4.63,8.74;4,86.07,460.40,4.08,6.12;4,85.63,467.23,9.73,6.12;4,98.63,461.97,70.67,8.97;4,169.30,460.40,4.08,6.12;4,169.30,467.23,9.73,6.12;4,179.53,461.97,54.25,8.97;4,59.78,476.00,6.22,6.94;4,81.68,474.40,8.64,8.74;4,85.88,479.66,9.73,6.12;4,98.88,474.40,68.52,8.74;4,167.84,472.83,4.08,6.12;4,167.41,479.66,9.73,6.12;4,177.63,474.40,17.19,8.74;4,59.78,486.16,42.80,8.99;4,71.00,500.56,49.80,8.61;4,59.78,512.30,20.55,8.74;4,75.89,516.80,10.63,6.12;4,89.78,512.30,9.96,8.74;4,104.90,510.42,3.97,6.12;4,103.71,517.77,6.02,6.12;4,123.48,509.31,6.02,6.12;4,123.48,517.35,13.59,6.12;4,139.91,512.30,8.64,8.74;4,144.11,517.55,9.73,6.12"><head>Algorithm 1 : 1 B</head><label>11</label><figDesc>MCdropout Input: data x * , encoder g(•), prediction network h(•), dropout probability p, number of iterations B Output: prediction ŷ * mc , uncertainty η 1 1: for b = 1 to B do 2: e * (b) ← VariationalDropout(g(x * ), p) 3: z * (b) ← Concatenate(e * (b) , extFeatures) 4: ŷ * (b) ← Dropout (h(z * (b) ), p) 5: end for // prediction 6: ŷ * mc ←</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,54.00,266.76,243.00,6.94;5,54.00,275.76,243.00,6.94;5,54.00,284.76,194.62,6.94;5,54.00,71.22,252.00,183.95"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Neural network architecture, with a pre-training phase using a LSTM encoder-decoder, followed by a prediction network, with input being the learned embedding concatenated with external features.</figDesc><graphic coords="5,54.00,71.22,252.00,183.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,315.00,183.17,243.00,6.94;6,315.00,192.17,243.00,6.94;6,315.00,201.17,243.00,6.94;6,315.00,210.17,178.15,6.94;6,315.00,71.22,252.00,100.36"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Daily completed trips in San Francisco during eight months of the testing set. True values are shown with the orange solid line, and predictions are shown with the blue dashed line, where the 95% prediction band is shown as the grey area. Exact values are anonymized.</figDesc><graphic coords="6,315.00,71.22,252.00,100.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,54.00,227.96,243.00,6.94;7,54.00,236.96,221.29,6.94;7,54.54,71.22,241.92,145.15"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Estimated prediction standard deviations on six U.S. holidays during testing period for eight cities. Exact values are anonymized.</figDesc><graphic coords="7,54.54,71.22,241.92,145.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,54.00,383.19,243.00,6.94;7,54.00,392.19,243.00,6.94;7,54.00,401.19,243.00,6.94;7,54.00,410.19,243.00,6.94;7,54.00,419.19,243.00,6.94;7,54.00,428.19,75.58,6.94;7,54.00,257.96,252.00,113.65"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Training set of time series, visualized in the embedding space.Each point represents a 28-day segment, colored by the day of the week of the last day. We evaluate the cell states of the two LSTM layers, where the first layer with dimension 128 is plotted on the left, and second layer with dimension 32 is plotted on the right. PCA is used to project into 2D space for visualization.</figDesc><graphic coords="7,54.00,257.96,252.00,113.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,54.00,274.15,243.00,6.94;8,54.00,283.15,243.00,6.94;8,54.00,292.15,243.00,6.94;8,54.00,300.84,243.00,7.25;8,54.00,310.15,243.00,6.94;8,54.00,318.84,243.00,7.25;8,54.00,328.15,243.00,6.94;8,54.00,336.84,243.00,7.25;8,54.00,345.84,243.00,7.25;8,54.00,355.15,117.02,6.94;8,54.00,179.72,125.99,75.60"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Four example metrics during a 12-hour span, and anomaly detection is performed for the following 30 minutes. All metrics are evaluated by minutes. The neural network constructs predictive intervals for the following 30 minutes, visualized by the shaded area in each plot. (a) A normal metric with large fluctuation, where the observation falls within the predictive interval. (b) A normal metric with small fluctuation, and an unusual inflation has just ended. The predictive interval still captures the observation. (c) An anomalous metric with a single spike that falls outside the predictive interval. (d) An anomalous metric with two consecutive spikes, also captured by our model.</figDesc><graphic coords="8,54.00,179.72,125.99,75.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,315.00,247.05,243.00,209.75"><head></head><label></label><figDesc>Var TRN is w.r.t the training data, which decreases as the training sample size increases, and → 0 as the training sample size N → ∞. Therefore, σ2 provides an asymptotically unbiased estimation on the inherent noise level. In the finite sample scenario, it always overestimates the noise level and tends to be more conservative.</figDesc><table coords="4,315.00,314.39,243.00,57.53"><row><cell cols="2">The final inference algorithm combines inherent noise</cell></row><row><cell cols="2">estimation with MC dropout, and is presented in Algo-</cell></row><row><cell>rithm 2.</cell><cell></cell></row><row><cell>Input: data x</cell><cell>Algorithm 2: Inference</cell></row></table><note coords="4,377.54,361.38,2.72,6.12;4,380.76,362.95,177.24,8.97;4,332.00,374.15,187.17,8.97;4,315.00,385.26,195.85,9.06;4,332.00,399.67,215.73,8.61;4,320.78,410.74,112.07,9.65;4,432.84,409.17,4.08,6.12;4,437.42,410.74,45.50,8.74;4,332.00,425.06,67.18,8.61;4,320.78,436.04,32.94,8.99;4,353.71,440.64,3.98,6.12;4,361.95,436.04,80.37,9.06;4,442.32,440.65,3.97,6.12;4,446.79,436.13,22.85,8.74;4,469.64,440.92,4.70,6.12;4,476.56,436.04,19.04,8.99;4,320.78,449.86,6.22,6.94"><p>* , encoder g(•), prediction network h(•), dropout probability p, number of iterations B Output: prediction ŷ * , predictive uncertainty η // prediction, model uncertainty and misspecification 1: ŷ * , η 1 ← MCdropout (x * , g, h, p, B) // Inherent noise 2: for x v in validation set {x 1 , ..., x V } do 3:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,66.50,73.85,217.81,143.55"><head>TABLE 1 .</head><label>1</label><figDesc>SMAPE OF FOUR DIFFERENT PREDICTION MODELS, EVALUATED ON THE TEST DATA.</figDesc><table coords="6,67.77,101.33,215.46,116.08"><row><cell>City</cell><cell>Last-Day</cell><cell>QRF</cell><cell>LSTM</cell><cell>Our Model</cell></row><row><cell>Atlanta</cell><cell>15.9</cell><cell>13.2</cell><cell>11.0</cell><cell>7.3</cell></row><row><cell>Boston</cell><cell>13.6</cell><cell>15.4</cell><cell>10.0</cell><cell>8.2</cell></row><row><cell>Chicago</cell><cell>16.0</cell><cell>12.7</cell><cell>9.5</cell><cell>6.1</cell></row><row><cell>Los Angeles</cell><cell>12.3</cell><cell>10.9</cell><cell>8.5</cell><cell>4.7</cell></row><row><cell>New York City</cell><cell>11.5</cell><cell>10.9</cell><cell>8.7</cell><cell>6.1</cell></row><row><cell>San Francisco</cell><cell>10.7</cell><cell>11.8</cell><cell>7.3</cell><cell>4.5</cell></row><row><cell>Toronto</cell><cell>15.2</cell><cell>11.7</cell><cell>10.0</cell><cell>5.3</cell></row><row><cell>Washington D.C.</cell><cell>13.0</cell><cell>13.3</cell><cell>8.2</cell><cell>5.2</cell></row><row><cell>Average</cell><cell>13.5</cell><cell>12.5</cell><cell>9.2</cell><cell>5.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,320.59,232.99,237.42,267.16"><head>TABLE 2 .</head><label>2</label><figDesc>EMPIRICAL COVERAGE OF 95% PREDICTIVE INTERVALS, EVALUATED ON THE TEST DATA. Use MC dropout in both the encoder and the prediction network, but without the inherent noise level. This is the term η 1 in Algorithm 2.3) Enc+Pred+Noise: Use the full prediction uncertainty η as presented in Algorithm 2, including η 1 as in 2), as well as the inherent noise level η 2 .</figDesc><table coords="6,330.00,260.46,228.00,183.00"><row><cell>City</cell><cell>PredNet</cell><cell>Enc+Pred</cell><cell>Enc+Pred+Noise</cell></row><row><cell>Atlanta</cell><cell>78.33%</cell><cell>91.25%</cell><cell>94.30%</cell></row><row><cell>Boston</cell><cell>85.93%</cell><cell>95.82%</cell><cell>99.24%</cell></row><row><cell>Chicago</cell><cell>71.86%</cell><cell>80.23%</cell><cell>90.49%</cell></row><row><cell>Los Angeles</cell><cell>76.43%</cell><cell>92.40%</cell><cell>94.30%</cell></row><row><cell>New York City</cell><cell>76.43%</cell><cell>85.55%</cell><cell>95.44%</cell></row><row><cell>San Francisco</cell><cell>78.33%</cell><cell>95.06%</cell><cell>96.20%</cell></row><row><cell>Toronto</cell><cell>80.23%</cell><cell>90.87%</cell><cell>94.68%</cell></row><row><cell>Washington D.C.</cell><cell>78.33%</cell><cell>93.54%</cell><cell>96.96%</cell></row><row><cell>Average</cell><cell>78.23%</cell><cell>90.59%</cell><cell>95.20%</cell></row><row><cell cols="4">1) PredNet: Use only model uncertainty estimated</cell></row><row><cell cols="4">from MC dropout in the prediction network, with</cell></row><row><cell cols="4">no dropout layers in the encoder.</cell></row><row><cell>2) Enc+Pred:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,72.33,402.99,224.67,6.94;8,72.33,411.85,224.67,7.07;8,72.33,420.99,50.80,6.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,198.00,402.99,92.08,6.94">Accounting for Mega-Events</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Horne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfram</forename><surname>Manzenreiter</surname></persName>
		</author>
		<idno type="DOI">10.1177/1012690204043462</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,72.33,411.85,157.30,6.89">International Review for the Sociology of Sport</title>
		<title level="j" type="abbrev">International Review for the Sociology of Sport</title>
		<idno type="ISSN">1012-6902</idno>
		<idno type="ISSNe">1461-7218</idno>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="203" />
			<date type="published" when="2004-06">2004</date>
			<publisher>SAGE Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.33,433.01,224.67,6.94;8,72.33,441.88,224.68,7.07;8,72.33,451.01,73.20,6.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,187.54,433.01,109.46,6.94;8,72.33,442.01,85.51,6.94">Automatic time series forecasting: the forecast package for R</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Hyndman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Khandakar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,167.58,441.88,98.29,6.89">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.33,462.90,224.67,7.07;8,72.33,471.90,161.66,7.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,188.08,463.04,77.03,6.94">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,274.90,462.90,22.10,6.89;8,72.33,471.90,38.50,6.89">Neural Computation</title>
		<title level="j" type="abbrev">Neural Computation</title>
		<idno type="ISSN">0899-7667</idno>
		<idno type="ISSNe">1530-888X</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11-01">1997</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.33,484.06,224.67,6.94;8,72.33,493.06,224.67,6.94;8,72.33,501.93,168.91,7.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,208.66,484.06,88.34,6.94;8,72.33,493.06,217.82,6.94">A new boosting algorithm for improved time-series forecasting with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Assaad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romuald</forename><surname>Boné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Cardot</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.inffus.2006.10.009</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,72.33,501.93,61.79,6.89">Information Fusion</title>
		<title level="j" type="abbrev">Information Fusion</title>
		<idno type="ISSN">1566-2535</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="55" />
			<date type="published" when="2008-01">2008</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.33,514.09,224.67,6.94;8,72.33,522.96,224.67,7.07;8,72.33,531.96,109.97,7.07" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,265.01,514.09,31.99,6.94;8,72.33,523.09,196.46,6.94">Nonlinear systems identification using deep dynamic neural networks</title>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">P</forename><surname>Ogunmolu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">R</forename><surname>Gans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01439</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,72.33,544.12,224.67,6.94;8,72.33,552.98,224.67,7.07;8,72.33,561.98,93.61,7.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,212.70,544.12,84.30,6.94;8,72.33,553.12,133.23,6.94">Time-series extreme event forecasting with neural networks at uber</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Smyl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,215.02,552.98,81.98,6.89;8,72.33,561.98,69.33,6.89">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.33,574.14,224.67,6.94;8,72.33,583.01,224.67,7.07;8,72.33,592.14,18.00,6.94" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="8,155.58,574.14,141.42,6.94;8,72.33,583.14,111.63,6.94">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04977</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,72.33,604.17,224.67,6.94;8,72.33,613.03,224.67,7.07;8,72.33,622.03,74.06,7.07" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,230.24,604.17,66.76,6.94;8,72.33,613.17,91.87,6.94">Explaining and harnessing adversarial examples</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,173.62,613.03,123.38,6.89;8,72.33,622.03,49.93,6.89">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.33,634.06,224.68,7.07;8,72.33,643.20,18.00,6.94" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="8,121.38,634.06,64.39,6.89">Time series analysis</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-S</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Addison-Wesley publ Reading</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.33,655.22,224.67,6.94;8,72.33,664.09,224.67,7.07;8,72.33,673.22,30.80,6.94" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,111.61,655.22,185.39,6.94;8,72.33,664.22,104.22,6.94">Modeling asymptotically independent spatial extremes based on laplace random fields</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Opitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,185.67,664.09,54.30,6.89">Spatial Statistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.33,685.25,224.67,6.94;8,72.33,694.25,224.67,6.94;8,72.33,703.11,224.67,7.07;8,72.33,712.25,61.38,6.94" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,117.17,694.25,179.83,6.94;8,72.33,703.25,127.35,6.94">Modeling the impact of run-time uncertainty on optimal computation scheduling using feedback</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">D</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Casavant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">E</forename><surname>Scheetz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Andersland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,218.18,703.11,75.66,6.89">Parallel Processing &apos;97</title>
		<imprint>
			<date>Aug</date>
			<biblScope unit="page" from="481" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,333.33,73.71,224.67,7.07;8,333.33,82.71,81.24,7.07" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="8,447.08,73.85,54.90,6.94">Concrete dropout</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07832</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,333.33,96.35,224.67,6.94;8,333.33,105.21,224.67,7.07;8,333.33,114.21,224.68,7.07;8,333.33,123.35,38.00,6.94" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,431.54,96.35,126.46,6.94;8,333.33,105.35,158.16,6.94">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,509.41,105.21,48.58,6.89;8,333.33,114.21,186.67,6.89">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,333.33,136.85,224.67,6.94;8,333.33,145.71,224.67,7.07;8,333.33,154.71,186.59,7.07" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,457.77,136.85,100.23,6.94;8,333.33,145.85,73.64,6.94">Variational bayesian inference with stochastic search</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,428.34,145.71,129.66,6.89;8,333.33,154.71,108.71,6.89">Proceedings of the 29th International Conference on Machine Learning</title>
		<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1367" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,333.33,168.35,224.67,6.94;8,333.33,177.21,216.20,7.07" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,446.10,168.35,105.27,6.94">Auto-encoding variational bayes</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,333.33,177.21,192.07,6.89">The International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,333.33,190.85,224.67,6.94;8,333.33,199.71,224.67,7.07;8,333.33,208.71,224.67,6.89;8,333.33,217.85,71.60,6.94" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,477.31,190.85,80.69,6.94;8,333.33,199.85,189.74,6.94">Probabilistic backpropagation for scalable learning of bayesian neural networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,543.70,199.71,14.30,6.89;8,333.33,208.71,221.19,6.89">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1861" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,333.33,231.35,224.67,6.94;8,333.33,240.21,224.67,7.07;8,333.33,249.21,208.06,7.07" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,535.32,231.35,22.68,6.94;8,333.33,240.35,97.65,6.94">Weight uncertainty in neural networks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,449.42,240.21,108.58,6.89;8,333.33,249.21,130.18,6.89">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1613" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,333.33,262.85,224.67,6.94;8,333.33,271.71,167.58,7.07" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="8,475.84,262.85,82.16,6.94;8,333.33,271.85,27.39,6.94">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:1704.02798</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,333.33,285.35,224.67,6.94;8,333.33,294.11,224.67,7.17;8,333.33,303.21,224.67,7.07;8,333.33,312.21,105.74,7.07" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="8,427.24,294.11,123.95,7.17">Black-box α-divergence minimization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,343.72,303.21,214.28,6.89;8,333.33,312.21,27.85,6.89">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1511" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,333.33,325.85,224.67,6.94;8,333.33,334.71,213.09,7.07" xml:id="b19">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:1703.02914</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,333.33,348.35,224.67,6.94;8,333.33,357.35,104.07,6.94" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="8,361.28,348.35,90.49,6.94">Uncertainty in deep learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation, PhD thesis</note>
</biblStruct>

<biblStruct coords="8,333.33,370.85,224.67,6.94;8,333.33,379.71,224.67,7.07;8,333.33,388.71,180.74,7.07" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="8,435.92,370.85,122.08,6.94;8,333.33,379.85,135.46,6.94">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,490.44,379.71,67.56,6.89;8,333.33,388.71,102.93,6.89">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,333.33,402.35,224.67,6.94;8,333.33,411.21,224.67,7.07;8,333.33,420.21,224.68,7.07;8,333.33,429.35,14.00,6.94" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="8,514.11,402.35,43.89,6.94;8,333.33,411.35,145.67,6.94">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,496.92,411.21,61.08,6.89;8,333.33,420.21,170.19,6.89">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
