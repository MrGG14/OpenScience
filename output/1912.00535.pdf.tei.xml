<?xml version="1.0" encoding="UTF-8"?><TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning for Visual Tracking: A Comprehensive Survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2013">2013 2014 2015 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Seyed</forename><surname>Mojtaba Marvasti-Zadeh</surname></persName>
							<email>mojtaba.marvasti@ualberta.ca</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Li</forename><surname>Cheng</surname></persName>
							<email>lcheng5@ualberta.ca</email>
						</author>
						<author>
							<persName><forename type="first">Hossein</forename><surname>Ghanei-Yakhdan</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Shohreh</forename><surname>Kasaei</surname></persName>
							<email>kasaei@sharif.edu</email>
						</author>
						<author>
							<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Marvasti-Zadeh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">H</forename><surname>Ghanei</surname></persName>
							<email>hghaneiy@yazd.ac.ir</email>
						</author>
						<author>
							<persName><roleName>DeepSRDCF, FCNT, CNN-SVM</roleName><surname>Hcft</surname></persName>
						</author>
						<author>
							<persName><roleName>RFL, MAM GAN VITAL, TGGAN, ADT, CRAC</roleName><forename type="first">Rnn</forename><surname>Fprnet</surname></persName>
						</author>
						<author>
							<persName><roleName>GOTURN, SiamFC, SINT, MDNet, PTAV, UCT</roleName><surname>Dpst</surname></persName>
						</author>
						<author>
							<persName><roleName>SiamFC, SINT, R-FCSN, LST, CFNet, DaSiamRPN, StructSiam</roleName><surname>Goturn</surname></persName>
						</author>
						<title level="a" type="main">Deep Learning for Visual Tracking: A Comprehensive Survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2013">2013 2014 2015 2016</date>
						</imprint>
					</monogr>
					<idno type="MD5">BC953016DDADF91310B7EB613107F120</idno>
					<idno type="arXiv">arXiv:1912.00535v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-05-21T16:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual tracking</term>
					<term>deep learning</term>
					<term>computer vision</term>
					<term>appearance modeling -SVM</term>
					<term>DPST</term>
					<term>CCOT</term>
					<term>MDNet</term>
					<term>HDT</term>
					<term>STCT</term>
					<term>RPNT</term>
					<term>CNT</term>
					<term>RDLT</term>
					<term>CREST</term>
					<term>UCT</term>
					<term>DiMP</term>
					<term>PrDiMP</term>
					<term>BGBDT</term>
					<term>MLT</term>
					<term>ROAM</term>
					<term>MetaUpdater</term>
					<term>TMAML Network objective</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual target tracking is one of the most soughtafter yet challenging research topics in computer vision. Given the ill-posed nature of the problem and its popularity in a broad range of real-world scenarios, a number of large-scale benchmark datasets have been established, on which considerable methods have been developed and demonstrated with significant progress in recent years -predominantly by recent deep learning (DL)based methods. This survey aims to systematically investigate the current DL-based visual tracking methods, benchmark datasets, and evaluation metrics. It also extensively evaluates and analyzes the leading visual tracking methods. First, the fundamental characteristics, primary motivations, and contributions of DLbased methods are summarized from nine key aspects of: network architecture, network exploitation, network training for visual tracking, network objective, network output, exploitation of correlation filter advantages, aerial-view tracking, long-term tracking, and online tracking. Second, popular visual tracking benchmarks and their respective properties are compared, and their evaluation metrics are summarized. Third, the state-ofthe-art DL-based methods are comprehensively examined on a set of well-established benchmarks of OTB2013, OTB2015, VOT2018, LaSOT, UAV123, UAVDT, and VisDrone2019. Finally, by conducting critical analyses of these state-of-the-art trackers quantitatively and qualitatively, their pros and cons under various common scenarios are investigated. It may serve as a gentle use guide for practitioners to weigh when and under what conditions to choose which method(s). It also facilitates a discussion on ongoing issues and sheds light on promising research directions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>G ENERIC visual tracking aims to estimate an unknown visual target trajectory when only an initial state of the target (in a video frame) is available. Visual tracking is an open and attractive research field with a broad extent of categories (see Fig. <ref type="figure" target="#fig_0">1</ref>) and applications, including self-driving cars <ref type="bibr" target="#b0">[1]</ref>, autonomous robots <ref type="bibr" target="#b1">[2]</ref>, surveillance <ref type="bibr" target="#b2">[3]</ref>, augmented reality <ref type="bibr" target="#b3">[4]</ref>, aerial-view tracking <ref type="bibr" target="#b4">[5]</ref>, sports <ref type="bibr" target="#b5">[6]</ref>, surgery <ref type="bibr" target="#b6">[7]</ref>, biology <ref type="bibr" target="#b7">[8]</ref>, ocean exploration <ref type="bibr" target="#b8">[9]</ref>, to name a few. The ill-posed definition of the visual tracking (i.e., model-free tracking, on-the-fly learning, single-camera, 2D information) is more challenging in complicated real-world scenarios which may include arbitrary classes of targets (e.g., human, drone, animal, vehicle) and motion models, various imaging characteristics (e.g., static/moving camera, smooth/fast movement, camera resolution), and changes in environmental conditions (e.g., illumination variation, background clutter, crowded scenes). Traditional methods employ various visual tracking frameworks, such as discriminative correlation filters (DCF) <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b15">[16]</ref>, silhouette tracking <ref type="bibr" target="#b16">[17]</ref>, Kernel tracking <ref type="bibr" target="#b17">[18]</ref>, point tracking <ref type="bibr" target="#b18">[19]</ref> -for appearance &amp; motion modeling of a target. In general, traditional trackers have inflexible assumptions about target structures &amp; their motion in real-world scenarios. These trackers exploit handcrafted features (e.g., the histogram of oriented gradients (HOG) <ref type="bibr" target="#b19">[20]</ref> and Color-Names (CN) <ref type="bibr" target="#b20">[21]</ref>), so they cannot interpret semantic target information and handle significant appearance changes. However, some tracking-by-detection methods (e.g., DCF-based trackers) provide an appealing trade-off of competitive tracking performance and efficient computations <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>. For instance, aerial-view trackers <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref> extensively use these CPUbased algorithms considering limited on-board computational power &amp; embedded hardware.</p><p>Inspired by deep learning (DL) breakthroughs <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b31">[32]</ref> in ImageNet large-scale visual recognition competition (ILSVRC) <ref type="bibr" target="#b32">[33]</ref> and also visual object tracking (VOT) challenge <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b39">[40]</ref>, DL-based methods have attracted consider-able interest in the visual tracking community to provide robust trackers. Although convolutional neural networks (CNNs) have been dominant networks initially, a broad range of architectures such as recurrent neural networks (RNNs), autoencoders (AEs), generative adversarial networks (GANs), and especially Siamese neural networks (SNNs) &amp; custom neural networks are currently investigated. Fig. <ref type="figure">2</ref> presents a brief history of the development of deep visual trackers in recent years. The state-of-the-art DL-based visual trackers have distinct characteristics such as exploitation of various architectures, backbone networks, learning procedures, training datasets, network objectives, network outputs, types of exploited deep features, CPU/GPU implementations, programming languages &amp; frameworks, speed, and so forth. Therefore, this work provides a comparative study of DL-based trackers, benchmark datasets, and evaluation metrics to investigate proposed trackers in detail and facilitate developing advanced trackers.</p><p>Visual target trackers can be roughly classified into two main categories, before and after the revolution of DL in computer vision. The first category is primarily reviewed by <ref type="bibr" target="#b40">[41]</ref>- <ref type="bibr" target="#b43">[44]</ref>, which include traditional trackers based on classical appearance &amp; motion models, and then examine their pros and cons systematically, experimentally, or both. These trackers employ manually-designed features for target modeling to alleviate appearance variations and to provide efficient computational complexity. For instance, although these trackers are suitable to implement on the flying robots <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref> due to the restrictions of using advanced GPUs, they do not have enough robustness to handle the challenges of in-thewild videos. Typically, these trackers try to ensemble multiple features to construct a complementary set of visual cues. But, tuning an optimal trade-off that also maintains efficiency for real-world scenarios is tricky. Considering DL-based trackers' significant progress in recent years, the reviewed methods by the mentioned works are outdated.</p><p>The second category includes DL-based trackers that employ either deep off-the-shelf features or end-to-end networks. A straightforward approach is integrating pre-trained deep features into the traditional frameworks. However, such trackers result in inconsistency problems considering task differences. But, end-to-end trained visual trackers have been investigated regarding existing tracking challenges. Recently, <ref type="bibr" target="#b46">[47]</ref>- <ref type="bibr" target="#b48">[49]</ref> review limited DL-based visual trackers. For instance, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref> categorize some handcrafted &amp; deep methods into the correlation filter trackers &amp; non-correlation filter ones. Then, a further classification based on architectures &amp; tracking mechanisms has been applied. The work <ref type="bibr" target="#b49">[50]</ref> particularly investigates some SNN-based trackers based on their network branches, layers, and training aspects. However, it does not include state-of-the-art trackers and custom networks with &amp; without partial exploitation of SNNs. At last, the work <ref type="bibr" target="#b48">[49]</ref> categorizes the DL-based trackers according to their structure, function, and training. Then, the evaluations are performed to conclude the categorizations based on the observations. From the structure perspective, the trackers are categorized into the CNN, RNN, and others, while they are classified into the feature extraction network (FEN) or end-to-end network (EEN) according to their functionality in visual tracking. The EENs are also classified in terms of the outputs, including object score, confidence map, and bounding box (BB). Finally, DL-based methods are categorized according to pre-training &amp; online learning based on the network training perspective.</p><p>According to the previous efforts, the motivations of this work are presented as follows.</p><p>1) Despite all efforts, existing review papers do not include state-of-the-art visual trackers that roughly employ Siamese or customized networks.</p><p>2) Notwithstanding significant progress in recent years, long-term trackers and tracking from aerial-views have not yet been studied. Hence, investigating the current issues and proposed solutions are necessary.</p><p>3) Many details are ignored in previous works studying the DL-based trackers (e.g., backbone networks, training details, exploited features, implementations, etc.).</p><p>4) State-of-the-art benchmark datasets (short-term, longterm, aerial-view) are not compared completely.</p><p>5) Finally, exhaustive comparisons of DL-based trackers on a wide variety of benchmarks have not been previously performed. These analyzes can demonstrate the advantages and limitations of existing trackers.</p><p>Motivated by the aforementioned concerns, this work's primary goals are filling the gaps, investigating the present issues, and studying potential future directions. Thus, we focus merely on extensive state-of-the-art DL-based trackers, namely: HCFT <ref type="bibr" target="#b50">[51]</ref>, DeepSRDCF <ref type="bibr" target="#b51">[52]</ref>, FCNT <ref type="bibr" target="#b52">[53]</ref>, CNN-SVM <ref type="bibr" target="#b53">[54]</ref>, DPST <ref type="bibr" target="#b54">[55]</ref>, CCOT <ref type="bibr" target="#b55">[56]</ref>, GOTURN <ref type="bibr" target="#b56">[57]</ref>, SiamFC <ref type="bibr" target="#b57">[58]</ref>, SINT <ref type="bibr" target="#b58">[59]</ref>, MDNet <ref type="bibr" target="#b59">[60]</ref>, HDT <ref type="bibr" target="#b60">[61]</ref>, STCT <ref type="bibr" target="#b61">[62]</ref>, RPNT</p><p>The main contributions are summarized as follows. 1) State-of-the-art DL-based visual trackers are categorized based on the architecture (i.e., CNN, SNN, RNN, GAN, and custom networks), network exploitation (i.e., off-the-shelf deep features and deep features for visual tracking), network training for visual tracking (i.e., only offline training, only online training, both offline &amp; online training, meta-learning), network objective (i.e., regression-based, classification-based, and both classification &amp; regression-based), exploitation of correlation filter advantages (i.e., DCF framework and utilizing correlation filter/layer/function), aerial-view tracking, longterm tracking, and online tracking. Such a study covering all of these aspects in the detailed categorization of visual tracking methods has not been previously presented.</p><p>2) The main issues and proposed solutions of DL-based trackers to tackle visual tracking challenges are presented. This classification provides proper insight into designing visual trackers.</p><p>3) The well-known single-object visual tracking datasets (i.e., short-term, long-term, aerial view) are completely compared based on their fundamental characteristics (e.g., the number of videos, frames, classes/clusters, sequence attributes, absent labels, and overlap with other datasets). These benchmark datasets include OTB2013 <ref type="bibr" target="#b218">[219]</ref>, OTB2015 <ref type="bibr" target="#b219">[220]</ref>, VOT <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b39">[40]</ref>, ALOV <ref type="bibr" target="#b41">[42]</ref>, TC128 <ref type="bibr" target="#b224">[225]</ref>, UAV123 <ref type="bibr" target="#b221">[222]</ref>, NUS-PRO <ref type="bibr" target="#b225">[226]</ref>, NfS <ref type="bibr" target="#b226">[227]</ref>, DTB <ref type="bibr" target="#b227">[228]</ref>, TrackingNet <ref type="bibr" target="#b228">[229]</ref>, Ox-UvA <ref type="bibr" target="#b229">[230]</ref>, BUAA-PRO <ref type="bibr" target="#b230">[231]</ref>, GOT10k <ref type="bibr" target="#b231">[232]</ref>, LaSOT <ref type="bibr" target="#b220">[221]</ref>, UAV20L <ref type="bibr" target="#b221">[222]</ref>, TinyTLP/TLPattr <ref type="bibr" target="#b232">[233]</ref>, TLP <ref type="bibr" target="#b232">[233]</ref>, TracKlinic <ref type="bibr" target="#b233">[234]</ref>, UAVDT <ref type="bibr" target="#b222">[223]</ref>, LTB35 <ref type="bibr" target="#b234">[235]</ref>, VisDrone <ref type="bibr" target="#b223">[224]</ref>, <ref type="bibr" target="#b235">[236]</ref>, VisDrone2019L <ref type="bibr" target="#b223">[224]</ref>, and Small-90/Small-112 <ref type="bibr" target="#b236">[237]</ref>.</p><p>4) Finally, extensive experimental evaluations are performed on a wide variety of tracking datasets, namely OTB2013 <ref type="bibr" target="#b218">[219]</ref>, OTB2015 <ref type="bibr" target="#b219">[220]</ref>, VOT2018 <ref type="bibr" target="#b38">[39]</ref>, LaSOT <ref type="bibr" target="#b220">[221]</ref>, UAV123 <ref type="bibr" target="#b221">[222]</ref>, UAVDT <ref type="bibr" target="#b222">[223]</ref>, and VisDrone2019 <ref type="bibr" target="#b223">[224]</ref>, and the state-of-theart visual trackers are analyzed based on different aspects. Moreover, this work specifies the most challenging visual attributes for the VOT2018 dataset and OTB2015, LaSOT, UAV123, UAVDT, and VisDrone2019 datasets. By doing so, the most primary challenges of each dataset for recent trackers are specified.</p><p>According to the comparisons, the following remarks are concluded.</p><p>1) The Siamese-based networks are the most promising deep architectures due to their satisfactory balance between performance and efficiency for visual tracking. Moreover, some methods recently attempt to exploit the advantages of RL &amp; GAN approaches to refine their decision-making and alleviate the lack of training data. Based on these advantages, recent trackers aim to design custom neural networks to fully exploit scene information.</p><p>2) The offline end-to-end learning of deep features appropriately transfers pre-trained generic features to visual tracking task. Although conventional online training of DNN increases the computational complexity such that most of these methods are not suitable for real-time applications, it considerably helps visual trackers to adapt with significant appearance variation, prevent visual distractors, and improve the performance of visual trackers. Exploiting meta-learning approaches have provided significant advances to the online adaptation of visual trackers. Therefore, both offline and (efficient) online training procedures result in promising tracking performances.</p><p>3) Leveraging deeper and wider backbone networks improves the discriminative power of distinguishing the target from its background. Pre-trained networks (e.g., ResNet <ref type="bibr" target="#b31">[32]</ref>) are sub-optimal, and tracking performance can be remarkably improved by training backbone networks for visual tracking.</p><p>4) The best trackers exploit both regression &amp; classification objective functions to distinguish the target from the background and find the tightest BB for target localization. These objectives are complementary such that the regression function has the role of auxiliary supervision on the classification one. Recently, video object segmentation (VOS) approaches are integrated into visual trackers for representing targets by segmentation masks.</p><p>5) The exploitation of different features enhances the robustness of the target model. For instance, most of the DCFbased methods fuse the deep off-the-shelf features and handcrafted features (e.g., HOG &amp; CN) for this reason. Also, the exploitation of complementary features, such as temporal or contextual information, has led to more robust features in challenging scenarios.</p><p>6) The most challenging attributes for DL-based visual tracking methods are occlusion, out-of-view, fast motion, aspect ratio change, and similar objects. Moreover, visual distractors with similar semantics may result in the drift problem.</p><p>The rest of this paper is as follows. Section II introduces our taxonomy of deep visual trackers. The visual tracking benchmark datasets and evaluation metrics are compared in Section III. Experimental comparisons of the state-of-the-art visual tracking methods are performed in Section IV. Finally, Section V summarizes the conclusions and future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DEEP VISUAL TRACKING TAXONOMY</head><p>Generally, three major components of: i) target representation/information, ii) training process, and iii) learning procedure play important roles in designing visual tracking methods. Most DL-based trackers aim to improve a target representation by utilizing/fusing deep hierarchical features, exploiting contextual/motion information, and select more discriminative/robust deep features. To effectively train DNNs for visual tracking systems, general motivations can be classified into employing various training schemes (e.g., network pre-training, online training (also meta-learning), or both) and handling training problems (e.g., lacking training samples, over-fitting, or computational complexity). Unsupervised training is another recent scheme to use abundant unlabeled samples, which can be performed by clustering the samples according to contextual information, mapping training data to a manifold space, or exploiting consistency-based objective function. Finally, the primary motivations regarding learning procedures are online update schemes, scale/aspect ratio estimation, search strategies, and long-term memory.</p><p>In the following, DL-based methods are comprehensively categorized based on nine aspects, and the main motivations and contributions of trackers are classified. Fig. <ref type="figure" target="#fig_1">3</ref> presents the proposed taxonomy of DL-based visual trackers, including network architecture, network exploitation, network training for visual tracking purposes, network objective, network output, exploitation of correlation filter advantages, aerial-view tracking, long-term tracking, and online tracking. Moreover, DL-based trackers are compared in detail regarding the pretrained networks, backbone networks, exploited layers, types of deep features, the fusions of hand-crafted &amp; deep features, training datasets, tracking outputs, tracking speeds, hardware implementation details, programming languages, and DL frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture</head><p>Although CNNs have been extensively adopted for visual tracking, other architectures also have been mainly developed to improve the efficiency and robustness of visual trackers in recent years. Accordingly, the proposed taxonomy consists of the CNN-, SNN-, GAN-, RNN-, and custom network-based visual trackers.</p><p>CNN-based trackers were the first to provide powerful representations of a target by hierarchical processing of twodimensional frames, independently. However, conventional CNNs have inherent limitations, such as training on large supervised datasets, ignoring temporal dependencies, and computational complexities for online adaptation. As an alternative approach, SNN-based trackers measure the similarity between the target exemplar and the search region to overcome the limitations. Generally, SNNs employ CNN layers/blocks/networks in two or more branches for similarity learning purposes and run (near/over) real-time speed. However, online adaptation and handling challenges like occlusion are still under investigation. Architectures such as RNNs and GANs have been limitedly studied for visual tracking. In general, RNNs are used to capture temporal information among video frames, but they have limitations in their stability and long-term learning dependencies. GANs comprise generator &amp; discriminator subnetworks, which can provide the possibility to address some limitations. For instance, competing for these networks can help trackers handle scarce positive samples, although there are some barriers to training and generalization of GANs. Finally, recent custom networks include various architectures to strengthen learning features and reduce computational complexity. In the following, the primary contributions of DLbased visual trackers are summarized.</p><p>1) Convolutional Neural Network (CNN): Motivated by CNN breakthroughs in computer vision and their attractive advantages (e.g., parameter sharing, sparse interactions, and dominant representations), a wide range of CNN-based trackers have been proposed. The main motivations are presented as follows.</p><p>• Robust target representation: Providing powerful representations of targets is the primary advantage of employing CNNs for visual tracking. To learn robust target models, the contributions can be classified into: i) offline training of CNNs on large-scale visual tracking datasets <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b81">[82]</ref>, [90], <ref type="bibr" target="#b92">[93]</ref>, <ref type="bibr" target="#b93">[94]</ref>, <ref type="bibr" target="#b96">[97]</ref>, <ref type="bibr" target="#b104">[105]</ref>, <ref type="bibr" target="#b108">[109]</ref>, <ref type="bibr" target="#b127">[128]</ref>, <ref type="bibr" target="#b129">[130]</ref>, <ref type="bibr" target="#b134">[135]</ref>, <ref type="bibr" target="#b136">[137]</ref>, <ref type="bibr" target="#b145">[146]</ref>, <ref type="bibr" target="#b160">[161]</ref>, <ref type="bibr" target="#b163">[164]</ref>, <ref type="bibr" target="#b164">[165]</ref>, <ref type="bibr" target="#b168">[169]</ref>, ii) designing specific CNNs instead of employing pre-trained models <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b89">[90]</ref>, <ref type="bibr" target="#b92">[93]</ref>, <ref type="bibr" target="#b93">[94]</ref>, <ref type="bibr" target="#b96">[97]</ref>, <ref type="bibr" target="#b97">[98]</ref>, <ref type="bibr" target="#b100">[101]</ref>, <ref type="bibr" target="#b104">[105]</ref>, <ref type="bibr" target="#b108">[109]</ref>, <ref type="bibr" target="#b119">[120]</ref>, <ref type="bibr" target="#b127">[128]</ref>, <ref type="bibr" target="#b129">[130]</ref>, <ref type="bibr" target="#b133">[134]</ref>, <ref type="bibr" target="#b134">[135]</ref>, <ref type="bibr" target="#b136">[137]</ref>, <ref type="bibr" target="#b138">[139]</ref>, <ref type="bibr" target="#b142">[143]</ref>, <ref type="bibr" target="#b145">[146]</ref>, <ref type="bibr" target="#b160">[161]</ref>, <ref type="bibr" target="#b162">[163]</ref>- <ref type="bibr" target="#b164">[165]</ref>, <ref type="bibr" target="#b166">[167]</ref>, <ref type="bibr" target="#b168">[169]</ref>, <ref type="bibr" target="#b204">[205]</ref>, iii) constructing multiple target models to capture varieties of target appearances <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b108">[109]</ref>, <ref type="bibr" target="#b119">[120]</ref>, <ref type="bibr" target="#b121">[122]</ref>, <ref type="bibr" target="#b122">[123]</ref>, <ref type="bibr" target="#b135">[136]</ref>, <ref type="bibr" target="#b138">[139]</ref>, <ref type="bibr" target="#b167">[168]</ref>, <ref type="bibr" target="#b209">[210]</ref>, iv) incorporating spatial and temporal information to improve model generalization <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b98">[99]</ref>, <ref type="bibr" target="#b111">[112]</ref>, <ref type="bibr" target="#b114">[115]</ref>, <ref type="bibr" target="#b129">[130]</ref>, <ref type="bibr" target="#b143">[144]</ref>, <ref type="bibr" target="#b145">[146]</ref>, <ref type="bibr" target="#b207">[208]</ref>, <ref type="bibr" target="#b208">[209]</ref>, v) fusion of different deep features to exploit complementary spatial and semantic information <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b93">[94]</ref>, <ref type="bibr" target="#b100">[101]</ref>, <ref type="bibr" target="#b101">[102]</ref>, <ref type="bibr" target="#b127">[128]</ref>, <ref type="bibr" target="#b203">[204]</ref>- <ref type="bibr" target="#b205">[206]</ref>, vi) learning particular target models such as relative model <ref type="bibr" target="#b96">[97]</ref> or part-based models <ref type="bibr" target="#b108">[109]</ref>, <ref type="bibr" target="#b119">[120]</ref>, <ref type="bibr" target="#b138">[139]</ref> to handle partial occlusion and deformation, vii) utilizing a two-stream network <ref type="bibr" target="#b119">[120]</ref> to prevent over-fitting and to learn rotation information, and accurately estimating target aspect ratio to avoid contaminating target model with non-relevant information <ref type="bibr" target="#b173">[174]</ref>, and viii) group feature selection through channel &amp; spatial dimensions to learn the structural relevance of features.</p><p>• Balancing training data: Based on problem definition, there is just one positive sample in the first frame that increases the risk of over-fitting during tracking. Although the background information arbitrary can be considered negative in each frame, target sampling based on imperfect target estimations may also lead to noisy/unreliable training samples. These issues dramatically affect the performance of visual tracking methods. To alleviate them, CNN-based trackers propose: i) domain adaption mechanism (i.e., transferring learned knowledge from the source domain to target one with insufficient samples) <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b160">[161]</ref>, ii) various update mechanisms (e.g., periodic, stochastic, short-term, &amp; long-term updates) <ref type="bibr" target="#b97">[98]</ref>, <ref type="bibr" target="#b121">[122]</ref>, <ref type="bibr" target="#b135">[136]</ref>, <ref type="bibr" target="#b141">[142]</ref>, <ref type="bibr" target="#b164">[165]</ref>, iii) convolutional Fisher discriminative analysis (FDA) for positive and negative sample mining <ref type="bibr" target="#b133">[134]</ref>, iv) multiple-branch CNN for online ensemble learning <ref type="bibr" target="#b89">[90]</ref>, v) efficient sampling strategies to increase the number of training samples <ref type="bibr" target="#b166">[167]</ref>, and vi) a recursive leastsquare estimation algorithm to provide a compromise between the discrimination power &amp; update iterations during online learning <ref type="bibr" target="#b194">[195]</ref>.</p><p>• Computational complexity problem: Despite the significant progress of CNNs in appearance representation, the CNNbased methods still suffer from high computational complexity. To reduce this limitation, CNN-based visual tracking methods exploit different solutions, namely: i) employing a straightforward CNN architecture <ref type="bibr" target="#b183">[184]</ref>, ii) disassembling a CNN into several shrunken networks <ref type="bibr" target="#b68">[69]</ref>, iii) compressing or pruning training sample space <ref type="bibr" target="#b86">[87]</ref>, <ref type="bibr" target="#b107">[108]</ref>, <ref type="bibr" target="#b133">[134]</ref>, <ref type="bibr" target="#b145">[146]</ref>, <ref type="bibr" target="#b163">[164]</ref> or feature selection <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b146">[147]</ref>, iv) feature computation via RoIAlign operation <ref type="bibr" target="#b104">[105]</ref> (i.e., feature approximation via bilinear interpolation) or oblique random forest <ref type="bibr" target="#b91">[92]</ref> for better data capturing, v) corrective domain adaption method <ref type="bibr" target="#b160">[161]</ref>, vi) lightweight structure <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b162">[163]</ref>, vii) efficient optimization processes <ref type="bibr" target="#b90">[91]</ref>, <ref type="bibr" target="#b147">[148]</ref>, viii) particle sampling strategy <ref type="bibr" target="#b88">[89]</ref>, ix) utilizing attentional mechanism <ref type="bibr" target="#b92">[93]</ref>, x) extending the random vector functional link (RVFL) network to a convolutional structure <ref type="bibr" target="#b182">[183]</ref>, and xi) exploiting advantages of correlation filters <ref type="bibr" target="#b50">[51]</ref>- <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b69">[70]</ref>- <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b84">[85]</ref>, <ref type="bibr" target="#b86">[87]</ref>- <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b90">[91]</ref>, <ref type="bibr" target="#b92">[93]</ref>, <ref type="bibr" target="#b98">[99]</ref>, <ref type="bibr" target="#b100">[101]</ref>, <ref type="bibr" target="#b101">[102]</ref>, <ref type="bibr" target="#b107">[108]</ref>, <ref type="bibr" target="#b111">[112]</ref>, <ref type="bibr" target="#b114">[115]</ref>, <ref type="bibr" target="#b118">[119]</ref>, <ref type="bibr" target="#b119">[120]</ref>, <ref type="bibr" target="#b121">[122]</ref>- <ref type="bibr" target="#b123">[124]</ref>, <ref type="bibr" target="#b127">[128]</ref>, <ref type="bibr" target="#b132">[133]</ref>, <ref type="bibr" target="#b133">[134]</ref>, <ref type="bibr" target="#b135">[136]</ref>, <ref type="bibr" target="#b136">[137]</ref>, <ref type="bibr" target="#b141">[142]</ref>- <ref type="bibr" target="#b143">[144]</ref>, <ref type="bibr" target="#b147">[148]</ref>, <ref type="bibr" target="#b151">[152]</ref>, <ref type="bibr" target="#b160">[161]</ref>, <ref type="bibr" target="#b162">[163]</ref>, <ref type="bibr" target="#b166">[167]</ref>, <ref type="bibr" target="#b167">[168]</ref>, <ref type="bibr" target="#b169">[170]</ref> for efficient computations. Exploiting the advantages of correlation filters refers to either applying DCFs on pre-trained networks or combining correlation filters/layers/functions with end-toend networks.</p><p>2) Siamese Neural Network (SNN): SNNs are widely employed for visual trackers in the past few years. Given the pairs of target and search regions, these two-stream networks compute the same function to produce a similarity map. They mainly aim to overcome the limitations of pre-trained deep CNNs and take full advantage of end-to-end learning for realtime applications.</p><p>• Discriminative target representation: The ability to construct a robust target model majorly relies on target representation. For achieving more discriminative deep features and improving target modeling, SNN-based methods propose: i) learning distractor-aware <ref type="bibr" target="#b103">[104]</ref> or target-aware features <ref type="bibr" target="#b156">[157]</ref>, ii) fusing deep multi-level features <ref type="bibr" target="#b124">[125]</ref>, <ref type="bibr" target="#b149">[150]</ref> or combining confidence maps <ref type="bibr" target="#b80">[81]</ref>, <ref type="bibr" target="#b82">[83]</ref>, <ref type="bibr" target="#b116">[117]</ref>, iii) utilizing different loss functions in Siamese formulation to train more effective filters <ref type="bibr" target="#b99">[100]</ref>, <ref type="bibr" target="#b154">[155]</ref>, <ref type="bibr" target="#b156">[157]</ref>- <ref type="bibr" target="#b158">[159]</ref>, iv) leveraging different types of deep features such as context information <ref type="bibr" target="#b109">[110]</ref>, <ref type="bibr" target="#b116">[117]</ref>, <ref type="bibr" target="#b150">[151]</ref> or temporal features/models <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b117">[118]</ref>, <ref type="bibr" target="#b125">[126]</ref>, <ref type="bibr" target="#b150">[151]</ref>, <ref type="bibr" target="#b170">[171]</ref>, v) full exploring of low-level spatial features <ref type="bibr" target="#b124">[125]</ref>, <ref type="bibr" target="#b149">[150]</ref>, vi) considering angle estimation of a target to prevent salient background objects <ref type="bibr" target="#b110">[111]</ref>, vii) utilizing multi-stage regression to refine target representation <ref type="bibr" target="#b149">[150]</ref>, vii) using the deeper and wider deep network as the backbone to increase the receptive field of neurons, which is equivalent to capturing the structure of the target <ref type="bibr" target="#b153">[154]</ref>, viii) employing correlation-guided attention modules to exploit the relationship between the template &amp; RoI feature maps <ref type="bibr" target="#b189">[190]</ref>, ix) computing correlations between attentional features <ref type="bibr" target="#b196">[197]</ref>, x) accurately estimating the scale and aspect ratio of the target <ref type="bibr" target="#b197">[198]</ref>, xi) simultaneously learning classification &amp; regression models <ref type="bibr" target="#b198">[199]</ref>, xii) mining hard samples in training <ref type="bibr" target="#b199">[200]</ref>, and xiii) employing skimming &amp; perusal modules for inferring optimal target candidates <ref type="bibr" target="#b217">[218]</ref>. Finally, <ref type="bibr" target="#b190">[191]</ref>, <ref type="bibr" target="#b192">[193]</ref> are performed adversarial attacks on SNN-based trackers to evaluate misbehaving SNN models for visual tracking scenarios. These methods generate slight perturbations for deceiving the trackers to finally investigate DL models and improve their robustness.</p><p>• Adapting target appearance variation: Using only offline training of the first generation of SNN-based trackers caused a poor generalization to unseen targets. To solve it, recent SNNbased trackers propose: i) online update strategies considering strategies to reduce the risk of over-fitting <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b82">[83]</ref>, <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b95">[96]</ref>, <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b144">[145]</ref>, <ref type="bibr" target="#b186">[187]</ref>- <ref type="bibr" target="#b188">[189]</ref>, <ref type="bibr" target="#b215">[216]</ref>, ii) background suppression <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b103">[104]</ref>, iii) formulating tracking task as a one-shot local detection task <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b115">[116]</ref>, iv) and giving higher weights to important feature channels or score maps <ref type="bibr" target="#b80">[81]</ref>, <ref type="bibr" target="#b116">[117]</ref>, <ref type="bibr" target="#b120">[121]</ref>, <ref type="bibr" target="#b140">[141]</ref>, and v) modeling all potential distractors considering their motion and interaction <ref type="bibr" target="#b199">[200]</ref>. Alternatively, the DaSi-amRPN <ref type="bibr" target="#b103">[104]</ref> and MMLT <ref type="bibr" target="#b106">[107]</ref> use a local-to-global search region strategy and memory exploitation to handle critical challenges such as full occlusion and out-of-view and enhance local search strategy.</p><p>• Balancing training data: The same as CNN-based methods, some efforts by SNN-based methods have been performed to address the imbalance distribution of training samples. The main contributions of the SNN-based methods are: i) exploiting multi-stage Siamese framework to stimulate hard negative sampling <ref type="bibr" target="#b149">[150]</ref>, ii) adopting sampling heuristics such as fixed foreground-to-background ratio <ref type="bibr" target="#b149">[150]</ref> or sampling strategies such as random sampling <ref type="bibr" target="#b103">[104]</ref> or flow-guided sampling <ref type="bibr" target="#b125">[126]</ref>, and iii) taking advantages of correlation filter/layer into Siamese framework <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b94">[95]</ref>, <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b115">[116]</ref>, <ref type="bibr" target="#b117">[118]</ref>, <ref type="bibr" target="#b120">[121]</ref>, <ref type="bibr" target="#b140">[141]</ref>, <ref type="bibr" target="#b144">[145]</ref>, <ref type="bibr" target="#b146">[147]</ref>, <ref type="bibr" target="#b148">[149]</ref>, <ref type="bibr" target="#b156">[157]</ref>, <ref type="bibr" target="#b157">[158]</ref>, <ref type="bibr" target="#b165">[166]</ref>.</p><p>3) Recurrent Neural Network (RNN): Since visual tracking is related to both spatial and temporal information of video frames, RNN-based methods also consider target motion/movement. Because of arduous training and a considerable number of parameters, the number of RNN-based methods is limited. Almost all these methods try to exploit additional information and memory to improve target modeling. Also, the second aim of using RNN-based methods is to avoid fine-tuning of pre-trained CNN models, which takes a lot of time and is prone to over-fitting. The primary purposes of these methods can be classified to the spatio-temporal representation capturing <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b131">[132]</ref>, <ref type="bibr" target="#b170">[171]</ref>, leveraging contextual informa-tion to handle background clutters <ref type="bibr" target="#b131">[132]</ref>, exploiting multilevel visual attention to highlight target, background suppression <ref type="bibr" target="#b170">[171]</ref>, and using convolutional long short-term memory (LSTM) as the memory unit of previous target appearances <ref type="bibr" target="#b76">[77]</ref>. Moreover, RNN-based methods exploit pyramid multidirectional recurrent network <ref type="bibr" target="#b131">[132]</ref> or incorporate LSTM into different networks <ref type="bibr" target="#b76">[77]</ref> to memorize target appearance and investigate time dependencies. Finally, the <ref type="bibr" target="#b131">[132]</ref> encodes the self-structure of a target to reduce tracking sensitivity related to similar distractors.</p><p>4) Generative Adversarial Network (GAN): Based on some attractive advantages, such as capturing statistical distribution and generating desired training samples without extensive annotated data, GANs have been intensively utilized in many research areas. Although GANs are usually hard to train and evaluate, some DL-based trackers employ GANs to enrich training samples and target modeling. These networks can augment positive samples in feature space to address the imbalance distribution of training samples <ref type="bibr" target="#b113">[114]</ref>. Also, the GAN-based methods can learn general appearance distribution to deal with visual tracking's self-learning problem <ref type="bibr" target="#b128">[129]</ref>. Furthermore, the joint optimization of regression &amp; discriminative networks will take advantage of both these two tasks <ref type="bibr" target="#b159">[160]</ref>. Lastly, these networks can explore the relations between a target with its contextual information for searching interested regions and transfer this information to videos with different inherits such as transferring from ground-view to drone-view <ref type="bibr" target="#b206">[207]</ref>.</p><p>5) Custom Networks: Inspired by particular deep architectures and network layers, modern DL-based methods have combined a wide range of networks such as AE, CNN, RNN, SNN, detection networks &amp; also deep RL for visual tracking. The main motivation of custom networks is to compensate for ordinary trackers' deficiencies by exploiting the advantages of other networks. Furthermore, meta-learning (or learning to learn) has been recently attracted by the visual tracking community. It aims to address few-shot learning problems and fast adaptation of a learner to a new task by leveraging accumulated experiences from similar tasks. By employing the meta-learning framework, different networks can learn unseen target appearances during online tracking. The primary motivations and contributions of custom networks are classified as follows.</p><p>• Robust and accurate tracking: Recent networks seek general &amp; effective frameworks for better localization and BB estimation. For instance, aggregating several online trackers <ref type="bibr" target="#b202">[203]</ref> is a way to improve tracking performance. An alternative is a better understanding of the target's pose by exclusively designed target estimation and classification networks <ref type="bibr" target="#b148">[149]</ref>. Also, meta-learning based networks <ref type="bibr" target="#b158">[159]</ref>, <ref type="bibr" target="#b193">[194]</ref> can predict powerful target models inspiring by discriminative learning procedures. However, some other works <ref type="bibr" target="#b184">[185]</ref>, <ref type="bibr" target="#b200">[201]</ref> consider the tracking task as an instance detection and aim to convert modern object detectors directly to a visual tracker. These trackers exploit class-agnostic networks, which can: i) differentiate intra-class samples, ii) quickly adapt to different targets by meta-learners, and iii) consider temporal cues. The D3S <ref type="bibr" target="#b191">[192]</ref> method models a visual target from its segmentation mask with complementary geometric properties to improve the robustness of template-based trackers. The COMET <ref type="bibr" target="#b211">[212]</ref> bridges the gap between advanced visual trackers and aerialview ones in detecting small/tiny objects. It employs multiscale feature learning and attention modules to compensate for the inferior performance of generic trackers in medium-/high-attitude aerial views.</p><p>• Computational complexity problem: As stated before, this problem limits the performance of online trackers in realtime applications. To control computational complexity, the TRACA <ref type="bibr" target="#b112">[113]</ref> and AEPCF <ref type="bibr" target="#b166">[167]</ref> methods employ AEs to compress raw conventional deep features. The EAST <ref type="bibr" target="#b176">[177]</ref> adaptively takes either shallow features for simple frames for tracking or expensive deep features for challenging ones <ref type="bibr" target="#b176">[177]</ref>, and the TRACA <ref type="bibr" target="#b112">[113]</ref>, CFSRL <ref type="bibr" target="#b137">[138]</ref>, &amp; AEPCF <ref type="bibr" target="#b166">[167]</ref> exploit the DCF computation efficiency. An effective way to avoid high computational burden is exploiting meta-learning that quickly adapts pre-trained trackers on unseen targets. The target model of the meta-learning based trackers can be optimized in a few iterations <ref type="bibr" target="#b158">[159]</ref>, <ref type="bibr" target="#b181">[182]</ref>, <ref type="bibr" target="#b184">[185]</ref>, <ref type="bibr" target="#b187">[188]</ref>, <ref type="bibr" target="#b193">[194]</ref>, <ref type="bibr" target="#b195">[196]</ref>, <ref type="bibr" target="#b200">[201]</ref>, <ref type="bibr" target="#b216">[217]</ref>.</p><p>• Model update: To maintain the stability of the target model during the tracking process, different update strategies have been proposed; for instance, the CFSRL <ref type="bibr" target="#b137">[138]</ref> updates multiple models in parallel, the DRRL <ref type="bibr" target="#b161">[162]</ref> incorporates an LSTM to exploit long-range time dependencies, and the AEPCF <ref type="bibr" target="#b166">[167]</ref> utilizes long-term and short-term update schemes to increase tracking speed. To prevent the erroneous model update and drift problem, the RDT <ref type="bibr" target="#b179">[180]</ref> has revised the visual tracking formulation to a consecutive decision-making process about the best target template for the next localization. Moreover, efficient learning of good decision policies using RL <ref type="bibr" target="#b178">[179]</ref> is another technique to take either model update or ignore the decision. A recent alternative is employing meta-learning approaches for quick model adaptation. For instance, the works <ref type="bibr" target="#b158">[159]</ref>, <ref type="bibr" target="#b193">[194]</ref>, <ref type="bibr" target="#b195">[196]</ref> use recurrent optimization processes that update the target model in a few gradient steps. Finally, <ref type="bibr" target="#b216">[217]</ref> integrates sequential information (e.g., geometric, discriminative, and appearance cues) and exploits a meta-updater to effectively update on reliable frames.</p><p>• Limited training data: The soft and non-representative training samples can disturb visual tracking if occlusion, blurring, and large deformation happen. The AEPCF <ref type="bibr" target="#b166">[167]</ref> exploits a dense circular sampling scheme to prevent the over-fitting problem caused by limited training data. The SINT++ <ref type="bibr" target="#b180">[181]</ref> generates positive and hard training samples by positive sample generation network (PSGN) and hard positive transformation network (HPTN) to make diverse and challenging training data. To efficiently train DNNs without a large amount of training data, partially labeled training samples are utilized by an action-driven deep tracker <ref type="bibr" target="#b171">[172]</ref>, <ref type="bibr" target="#b172">[173]</ref>. The P-Track <ref type="bibr" target="#b178">[179]</ref> also uses active decision-making to label videos interactively while learning a tracker when limited annotated data are available. Meta-Tracker <ref type="bibr" target="#b181">[182]</ref> was the first attempt to exploit an offline meta-learning-based method for better online adaptation of visual trackers. This method can generalize the target model and avoid over-fitting to distractors. Besides, various pioneer trackers <ref type="bibr" target="#b158">[159]</ref>, <ref type="bibr" target="#b184">[185]</ref>, <ref type="bibr" target="#b187">[188]</ref>, <ref type="bibr" target="#b193">[194]</ref>, <ref type="bibr" target="#b195">[196]</ref>, <ref type="bibr" target="#b200">[201]</ref>, <ref type="bibr" target="#b216">[217]</ref> enjoy the advantages of meta-learners in one/few-shot learning tasks.</p><p>• Search strategy: From the definition, visual tracking methods estimate the new target state in the next frame's search region, given an initial target state in the first frame. The best search region selection depends on the iterative search strategies that usually are independent of video content and are heuristic, brute-force, and hand-engineered. Despite classical search strategies based on sliding windows, mean shift, or particle filter, the state-of-the-art DL-based visual trackers exploit RL-based methods to learn data-driven searching policies. To exhaustively explore a region of interest and select the best target candidate, action-driven tracking mechanisms <ref type="bibr" target="#b171">[172]</ref>, <ref type="bibr" target="#b172">[173]</ref> consider the target context variation and actively pursues the target movement. Furthermore, the ACT and DRRL have proposed practical RL-based search strategies for real-time requirements by dynamic search <ref type="bibr" target="#b102">[103]</ref> and coarseto-fine verification <ref type="bibr" target="#b161">[162]</ref>. Lastly, the full-image visual tracker <ref type="bibr" target="#b213">[214]</ref> exploits a two-stage detector for localizing the target without any assumptions (e.g., temporal consistency of target regions).</p><p>• Exploiting additional information: To enhance the target model by utilizing motion or contextual information, the DCTN <ref type="bibr" target="#b130">[131]</ref> establishes a two-stream network, and the SRT <ref type="bibr" target="#b79">[80]</ref> adopts multi-directional RNN to learn further dependencies of a target during visual tracking. Also, the FGTrack <ref type="bibr" target="#b201">[202]</ref> estimates the scale &amp; rotation of the target and its displacement by the finer-grained motion information provided by opticalflow. A recurrent convolutional network <ref type="bibr" target="#b175">[176]</ref> models previous semantic information and tracking proposals to encode relevant information for better localization. At last, DRL-IS <ref type="bibr" target="#b174">[175]</ref> has introduced an Actor-Critic network to estimate target motion parameters efficiently.</p><p>• Decision making: Online decision making has principal effects on the performance of DL-based visual tracking methods. The state-of-the-art methods attempt to learn online decision making by incorporating RL into the DL-based methods instead of hand-designed techniques. To gain effective decision policies, the P-Track <ref type="bibr" target="#b178">[179]</ref> ultimately exploits data-driven techniques in an active agent to decide about tracking, reinitializing, or updating processes. Also, the DRL-IS <ref type="bibr" target="#b174">[175]</ref> utilizes a principled RL-based method to select sensible action based on target status. Also, an action-prediction network has been proposed to adjust a visual tracker's continuous actions to determine the optimal hyper-parameters for learning the best action policies and make satisfactory decisions <ref type="bibr" target="#b177">[178]</ref>. On the other hand, the work <ref type="bibr" target="#b193">[194]</ref> considers the uncertainty in estimating target states. By predicting a conditional probability density of the visual target, direct interpretations can be provided for deciding about an update procedure or lost target. Also, a result judgment module <ref type="bibr" target="#b212">[213]</ref> can help short-term trackers in occlusion/out-of-view situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Exploitation</head><p>Roughly speaking, there are two main exploitations of DNNs for visual tracking, including reusing a pre-trained model on partially related datasets or exploiting deep features for visual tracking, which is equivalent to train DNNs for visual tracking tasks.</p><p>1) Model Reuse or Deep Off-the-Shelf Features: Exploiting deep off-the-shelf features is the simplest way to transfer the power of deep features into the traditional visual tracking methods. These features provide a generic representation of visual targets and help visual tracking methods to construct more robust target models. Regarding topologies, DNNs include either a simple multi-layer stack of non-linear layers (e.g., AlexNet <ref type="bibr" target="#b27">[28]</ref>, VGGNet <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>) or a directed acyclic graph topology (e.g., GoogLeNet <ref type="bibr" target="#b30">[31]</ref>, ResNet <ref type="bibr" target="#b120">[121]</ref>, SSD <ref type="bibr" target="#b237">[238]</ref>, Siamese convolutional neural network <ref type="bibr" target="#b238">[239]</ref>), which allows designing more complex deep architectures that include layers with multiple input/output. The main challenge of these trackers is how to benefit the generic representations effectively. Different methods employ various feature maps and models that have been pre-trained majorly on large-scale still images of the ImageNet dataset <ref type="bibr" target="#b32">[33]</ref> for the object recognition task. Numerous methods have studied the properties of pre-trained models and explored the impact of deep features in traditional frameworks (see Table <ref type="table" target="#tab_1">I</ref>). As a result, the DL-based methods have preferred simultaneous exploitation of both semantic and fine-grained deep features <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b132">[133]</ref>, <ref type="bibr" target="#b149">[150]</ref>, <ref type="bibr" target="#b203">[204]</ref>, <ref type="bibr" target="#b205">[206]</ref>, <ref type="bibr" target="#b239">[240]</ref>, <ref type="bibr" target="#b240">[241]</ref>. The fusion of deep features is also another motivation of these methods, which is performed by different techniques to utilize multi-resolution deep features <ref type="bibr" target="#b50">[51]</ref>- <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b101">[102]</ref>, <ref type="bibr" target="#b121">[122]</ref>, <ref type="bibr" target="#b122">[123]</ref>, <ref type="bibr" target="#b135">[136]</ref>, <ref type="bibr" target="#b144">[145]</ref>, <ref type="bibr" target="#b167">[168]</ref>, <ref type="bibr" target="#b203">[204]</ref>, <ref type="bibr" target="#b205">[206]</ref> and independent fusion of deep features with shallow ones at a later stage <ref type="bibr" target="#b101">[102]</ref>. Exploiting motion information <ref type="bibr" target="#b84">[85]</ref>, <ref type="bibr" target="#b98">[99]</ref>, <ref type="bibr" target="#b167">[168]</ref>, <ref type="bibr" target="#b241">[242]</ref> and selecting appropriate deep features for visual tracking tasks <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b146">[147]</ref>, <ref type="bibr" target="#b185">[186]</ref> are two other interesting motivations for DL-based methods. The detailed characteristics of DL-based visual trackers based on deep off-the-shelf features are shown in Table <ref type="table" target="#tab_1">I</ref>. Needless to say, the network output for these methods are deep feature maps.</p><p>2) Deep Features for Visual Tracking Purposes: One trending part of recent trackers is how to design and train DNNs for visual tracking. Using deep off-the-shelf features limits the tracking performance due to inconsistency among the objectives of different tasks. Also, offline learned deep features may not capture target variations and tend to over-fit on initial target templates. Hence, DNNs are trained on largescale datasets to specialize the networks for visual tracking purposes. Besides, applying a fine-tuning process during visual tracking can adjust some network parameters and produce more refined target representations. However, the fine-tuning process is time-consuming and prone to over-fitting because of a heuristically fixed iteration number and limited available training data. As shown in Table <ref type="table" target="#tab_2">II</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Training</head><p>The state-of-the-art DL-based visual tracking methods mostly exploit end-to-end learning with train/re-train a DNN by applying gradient-based optimization algorithms. However, these methods have differences according to their offline net-  <ref type="table" target="#tab_4">IV</ref>). A general-purpose dataset refers to a dataset from other tasks that provide desirable representations of different targets, e.g., object recognition or segmentation. It can include numerous datasets such as ImageNet <ref type="bibr" target="#b32">[33]</ref>, YouTube-VOS <ref type="bibr" target="#b242">[243]</ref>, YouTube-BoundingBoxes <ref type="bibr" target="#b243">[244]</ref>, KITTI <ref type="bibr" target="#b244">[245]</ref>, etc. That is, these datasets are used as auxiliary datasets in training procedures. However, tracking datasets are also utilized for training visual tracking networks. For instance, largescale tracking datasets such as LaSOT <ref type="bibr" target="#b220">[221]</ref> &amp; TrackingNet <ref type="bibr" target="#b228">[229]</ref> are explored in recent years. By exploring tracking datasets, the networks are trained on task-specific scenarios in the presence of challenging tracking attributes.</p><p>2) Only Offline Training: Most of the DL-based visual tracking methods only pre-train their networks to provide a generic target representation and reduce the high risk of overfitting due to imbalanced training data and fixed assumptions. To adjust the learned filter weights for visual tracking task, the specialized networks are trained on large-scale data to exploit better representation and achieve acceptable tracking speed by preventing from training during visual tracking (see Table <ref type="table" target="#tab_2">II</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Only Online Training:</head><p>To discriminate unseen targets that may consider as the target in evaluation videos, some DLbased visual tracking methods use online training of whole or a part of DNNs to adapt network parameters according to the large variety of target appearance. Because of the timeconsuming process of offline training on large-scale training data and insufficient discrimination of pre-trained models for representing tracking particular targets, the methods shown in Table <ref type="table" target="#tab_3">III</ref> use directly training of DNNs and inference process alternatively online. However, these methods usually exploit some strategies to prevent over-fitting problem and divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Both Offline and Online Training:</head><p>To exploit the maximum capacity of DNNs for visual tracking, the methods shown in Table <ref type="table" target="#tab_4">IV</ref> use both offline and online training. The offline and online learned features are known as shared and domain-specific representations, which majorly can discriminate the target from foreground information or intra-class distractors, respectively. Because visual tracking is a hard and challenging problem, the DL-based visual trackers attempt to simultaneously employ feature transferability and online domain adaptation.</p><p>5) Data Augmentation: Data augmentation comprises a set of techniques for increasing training samples' size to improve data quality and avoid the over-fitting problem. Visual trackers broadly employ these techniques based on the few-data regime of this task (see Table <ref type="table" target="#tab_5">V</ref>). The geometric transformations &amp; color space augmentations are vastly exploited for visual tracking. However, other algorithms, such as employing GANs <ref type="bibr" target="#b113">[114]</ref>, can effectively impact tracking performance by capturing various appearance changes.</p><p>6) Meta-Learning: As a well-known paradigm in machine learning, meta-learning <ref type="bibr" target="#b245">[246]</ref> (an alternative for data augmentation) has provided promising results for visual tracking task. Generally, it aims to provide experience on several learning tasks and use them to improve the performance of a new task. Inspired by model-agnostic meta-learning (MAML) <ref type="bibr" target="#b246">[247]</ref>, visual trackers mainly seek to exploit meta-learners for constructing more flexible target models regarding unseen targets/scenarios (see Table <ref type="table" target="#tab_4">IV</ref>). For instance, it can be leveraged into the initialization <ref type="bibr" target="#b181">[182]</ref>, <ref type="bibr" target="#b200">[201]</ref>, fast model adaptation <ref type="bibr" target="#b184">[185]</ref>, <ref type="bibr" target="#b187">[188]</ref>, or model update <ref type="bibr" target="#b195">[196]</ref>, <ref type="bibr" target="#b216">[217]</ref> procedures of    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Augmentations GOTURN <ref type="bibr" target="#b56">[57]</ref> Motion model, random crops MDNet <ref type="bibr" target="#b59">[60]</ref> Multiple positive examples around the target DeepTrack <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref> Horizontal flip RFL <ref type="bibr" target="#b76">[77]</ref> Random color distortion, translation, stretching VRCPF <ref type="bibr" target="#b83">[84]</ref> Annotated face ROIs using PASCAL VOC 2007 DNT <ref type="bibr" target="#b97">[98]</ref> Center-shifted random patches, translation schemes UPDT <ref type="bibr" target="#b101">[102]</ref> Flip, rotation, shift, blur, dropout DaSiamRPN <ref type="bibr" target="#b103">[104]</ref> Translation, scale variations and illumination changes, motion blur STP <ref type="bibr" target="#b108">[109]</ref> Randomly shifted pairs Siam-MCF <ref type="bibr" target="#b109">[110]</ref> Random cropping, color distortion, horizontal flipping, small resizing perturbations TRACA <ref type="bibr" target="#b112">[113]</ref> Blur, flip VITAL <ref type="bibr" target="#b113">[114]</ref> Randomly masks SiamRPN <ref type="bibr" target="#b115">[116]</ref> Affine transformation YCNN <ref type="bibr" target="#b134">[135]</ref> Rotation, translation, illumination variation, mosaic, salt &amp; pepper noise DeepFWDCF <ref type="bibr" target="#b143">[144]</ref> Gray-scale rotation invariant LBP histograms ORHF <ref type="bibr" target="#b146">[147]</ref> Augmentation of negative samples ATOM <ref type="bibr" target="#b148">[149]</ref> Translation, rotation, blur, dropout, flip, color jittering MAM <ref type="bibr" target="#b170">[171]</ref> All detected windows from target category DiMP50 <ref type="bibr" target="#b158">[159]</ref> Translation, rotation, blur, dropout, flip, color jittering PrDiMP50 <ref type="bibr" target="#b193">[194]</ref> Translation, rotation, blur, dropout, flip, color jittering TAAT <ref type="bibr" target="#b168">[169]</ref> Spatial &amp; temporal pairs CGACD <ref type="bibr" target="#b189">[190]</ref> RoI augmentation MLT <ref type="bibr" target="#b187">[188]</ref> Horizontal flip, noise, Gaussian blur, translation ROAM <ref type="bibr" target="#b195">[196]</ref> Stretching and scaling the images SiamRCNN <ref type="bibr" target="#b199">[200]</ref> Motion blur, gray-scale, gamma, flip, and scale augmentations TMAML <ref type="bibr" target="#b200">[201]</ref> Random scaling, shifting, zoom in/out visual trackers. However, some visual trackers <ref type="bibr" target="#b158">[159]</ref>, <ref type="bibr" target="#b193">[194]</ref> exploit meta-learning ideas to adjust their model weights during tracking, which is different from the classic metalearning definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Network Objective</head><p>For the training and inference stages, DL-based visual trackers localize the given target based on network objective function. Hence, these methods are categorized into classificationbased, regression-based, or both classification and regressionbased methods as follows. This sub-section does not include the methods that exploit deep off-the-shelf features because these methods do not design and train the networks and usually employ pre-trained DNNs for feature extraction.</p><p>1) Classification-based Objective Function: Motivated by other computer vision tasks such as image detection, classification-based visual tracking methods employ object proposal methods to produce hundreds of candidate/proposal BBs extracted from the search region. These methods aim to select the high score proposal by classifying the proposals to the target and background classes. This two-class (or binary) classification involves visual targets from various class and moving patterns and individual sequences, including challenging scenarios. Due to the main attention of these methods on inter-class classification, tracking a visual target in the presence of the same labeled targets is intensely prone to driftproblem. Also, tracking the arbitrary appearance of targets may lead to recognizing different targets with varying appearances. Therefore, the performance of the classification-based visual tracking methods is also related to their object proposal method, which usually produces a considerable number of candidate BBs. On the other side, some recent DL-based trackers utilize this objective function to take optimal actions <ref type="bibr" target="#b161">[162]</ref>, <ref type="bibr" target="#b171">[172]</ref>- <ref type="bibr" target="#b173">[174]</ref>, <ref type="bibr" target="#b173">[174]</ref>, <ref type="bibr" target="#b174">[175]</ref>, <ref type="bibr" target="#b176">[177]</ref>, <ref type="bibr" target="#b180">[181]</ref>.</p><p>2) Regression-based Objective Function: Due to the continuous instinct of estimation space of visual tracking, regression-based methods usually aim to directly localize the target in the subsequent frames by minimizing a regularized least-squares function. Generally, extensive training data are needed to train these methods effectively. The primary goal of regression-based methods is to refine the formulation of L2 or L1 loss functions, such as utilizing shrinkage loss in the learning procedure <ref type="bibr" target="#b100">[101]</ref>, modeling both regression coefficients and patch reliability to optimize a neural network efficiently <ref type="bibr" target="#b119">[120]</ref>, or applying a cost-sensitive loss to enhance unsupervised learning performance <ref type="bibr" target="#b157">[158]</ref>. Meanwhile, recent visual trackers define a loss function for BB regression (e.g., <ref type="bibr" target="#b189">[190]</ref>, <ref type="bibr" target="#b195">[196]</ref>, <ref type="bibr" target="#b201">[202]</ref>) to provide accurate localization.</p><p>3) Classification-and Regression-based Objective Function: To take advantages of both foregroundbackground/category classification and ridge regression (i.e., regularized least-squares objective function), a broad range of trackers employ both classification-and regression-based objective functions for visual tracking (see Fig. <ref type="figure" target="#fig_1">3</ref>), which their goal is to bridge the gap between the recent trackingby-detection and continuous localization process of visual tracking. These methods commonly utilize classification-based methods to find the most similar object proposal to target, and then the estimated region will be refined by a BB regression method <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b79">[80]</ref>, <ref type="bibr" target="#b93">[94]</ref>, <ref type="bibr" target="#b102">[103]</ref>- <ref type="bibr" target="#b104">[105]</ref>, <ref type="bibr" target="#b115">[116]</ref>, <ref type="bibr" target="#b129">[130]</ref>, <ref type="bibr" target="#b145">[146]</ref>, <ref type="bibr" target="#b155">[156]</ref>, <ref type="bibr" target="#b163">[164]</ref>, <ref type="bibr" target="#b168">[169]</ref>. The target regions are estimated by classification scores and optimized regression/matching functions <ref type="bibr" target="#b126">[127]</ref>, <ref type="bibr" target="#b137">[138]</ref>, <ref type="bibr" target="#b138">[139]</ref>, <ref type="bibr" target="#b148">[149]</ref>, <ref type="bibr" target="#b149">[150]</ref>, <ref type="bibr" target="#b152">[153]</ref>- <ref type="bibr" target="#b154">[155]</ref>, <ref type="bibr" target="#b158">[159]</ref>, <ref type="bibr" target="#b159">[160]</ref>, <ref type="bibr" target="#b162">[163]</ref>, <ref type="bibr" target="#b174">[175]</ref>, <ref type="bibr" target="#b211">[212]</ref> to enhance efficiency and accuracy. The classification outputs are mainly inferred for candidate proposals' confidence scores, foreground detection, candidate window response, actions, and so forth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Network Output</head><p>According to the network objective, the DL-based methods generate different network outputs to estimate or refine the estimated target location. Based on their network outputs, the DL-based methods are classified into six main categories (see Fig. <ref type="figure" target="#fig_1">3</ref> and Table <ref type="table" target="#tab_2">II</ref> to Table <ref type="table" target="#tab_4">IV</ref>), namely confidence map (also includes score map, response map, and voting map), BB (also includes rotated BB), object score (also includes the probability of object proposal, verification score, similarity score, and layer-wise score), action, feature maps, and segmentation mask. Besides template-based methods, segmentation-based trackers have been explored to boost tracking performance. Despite initial works <ref type="bibr" target="#b247">[248]</ref>- <ref type="bibr" target="#b249">[250]</ref>, employing independent deep networks for tracking &amp; VOS may lead to irretrievable tracking failures and high computational complexity. Thus, segmentation-based trackers aim to jointly track and segment visual targets by adding a segmentation branch to the network <ref type="bibr" target="#b126">[127]</ref>, <ref type="bibr" target="#b154">[155]</ref>, <ref type="bibr" target="#b191">[192]</ref>, <ref type="bibr" target="#b196">[197]</ref> or off-the-shelf BB-to-segmentation (Box2Seg) networks <ref type="bibr" target="#b250">[251]</ref>, <ref type="bibr" target="#b251">[252]</ref> to the base tracking network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Exploitation of Correlation Filters Advantages</head><p>The DCF-based methods aim to learn a set of discriminative filters that an element-wise multiplication of them with a set of training samples in the frequency domain determines spatial target location. Since DCF has provided competitive tracking performance and computational efficiency compared to sophisticated techniques, DL-based visual trackers use correlation filter advantages. These methods are categorized based on how they exploit DCF advantages by using either a whole DCF framework or some benefits, such as its objective function or correlation filters/layers. Considerable visual tracking methods are based on integrating deep features in the DCF framework (see Fig. <ref type="figure" target="#fig_1">3</ref>). These methods aim to improve the robustness of target representation against challenging attributes, while other methods attempt to benefit the computational efficiency of correlation filter(s) <ref type="bibr" target="#b85">[86]</ref>, correlation layer(s) <ref type="bibr" target="#b117">[118]</ref>, <ref type="bibr" target="#b133">[134]</ref>, <ref type="bibr" target="#b140">[141]</ref>, <ref type="bibr" target="#b156">[157]</ref>, <ref type="bibr" target="#b165">[166]</ref>, <ref type="bibr" target="#b204">[205]</ref>, and the objective function of correlation filters <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b94">[95]</ref>, <ref type="bibr" target="#b120">[121]</ref>, <ref type="bibr" target="#b148">[149]</ref>, <ref type="bibr" target="#b157">[158]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Aerial-view Tracking</head><p>By pervasive applications of flying robots, tracking from aerial views introduces extra attractive challenges, such as tiny objects, weather conditions, dense environments, long occlusions, significant viewpoint change, etc. Aerial-view trackers can be classified into class-specific &amp; class-agnostic methods. Class-specific trackers mostly focus on human or vehicle classes, such as the unified contextual relation actorcritic (CRAC) <ref type="bibr" target="#b206">[207]</ref>, a GAN-based vehicle tracker that aims to model contextual relation and transfer the ground-view features to the aerial-ones. In contrast, class-agnostic trackers can track arbitrary classes of targets. Some DCF-based trackers <ref type="bibr" target="#b205">[206]</ref>, <ref type="bibr" target="#b207">[208]</ref>- <ref type="bibr" target="#b210">[211]</ref> were the first generation of flying robot trackers, which address the inherent limitations of correlation filters (e.g., boundary effect and filter corruption) given aerial view conditions. However, the coarse-to-fine tracker (C2FT) <ref type="bibr" target="#b173">[174]</ref> employs deep RL coarse-&amp; fine-trackers (for estimating entire BB and its refinement) to address significant aspectratio change of targets from aerial-views. Finally, the contextaware IoU-guided network for small object tracking (COMET) aims to narrow the performance gap between the aerial-view trackers &amp; state-of-the-art ones. It employs an offline proposal generation strategy &amp; a multitask two-stream network to exploit context information and handle out-of-view &amp; occlusion effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Long-term Tracking</head><p>Long-term tracking performs on more realistic scenarios, including (relatively) long videos in which targets may disappear &amp; reappear. Despite the close relationship to practical applications, limited trackers have been proposed for this task. One way is the extension of a short-term tracker by various strategies. For instance, DaSiamRPN <ref type="bibr" target="#b103">[104]</ref> uses a local-toglobal search region strategy to handle out-of-view &amp; full occlusion, while LCTdeep <ref type="bibr" target="#b141">[142]</ref> utilizes a detection module with incremental updates. The FGLT <ref type="bibr" target="#b212">[213]</ref> employs both MDNet <ref type="bibr" target="#b59">[60]</ref> &amp; SiamRPN++ <ref type="bibr" target="#b155">[156]</ref> trackers for the tracking result judgment, and a detection module modifies tracking failures. The memory model via the Siamese network for longterm tracking (MMLT) <ref type="bibr" target="#b106">[107]</ref> modifies the SiamFC tracker <ref type="bibr" target="#b57">[58]</ref> by re-detection and memory management parts. Moreover, the improved Siamese tracker (i-Siam) <ref type="bibr" target="#b214">[215]</ref> revisits the SiamFC tracker via a negative signal suppression approach &amp; a diverse multi-template one. The multi-level CF-based tracker <ref type="bibr" target="#b203">[204]</ref> employs an oriented re-detection technique, while MetaUpdater <ref type="bibr" target="#b216">[217]</ref> exploits a SiamRPN-based re-detector and an online verifier with a meta-updater. Finally, the SPLT tracker <ref type="bibr" target="#b217">[218]</ref> is based on SiamRPN <ref type="bibr" target="#b115">[116]</ref> and comprises perusal and skimming modules for local tracking &amp; search window selection.</p><p>On the other hand, various long-term trackers have been inspired by successful detection methods. For instance, Glob-alTrack <ref type="bibr" target="#b213">[214]</ref> is a two-stage tracker, including a query-guided region proposal network &amp; query-guided region CNN to generate object candidates and produce the final predictions, respectively. Also, the LRVN tracker <ref type="bibr" target="#b215">[216]</ref> consists of the combination of an offline-learned regression network with an online-updated verification network to generate target candidates &amp; evaluate/update them on reliable observations. Lastly, the SiamRCNN tracker <ref type="bibr" target="#b199">[200]</ref> introduces a hard example mining procedure and tracklet dynamic programming algorithm to detect potential distractors &amp; select the best target at each time-step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Online Tracking</head><p>While initial DL-based methods had focused on their performance, recent trackers aim to be accurate, robust, and efficient simultaneously. According to Fig. <ref type="figure" target="#fig_1">3</ref>, a wide variety of algorithms are classified as online trackers regarding different hardware implementations (Table <ref type="table" target="#tab_1">I</ref>-Table <ref type="table" target="#tab_4">IV</ref>). Most of these trackers (e.g., <ref type="bibr" target="#b56">[57]</ref>- <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b115">[116]</ref>, <ref type="bibr" target="#b155">[156]</ref>) are based on offlinetrained SNNs that do not update the target model (i.e., initial frame) during tracking. Some deep DCF-based trackers (e.g., <ref type="bibr" target="#b114">[115]</ref>, <ref type="bibr" target="#b147">[148]</ref>, <ref type="bibr" target="#b162">[163]</ref>, <ref type="bibr" target="#b204">[205]</ref>) exploit efficient optimizations &amp; computations in the frequency domain, although employing pre-trained networks limits their speeds. Lately, custom-based trackers (e.g., <ref type="bibr" target="#b148">[149]</ref>, <ref type="bibr" target="#b158">[159]</ref>, <ref type="bibr" target="#b191">[192]</ref>, <ref type="bibr" target="#b193">[194]</ref>, <ref type="bibr" target="#b200">[201]</ref>, <ref type="bibr" target="#b211">[212]</ref>) employ shallow networks with robust optimization strategies to attain high-speed tracking. Finally, numerous techniques have been used to speed up DL-based trackers (e.g., learning-based search strategy <ref type="bibr" target="#b102">[103]</ref>, domain adaption <ref type="bibr" target="#b200">[201]</ref>, <ref type="bibr" target="#b206">[207]</ref>, embedding space learning <ref type="bibr" target="#b104">[105]</ref>, collaborative framework <ref type="bibr" target="#b130">[131]</ref>, offline-trained CNN <ref type="bibr" target="#b134">[135]</ref>, and efficient updating &amp; scale estimation strategies <ref type="bibr" target="#b72">[73]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. VISUAL TRACKING BENCHMARK DATASETS</head><p>Visual tracking benchmark datasets have been introduced to provide fair and standardized evaluations of single-object tracking algorithms. These benchmarks are mainly categorized based on generic or aerial-view tracking applications while providing short-or long-term scenarios. These datasets contain various sequences, frames, attributes, and classes (or clusters). The attributes include illumination variation (IV), scale variation (SV), occlusion (OCC), deformation (DEF), motion blur (MB), fast motion (FM), in-plane rotation (IPR), out-of-plane rotation (OPR), out-of-view (OV), background clutter (BC), low resolution (LR), aspect ratio change (ARC), camera motion (CM), full occlusion (FOC), partial occlusion (POC), similar object (SIB), viewpoint change (VC), light (LI), surface cover (SC), specularity (SP), transparency (TR), shape (SH), motion smoothness (MS), motion coherence (MCO), confusion (CON), low contrast (LC), zooming camera (ZC), long duration (LD), shadow change (SHC), flash (FL), dim light (DL), camera shaking (CS), rotation (ROT), fast background change (FBC), motion change (MOC), object color change (OCO), scene complexity (SCO), absolute motion (AM), size (SZ), relative speed (RS), distractors (DI), length (LE), fast camera motion (FCM), object motion (OM), object blur (OB), large occlusion (LOC), small objects (SOB), occlusion with background clutter (O-B), occlusion with rotation (O-R) and long-term tracking (LT). Table VI compares the applications, scenarios, characteristics, missing labeled data for unsupervised training, and the overlap of single object tracking datasets. By different evaluation protocols, existing visual tracking benchmarks assess the accuracy &amp; robustness of trackers in realistic scenarios. The homogenized evaluation protocols facilitate straightforward comparison and development of visual trackers. Below the most popular visual tracking benchmark datasets and evaluation metrics are briefly described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Short-term Tracking Datasets</head><p>Generic Object Tracking. As one of the first object tracking benchmarks, OTB2013 <ref type="bibr" target="#b218">[219]</ref> is developed by fully annotated video sequences to address the issues of reported tracking results based on a few video sequences or inconsistent initial conditions or parameters. The OTB2015 <ref type="bibr" target="#b219">[220]</ref> is an extended OTB2013 dataset with the aim of unbiased performance comparisons. To compare the performance of visual trackers on color sequences, the Temple-Color 128 (TColor128 or TC128) <ref type="bibr" target="#b224">[225]</ref> collected a set of 129 fully annotated video sequences that 78 ones are different from the OTB datasets. The Amsterdam library of ordinary videos (ALOV) dataset <ref type="bibr" target="#b41">[42]</ref> has been gathered to cover diverse video sequences and attributes. By emphasizing challenging visual tracking scenarios, the ALOV dataset comprises 304 assorted short videos and 10 longer ones. The video sequences are chosen from real-life YouTube videos and have 13 difficulty degrees. The videos of ALOV have been categorized according to one of its attributes (Table <ref type="table" target="#tab_6">VI</ref>), although in the OTB datasets, each video has been annotated by several visual attributes.</p><p>Motivated by the large dataset's inequality with a useful one, the VOT dataset <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b39">[40]</ref> aims to provide a diverse and sufficiently small dataset from existing ones, annotated per-frame by rotatable BBs and visual properties. To evaluate different visual tracking methods fast and straightforward, the VOT includes a visual tracking exchange (TraX) protocol <ref type="bibr" target="#b252">[253]</ref> that not only prepares data, runs experiments, and performs analyses but also can detect tracking failures (i.e., losing the target) and re-initialize the tracker after each failure to assess tracking robustness. Despite some small and saturated tracking datasets in the wild, mostly provided for object detection tasks, the large-scale TrackingNet benchmark dataset <ref type="bibr" target="#b228">[229]</ref> has been proposed to properly feed deep visual trackers. It provides videos for tracking in the wild with 500 original videos, more than 14 million upright BB annotations, densely annotated data in time, rich distribution of object classes, and real-world scenarios by sampled YouTube videos.</p><p>For tracking pedestrian and rigid objects, the NUS people and rigid objects (NUS-PRO) dataset <ref type="bibr" target="#b225">[226]</ref> has been provided 365 video sequences from YouTube that are majorly captured by moving cameras and annotated the level of occluded objects of each frame with no occlusion, partial occlusion, and full occlusion labels. By higher frame rate (240 FPS) cameras, the need for speed (NfS) dataset <ref type="bibr" target="#b226">[227]</ref> provides video sequences from real-world scenarios to systematically investigate trade-off bandwidth constraints related to real-time analysis of visual trackers. These videos are either recorded by hand-held iPhone/iPad cameras or from YouTube videos. Two short sequence datasets, namely TinyTLP &amp; TLPattr <ref type="bibr" target="#b232">[233]</ref>, are derived from the long-term TLP dataset. For each sequence, one visual attribute has been specified for investigating various challenges. The large high-diversity dataset, called GOT-10k <ref type="bibr" target="#b231">[232]</ref>, includes more than ten thousand videos classified into 563 classes of moving objects and 87 classes of motion to cover as many challenging patterns in real-world scenarios as possible. The GOT-10k has informative continuous attributes, including absent labels, which show the target does not exist in the frame. Finally, the TracKlinic <ref type="bibr" target="#b233">[234]</ref>  Aerial View Object Tracking. Tracking from aerial views has been developed in recent years, considering the broad range of applications. The unmanned aerial vehicle 123 (UAV123) <ref type="bibr" target="#b221">[222]</ref> provides a sparse and low altitude aerialview tracking dataset that contains the realistic and synthetic HD video sequences captured by professional-grade flying robots, a board-cam mounted on small low-cost flying robots &amp; simulator ones. Drone tracking benchmark (DTB) <ref type="bibr" target="#b227">[228]</ref> is a dataset captured by flying robots or drones that consists of RGB videos with massive displacement of target location due to abrupt camera motion. The BUAA-PRO dataset <ref type="bibr" target="#b230">[231]</ref> is a segmentation-based benchmark dataset to address the problem of inevitable non-target elements in BBs. It exploits the segmentation mask-based version of a level-based occlusion attribute. The UAVDT dataset <ref type="bibr" target="#b222">[223]</ref> provides an aerialview dataset with high object density scenarios (e.g., different weather conditions, camera views, flying altitudes) focusing on pedestrians and vehicles. Furthermore, VisDrone dataset <ref type="bibr" target="#b223">[224]</ref>, <ref type="bibr" target="#b235">[236]</ref> includes videos captured by different drone platforms in real-world scenarios. For small object tracking, the Small-90 dataset <ref type="bibr" target="#b236">[237]</ref> presents aerial videos mostly collected from other visual tracking datasets. By adding 22 more challenging sequences, the Small-112 dataset <ref type="bibr" target="#b236">[237]</ref> has been formed based on the Small-90 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Long-term Tracking Datasets</head><p>Generic Object Tracking. With the aim of long-term tracking of frequently disappearance targets, the OxUvA dataset <ref type="bibr" target="#b229">[230]</ref> includes 14 hours of videos from YouTube-BoundingBoxes (or YTBB) <ref type="bibr" target="#b243">[244]</ref> to provide development and test sets with continuous attributes. Also, it provides absent labels, which show that the target does not exist in some frames. The TLP dataset <ref type="bibr" target="#b232">[233]</ref> also has been collected highresolution videos with a longer duration per sequence from YouTube, which provides the possibility of studying tracking consistency. However, target disappearances do not frequently occur in the TLP dataset. Hence, the LTB-35 <ref type="bibr" target="#b234">[235]</ref> presents an enriched long-term dataset with consistent target disappearances (twelve disappearances on average for each video). The large-scale single object tracking (LaSOT) <ref type="bibr" target="#b220">[221]</ref> has been developed to address the problems of existing datasets, such as small scale, lack of high-quality, dense annotations, short video sequences, and category bias. The object categories are from the ImageNet and a few visual tracking applications (such as drones) with an equal number of videos per category. The training and testing subsets include 1120 (2.3M frames) and 280 (690K frames) video sequences, respectively. Aerial View Object Tracking. As the parent set of the UAV-123 dataset, the UAV20L is an aerial surveillance dataset, including one continuous shot video sequences. It consists of tolerable occlusions and provides difficult scenarios for small object tracking. Moreover, the VisDrone-2019/2020L dataset <ref type="bibr" target="#b223">[224]</ref> includes 25 challenging sequences (i.e., 12/13 videos in the day/night) with tiny targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Metrics</head><p>Visual trackers are evaluated by two fundamental evaluation categories of performance measures and performance plots to perform experimental comparisons on large-scale datasets. These metrics are briefly described as follows.</p><p>1) Performance Measures: Performance measures attempt to intuitively interpret performance comparisons in terms of complementary metrics of accuracy, robustness and tracking speed. For long-term trackers, the measures close relate to their detection counterparts to reflect re-detection &amp; target absence prediction capabilities. In the following, these measures are concisely investigated.</p><p>(A) Short-term Tracking Measures:</p><p>(i) Center location error (CLE)/ (Normalized) precision: The CLE or precision metric is defined as the average Euclidean distance between the precise ground-truth locations of the target and estimated locations by a visual tracker. The CLE is the oldest metric that is sensitive to dataset annotation and does not consider tracking failures, and ignores the target's BB, resulting in significant errors. The normalized precision <ref type="bibr" target="#b228">[229]</ref> aims to relieve the sensitivity of CLE to the size of BBs and frame resolutions. Given the size of ground-truth BB (b g ), this metric normalizes the CLE over b g to keep its consistency for various target scales.</p><p>(ii) Accuracy: For this metric, first, the overlap score is calculated as S = |bt∩bg| |bt∪bg| which b g , b t , ∩, ∪ and |.| represent the ground-truth BB, an estimated BB by a visual tracking method, intersection operator, union operator, and the number of pixels in the resulted region, respectively. By considering a certain threshold, the overlap score indicates a visual tracker's success in one frame. The accuracy is then calculated by the average overlap scores (AOS) during the tracking when a visual tracker's estimations have overlap with the ground-truth ones. This metric jointly considers both location and region to measure the estimated target's drift rate up to its failure.</p><p>(iii) Robustness/ failure score: The robustness or failure score is defined as the number of required re-initializations when a tracker loses (or drifts) the target during the tracking task. The failure is detected when the overlap score drops to zero.</p><p>(iv) Expected average overlap (EAO): This score is interpreted as the combination of accuracy and robustness scores. Given N s frames long sequences, the EAO score is calculated as Φ Ns = 1 Ns Ns i=1 Φ i , where Φ i is defined as the average of per-frame overlaps until the end of sequences, even if failure leads to zero overlaps.</p><p>(v) Area under curve (AUC): The AUC score has defined the average success rates (normalized between 0 and 1) according to the pre-defined thresholds. To rank the visual tracking methods based on their overall performance, the AUC score summarizes the AOS of visual tracking methods across a sequence. (B) Long-term Tracking Measures:</p><p>(i) Precision (P r): Tracking measures for long-term trackers depend on being a target in the scene and prediction confidence to be higher than a classification threshold for each frame. The precision <ref type="bibr" target="#b234">[235]</ref> is calculated by the intersection over union (IoU) between the ground-truth (b g ) and predicted target (b t ), normalized by the number of frames with existing predictions. The integration of these scores over all precision thresholds provides the overall tracking precision.</p><p>(ii) Recall (Re): Similar to the precision, it calculates the IoU between the b g and b t , which is normalized by the number of frames with no absent targets. The overall tracking recall <ref type="bibr" target="#b234">[235]</ref> is achieved by integrating the scores over all recall thresholds.</p><p>(iii) F-score: It compromises the precision &amp; recall scores by calculating F = 2P r.Re P r+Re to rank the trackers according to their maximum values over all thresholds.</p><p>(iv) Maximum Geometric Mean (MaxGM): Inspired by binary classification, the MaxGM employs the true positive rate (TPR) and true negative rate (TNR) for evaluation of trackers. While the TPR reports the fraction of correctly located targets, the TNR presents the fraction of correctly reported absent targets. As a single metric, the geometric mean is defined as GM = √ T P R.T N R but it will be zero for the trackers that cannot predict absent targets. Hence, M axGM = max 0≤p≤1 {(1 -p).T P R)}{(1 -p).T N R + p} provides a more informative comparison in terms of various probabilistic thresholds p.</p><p>2) Performance Plots: Generally, visual trackers are analyzed in terms of various thresholds to provide more intuitive quantitative comparisons. These metrics are summarized as follows.</p><p>(A) Short-term Tracking Plots:</p><p>(i) Precision plot: Given the CLEs per different thresholds, the precision plot shows the percentage of video frames in which the estimated locations have at most the specific threshold with the ground-truth locations.</p><p>(ii) Success plot: Given the calculated various accuracy per thresholds, the success plot measures the percentage of frames in which the estimated overlaps and the ground-truth ones have larger overlap than a certain threshold.</p><p>(iii) Expected average overlap curve: For an individual length of video sequences, the expected average overlap curve has resulted from the range of values in a specific interval [N lo , N hi ] as Φ = (ii) F-score plot: This is the main curve to rank the tracking methods based on the highest score on the plot.</p><p>IV. EXPERIMENTAL ANALYSES To analyze the performance of state-of-the-art visual tracking methods, 48 different methods are quantitatively compared on seven well-known tracking datasets, namely OTB2013 <ref type="bibr" target="#b218">[219]</ref>, OTB2015 <ref type="bibr" target="#b219">[220]</ref>, VOT2018 <ref type="bibr" target="#b38">[39]</ref>, LaSOT <ref type="bibr" target="#b220">[221]</ref>, UAV-123 <ref type="bibr" target="#b221">[222]</ref>, UAVDT <ref type="bibr" target="#b222">[223]</ref>, and VisDrone2019test-dev <ref type="bibr" target="#b223">[224]</ref>. Due to the page limitation, all experimental results are publicly available on https://github.com/MMarvasti/ Deep-Learning-for-Visual-Tracking-Survey. The included 48 DL-based trackers in the experiments are shown in Table <ref type="table" target="#tab_7">VII</ref>. All evaluations are performed on an Intel I7-9700K 3.60GHz CPU with 32GB RAM with the aid of MatConvNet toolbox <ref type="bibr" target="#b253">[254]</ref> that uses an NVIDIA GeForce RTX 2080Ti GPU for its computations. The OTB, LaSOT, UAV123, UAVDT, and VisDrone2019 toolkits evaluate the visual trackers in terms of the well-known precision &amp; success plots and then rank the methods based on the AUC score. For performance comparison on the VOT2018 dataset, the visual trackers have been assessed based on the TraX evaluation protocol using three primary measures of accuracy, robustness, and EAO to provide the Accuracy-Robustness (AR) plots, expected average overlap curve, and ordering plots according to its five challenging visual attributes <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Quantitative Comparisons</head><p>According to the results shown in Fig. <ref type="figure" target="#fig_4">4</ref>, the top-3 visual tracking methods in terms of the precision metric are the VITAL, MDNet, and DAT on the OTB2013 dataset, the SiamDW-SiamRPN, ASRCF, and VITAL on the OTB2015 dataset, and the PrDiMP50, DiMP50, and ATOM on the LaSOT dataset, respectively. In terms of success metric, the ASRCF, VITAL, and MDNet on the OTB2013 dataset, the SiamRPN++, SANet, and ASRCF on the OTB2015 dataset, and the PrDiMP50, DiMP50, and ATOM on the LaSOT dataset have achieved the best performance, respectively. On the VOT2018 dataset (see Fig. <ref type="figure" target="#fig_5">5</ref>), the top-3 visual trackers are the SiamMask, SiamRPN++, and DiMP50 in terms of accuracy measure while the PrDiMP50, DiMP50, and ATOM trackers have the best robustness, respectively. For the aerialview tracking, the PrDiMP50, DiMP50, SiamRPN++, and SiamMask have provided the best results for average precision &amp; success metrics.</p><p>On the other hand, the best trackers based on both precisionsuccess measures (see Fig. <ref type="figure" target="#fig_4">4</ref>) are the VITAL, MDNet, and AS-RCF on the OTB2013 dataset, the SiamRPN++, ASRCF, and VITAL on the OTB2015 dataset, the PrDiMP50, DiMP50, and ATOM on the LaSOT dataset, and the PrDiMP50, DiMP50, and SiamRPN++ on the aerial-view datasets (i.e., the UAV123, UAVDT, and VisDrone2019). On the VOT2018 dataset, the DiMP50, SiamRPN++, and ATOM are the best performing trackers based on the EAO score. Moreover, the PrDiMP50,  DiMP50, SiamRPN++, and ATOM have achieved the best AUC scores while the SiamRPN, SiamRPN++, and CFNet are the fastest visual trackers, respectively. According to the results (i.e., Fig. <ref type="figure" target="#fig_4">4</ref>, and Fig. <ref type="figure" target="#fig_5">5</ref>), the best visual tracking methods that repeated the best results on different tracking datasets are the PrDiMP50 <ref type="bibr" target="#b193">[194]</ref>, DiMP50 <ref type="bibr" target="#b158">[159]</ref>, ATOM <ref type="bibr" target="#b148">[149]</ref>, VITAL <ref type="bibr" target="#b113">[114]</ref>, MDNet <ref type="bibr" target="#b59">[60]</ref>, DAT <ref type="bibr" target="#b129">[130]</ref>, ASRCF <ref type="bibr" target="#b147">[148]</ref>, SiamDW-SiamRPN <ref type="bibr" target="#b153">[154]</ref>, SiamRPN++ <ref type="bibr" target="#b155">[156]</ref>, C-RPN <ref type="bibr" target="#b149">[150]</ref>, StructSiam <ref type="bibr" target="#b149">[150]</ref>, SiamMask <ref type="bibr" target="#b154">[155]</ref>, DaSiamRPN <ref type="bibr" target="#b103">[104]</ref>, UPDT <ref type="bibr" target="#b101">[102]</ref>, LSART <ref type="bibr" target="#b119">[120]</ref>, DeepSTRCF <ref type="bibr" target="#b114">[115]</ref>, and DRT <ref type="bibr" target="#b118">[119]</ref>. These methods will be investigated in Sec. IV-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Most Challenging Attributes per Benchmark Dataset</head><p>Following on the VOT challenges <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b38">[39]</ref>, which have specified the most challenging visual tracking attributes, this work also introduces the most challenging attributes on the OTB, LaSOT, UAV123, UAVDT, and VisDrone datasets. These attributes are determined by the median accuracy &amp; robustness per attribute on the VOT or the median precision &amp; success per attribute on other datasets. Table <ref type="table" target="#tab_8">VIII</ref> shows the most challenging attributes for each benchmark dataset. The OCC/LOC, OV, FM, DEF, LR, ARC, and SIB are selected as the most challenging attributes that can effectively impact the performance of DL-based visual trackers. Fig. <ref type="figure">6</ref> compares the performances of these methods on the most challenging attributes on the OTB2015, LaSOT, UAV123, UAVDT, and VisDrone2019 datasets.</p><p>According to the OCC attribute, the most accurate &amp; robust visual trackers on the VOT2018 dataset are the SiamRPN++ <ref type="bibr" target="#b155">[156]</ref> and DRT <ref type="bibr" target="#b118">[119]</ref>, respectively. In terms of success metric, the SiamRPN++ <ref type="bibr" target="#b155">[156]</ref> is the best visual tracker to tackle the DEF and OV attributes, while the Siam-MCF <ref type="bibr" target="#b109">[110]</ref> is the best one to deal with the visual tracking in LR videos on the OTB2015 dataset. The ASRCF <ref type="bibr" target="#b147">[148]</ref>, ECO <ref type="bibr" target="#b86">[87]</ref>, and SiamDW-SiamRPN <ref type="bibr" target="#b153">[154]</ref> are the best trackers in precision metric to face with OV, OCC, and DEF attributes on the OTB-2015 dataset. The PrDiMP50 <ref type="bibr" target="#b193">[194]</ref>, DiMP50 <ref type="bibr" target="#b158">[159]</ref>, and ATOM <ref type="bibr" target="#b148">[149]</ref> trackers are the absolute best methods on the LaSOT dataset on all visual attributes. On the UAV123 dataset, the ATOM and DiMP50 have achieved the best results in terms of precision and success metrics, respectively. Also, the PrDiMP50 is the best tracker for handling large occlusions on the UAVDT dataset. Finally, the SiamDW is the best tracker to tackle similar objects on the VisDrone2019-test-dev dataset.</p><p>As shown in Fig. <ref type="figure" target="#fig_5">5</ref>, the DCF-based methods have achieved fewer failures among the other methods, while the SNN-&amp; custom-network based trackers have gained more over- lap between the estimated BBs and ground-truth ones. The SiamRPN-based methods (i.e., <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b153">[154]</ref>- <ref type="bibr" target="#b155">[156]</ref>) accurately handle scenarios under each of CM, IV, MC, OCC, or SC attributes by adopting deeper and wider backbone networks, including classification and regression branches. Moreover, the ATOM, DiMP50, and PrDiMP50 exploit powerful classification &amp; regression networks and optimization processes for online training and fast adaptation. Thus, these trackers have provided significant advances on various tracking benchmarks. By considering the fusion of hand-crafted and deep features <ref type="bibr" target="#b101">[102]</ref>, <ref type="bibr" target="#b114">[115]</ref>, <ref type="bibr" target="#b118">[119]</ref>, temporal regularization term <ref type="bibr" target="#b114">[115]</ref>, reliability term <ref type="bibr" target="#b118">[119]</ref>, data augmentation <ref type="bibr" target="#b101">[102]</ref>, and exploitation of ResNet-50 model <ref type="bibr" target="#b101">[102]</ref>, the DCF-based methods have attained desirable robustness against CM attribute. Furthermore, the computational efficiency and the robustness of DCF-based trackers are attractive for aerial-view trackers.</p><p>To effectively deal with the IV attribute, focusing on the discrimination power between the target and its background is the main problem. The strategies such as training a fully convolutional network for correlation filter cost function, spatialaware KRR and spatial-aware CNN, and employing semisupervised video object segmentation improve the robustness of DL-based trackers when significant IV occurs. To robustly deal with MC and OCC attributes, the DCF-and CNNbased trackers have performed the best. However, the SNNbased methods with the aid of region proposal subnetwork and proposal refinement can robustly estimate the tightest BB under severe scale changes. However, recently, the IoU-based refinement network (based on IoU-Net <ref type="bibr" target="#b254">[255]</ref>) employed in ATOM, DiMP, and PrDiMP trackers can effectively handle aspect-ratio change of target during tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discussion</head><p>The overall best methods (i.e., PrDiMP50 <ref type="bibr" target="#b193">[194]</ref>, DiMP50 <ref type="bibr" target="#b158">[159]</ref>, ATOM <ref type="bibr" target="#b148">[149]</ref>, VITAL <ref type="bibr" target="#b113">[114]</ref>, MDNet <ref type="bibr" target="#b59">[60]</ref>, DAT <ref type="bibr" target="#b129">[130]</ref>, ASRCF <ref type="bibr" target="#b147">[148]</ref>, SiamDW-SiamRPN <ref type="bibr" target="#b153">[154]</ref>, SiamRPN++ <ref type="bibr" target="#b155">[156]</ref>, C-RPN <ref type="bibr" target="#b149">[150]</ref>, StructSiam <ref type="bibr" target="#b105">[106]</ref>, SiamMask <ref type="bibr" target="#b154">[155]</ref>, DaSi-amRPN <ref type="bibr" target="#b103">[104]</ref>, UPDT <ref type="bibr" target="#b101">[102]</ref>, LSART <ref type="bibr" target="#b119">[120]</ref>, DeepSTRCF <ref type="bibr" target="#b114">[115]</ref>, and DRT <ref type="bibr" target="#b118">[119]</ref>) belong to a wide range of network architectures. For instance, the MDNet, LSART, and DAT (uses the MDNet architecture) utilize CNNs to localize a visual target while the ASRCF, UPDT, DRT, and DeepSTRCF exploit deep off-the-shelf features. All the ATOM, DiMP, and PrDiMP trackers employ custom classification &amp; refinement networks. Besides the VITAL that is a GAN-based tracker, the C-RPN, StructSiam, SiamMask, DaSiamRPN, SiamDW, and SiamRPN++ have the SNN architecture. Although the most recent attractive deep architectures for visual tracking are based on Siamese or custom networks, GAN-and RL-based trackers have been recently developed for some specific purposes, such as addressing the imbalance distribution of training samples <ref type="bibr" target="#b113">[114]</ref> or selecting an appropriate real-time search strategy <ref type="bibr" target="#b102">[103]</ref>, <ref type="bibr" target="#b161">[162]</ref>. GAN-based trackers can successfully augment positive samples to enrich the target appearance model. These trackers also enjoy cost-sensitive losses to focus on hard negative samples. RL-based trackers learn continuous actions to provide more reliable search &amp; verification strategies for visual trackers. Besides, the combinations of RL-trackers with other architectures may add more advantages; for instance, recurrent RL-based tracking considers time dependencies to the key components (i.e., actions &amp; states). By doing so, these trackers boost their performance by verifying confidence through an RNN motion model.</p><p>In Although diversified backbone networks are employed for these methods, state-of-the-art methods have been leveraging deeper networks such as the ResNet-50 to strengthen the discriminative power of target modeling. From the network training perspective, the SiamDW-SiamRPN, SiamRPN++, C-RPN, StructSiam, SiamMask, and DaSiamRPN use offline training, the LSART utilizes online training, and the PrDiMP50, DiMP50, and ATOM take offline &amp; online training procedures. In particular, the PrDiMP50 and DiMP50 exploit metalearning based networks to improve network adaptation for the tracking task. The offline trained trackers aim to provide dominant representations to achieve real-time tracking speed. Handling significant appearance variations needs to adapt to network parameters during tracking, but online training has an over-fitting risk because of limited training samples. Hence, the VITAL, MDNet, and DAT by employing adversarial learning, domain-independent information, and attention maps as regularization terms benefit both offline and online training of DNNs. However, these methods provide a tracking speed of about one frame per second (FPS) that is not suitable for real-time applications. In contrast, recent proposed PrDiMP50, DiMP50, and ATOM trackers exploit custom-designed networks, efficient optimization strategies to achieve acceptable tracking speed. From the perspective of the objective function of DNNs, the VITAL and StructSiam are classification-based, the LSART is regression-based, and the other best-performing trackers <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b129">[130]</ref>, <ref type="bibr" target="#b148">[149]</ref>, <ref type="bibr" target="#b149">[150]</ref>, <ref type="bibr" target="#b153">[154]</ref>- <ref type="bibr" target="#b155">[156]</ref>, <ref type="bibr" target="#b158">[159]</ref>, <ref type="bibr" target="#b193">[194]</ref> employ both classification and regression objectives. For instance, five modified versions of the SiamRPN <ref type="bibr" target="#b115">[116]</ref> (i.e., SiamDW-SiamRPN <ref type="bibr" target="#b153">[154]</ref>, SiamRPN++ <ref type="bibr" target="#b155">[156]</ref>, C-RPN <ref type="bibr" target="#b149">[150]</ref>, SiamMask <ref type="bibr" target="#b154">[155]</ref>, and DaSiamRPN <ref type="bibr" target="#b103">[104]</ref>) have two branches for classification and regression. Besides, the ATOM, DiMP50, and PrDiMP50 use a classification network for distinguishing target from the background and an IoU-Net for BB regression.</p><p>Based on the motivation categorization of the best trackers, the recent advanced methods rely on 1) alleviating the imbalanced distribution of visual training data by the data augmentation <ref type="bibr" target="#b101">[102]</ref>, <ref type="bibr" target="#b103">[104]</ref> and generative network from adversarial learning <ref type="bibr" target="#b113">[114]</ref>, 2) efficient training and learning procedures by reformulating classification/regression problems <ref type="bibr" target="#b101">[102]</ref>, <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b113">[114]</ref>, <ref type="bibr" target="#b114">[115]</ref>, <ref type="bibr" target="#b118">[119]</ref>, <ref type="bibr" target="#b119">[120]</ref>, <ref type="bibr" target="#b147">[148]</ref>, <ref type="bibr" target="#b148">[149]</ref>, <ref type="bibr" target="#b158">[159]</ref>, <ref type="bibr" target="#b193">[194]</ref> and providing specified features for visual tracking <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b105">[106]</ref>, <ref type="bibr" target="#b113">[114]</ref>, <ref type="bibr" target="#b129">[130]</ref>, <ref type="bibr" target="#b148">[149]</ref>, <ref type="bibr" target="#b149">[150]</ref>, <ref type="bibr" target="#b153">[154]</ref>- <ref type="bibr" target="#b155">[156]</ref>, <ref type="bibr" target="#b158">[159]</ref>, <ref type="bibr" target="#b193">[194]</ref>, 3) exploiting state-of-the-art architectures to provide more discriminative representations by leveraging ResNet models as the backbone networks <ref type="bibr" target="#b101">[102]</ref>, <ref type="bibr" target="#b148">[149]</ref>, <ref type="bibr" target="#b153">[154]</ref>- <ref type="bibr" target="#b155">[156]</ref>, <ref type="bibr" target="#b158">[159]</ref>, <ref type="bibr" target="#b193">[194]</ref>, and 4) extracting complementary features by employing additional information such as contextual <ref type="bibr" target="#b101">[102]</ref>, <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b105">[106]</ref>, <ref type="bibr" target="#b153">[154]</ref> or temporal information <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b113">[114]</ref>, <ref type="bibr" target="#b114">[115]</ref>, <ref type="bibr" target="#b129">[130]</ref>. The VITAL, DaSiamRPN, and UPDT attempt to alleviate the imbalanced distribution of positive and negative training data samples and extract more discriminative features. The VITAL uses adversarial learning to augment positive samples and decrease simple negative ones and preserve the most discriminative and robust features during tracking. Furthermore, the DaSiamRPN utilizes both data augmentation and negative semantic samples to consider visual distractors and improve visual tracking robustness. The UPDT uses standard data augmentation and a quality measure for estimated states to fuse shallow and deep features effectively. Finally, the ATOM employs standard data augmentation to improve its online adaptation, while the DiMP50 &amp; PrDiMP50 trackers enjoy meta-learning strategies to form their training set.</p><p>To improve the learning process of the best DL-based methods, the ATOM, UPDT, DeepSTRCF, DRT, LSART, and ASRCF have revised the conventional ridge regression of DCF formulation. Moreover, the DaSiamRPN and VITAL utilize the distractor-aware objective function and reformulated objective function of GANs using a cost-sensitive loss to improve the training process of these visual trackers, respectively. Finally, the PrDiMP tracker computes the similarity of predictive and ground-truth distributions by Kullback-Leibler (KL) divergence. Training of DL-based methods on large-scale datasets adapts their network function for visual tracking. The SiamDW, SiamRPN++, and SiamMask methods have aimed to leverage state-of-the-art deep networks as a backbone network of Siamese trackers. The ATOM, DiMP50, and PrDiMP tracker employ ResNet blocks as the backbone network, while the DiMP &amp; PrDiMP train these blocks on tracking datasets. While these methods exploit ResNet models, the SiamDW proposes new residual modules and architectures to prevent significant receptive field increase and simultaneously improve feature discriminability and localization accuracy. Also, the ResNet-driven SNN-based tracker proposed by the SiamRPN++ includes different layerwise and depth-wise aggregations to fill the performance gap between SNN-based and CNN-based methods. In addition to the spatial information, the DAT (using reciprocative learning) and DeepSTRCF (using online passive-aggressive (PA) learning) also consider temporal information in different ways to provide more robust features. Generally, six learning schemes of the similarity learning (i.e., SiamDW, SiamRPN++, C-RPN, StructSiam, SiamMask, DaSiamRPN, ATOM), bluemeta-learning (i.e., PrDiMP50, DiMP50), multidomain learning (i.e., MDNet, DAT), adversarial learning (i.e., VITAL), spatial-aware regressions learning (i.e., LSART), and DCF learning are utilized.</p><p>In the following, the best visual tracking methods are studied based on their advantages and disadvantage. The ATOM, DiMP, and PrDiMP trackers consider visual tracking as twostep classification and target estimation procedures. These trackers are robust to handle CM, MC, SV, and ARC attributes by employing custom networks and elaborated optimization strategies. However, the SOB, LR, and OB attributes can dramatically impact their performances. Three SNN-based methods of the C-RPN, StructSiam, and DaSiamRPN exploit the shallow AlexNet as their backbone network (see Table <ref type="table" target="#tab_2">II</ref>), which is the main weakness of these trackers according to their discriminative power. To improve tracking robustness in the presence of significant SV and visual DI, the C-RPN cascades multiple RPNs in a Siamese network to exploit from hard negative sampling (to provide more balanced training samples), multi-level features, and multiple steps of regressions. To decrease the sensitivity of SNN-based methods specifically for non-rigid appearance change and POC attributes, the Struct-Siam detects contextual information of local patterns and their relationships and matches them by a Siamese network in realtime speed. By adopting the local-to-global search strategy and the non-maximum suppression (NMS) to re-detect target and reduce potential distractors, the DaSiamRPN correctly handles the FOC, OV, POC, and BC challenges. In contrast, the SiamMask, SiamDW-SiamRPN, and SiamRPN++ exploit the ResNet models. To rely on rich target representation, the SiamMask uses three-branch architecture to estimate the target location by a rotated BB, including the target's binary mask. The most failure reasons for SiamMask are the MB &amp; OV attributes that produce erroneous target masks. To reduce the performance margin of the SNN-based methods with stateof-the-art visual tracking methods, the SiamDW-SiamRPN and SiamRPN++ study the exploitation of deep backbone networks to reduce the sensitivity of these methods to the most challenging attributes.</p><p>The MDNet and the other methods based on it (e.g., DAT) are still among the best visual tracking methods. Because of specialized offline and online training of these networks on large-scale visual tracking datasets, these methods can handle various challenging situations, hardly miss the visual targets, and have a satisfactory performance to track LR targets. However, these methods suffer from high computational complexity, intra-class discrimination of targets with similar semantics, and performing discrete space for scale estimation. The VITAL can tolerate massive DEF, IPR, and OPR because it focuses on hard negative samples through high-order costsensitive loss. However, it does not have a robust performance in the case of significant SV due to the producing a fixed size of weight mask via a generative network. The LSART utilizes the modified Kernelized ridge regression (KRR) by the weighted combination of patch-wise similarities to concentrate on the target's reliable regions. Due to the consideration of rotation information and online adaptation of CNN models, this method provides promising responses to tackle the DEF and IPR challenges.</p><p>The DeepSTRCF, ASRCF, DRT, and UPDT are the DCFbased methods that exploit deep off-the-shelf features and fuse them with shallow ones (e.g., HOG and CN) to improve the robustness of visual tracking (see Table <ref type="table" target="#tab_1">I</ref>). To reduce the adverse impact of the OCC and OV attributes, the DeepSTRCF adds a temporal regularization term to the spatially regularized DCF formulation. The revisited formulation helps the Deep-STRCF enduring some appearance variations such as the IV, IPR, OPR, and POC. Using object-aware spatial regularization and reliability terms, the ASRCF and DRT methods attempt to optimize models to effectively learn adaptive correlation filters. Both these methods have studied major imperfections of DCF-based methods such as circular shifted sampling process, same feature space for localization and scale estimation processes, the strict focus on discrimination, and sparse and non-uniform distribution of correlation responses. Hence, these methods handle the DEF, BC, and SV, suitably. Finally, the UPDT focuses on enhancing the visual tracking robustness through independently training a shallow feature-based DCF and a deep off-the-shelf feature-based DCF and considering augmented training samples with an adaptive fusion model. Although these methods demonstrate the competitive performance of well-designed DCF-based trackers compared to more sophisticated trackers, they suffer from the limitations of pretrained models, aspect ratio variation, model degradation, and considerable appearance variation.</p><p>Finally, we have modified the VOT toolkit to be able to compare state-of-the-art visual trackers qualitatively. Fig. <ref type="figure" target="#fig_6">7</ref> shows the tracking results of the SiamRPN++ <ref type="bibr" target="#b155">[156]</ref>, SiamMask <ref type="bibr" target="#b154">[155]</ref>, LSART <ref type="bibr" target="#b119">[120]</ref>, UPDT <ref type="bibr" target="#b101">[102]</ref>, ATOM <ref type="bibr" target="#b148">[149]</ref>, and DiMP50 <ref type="bibr" target="#b158">[159]</ref> on some video sequences of the VOT2018 dataset (modified toolkit &amp; all videos are publicly available on the aforementioned page). According to the achieved results, the DiMP50, ATOM, and SiamRPN++ have provided the best results. However, failures usually happen when multiple critical attributes simultaneously occur in a scene. For instance, the SiamMask misuses the semi-supervised video object segmentation when the OCC and SV co-occur, or the significant SV dramatically reduces the performance of the SiamRPN++. Despite considerable advances that are emerged in visual tracking, the state-of-the-art visual trackers are still unable to handle serious real-world challenges; severe variations of target appearance, MOC, OCC, SV, CM, DEF, and even IV can have drastic effects on the performance, which may lead to tracking failures. These results demonstrate that the visual trackers are still not completely reliable for real-world applications because they lack the intelligence for scene understanding. Current trackers improve object-scene distinction, but they cannot infer scene information, immediately recognize the global/configural structure of a scene, or organize purposeful decisions based on space and acts within.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE DIRECTIONS</head><p>The state-of-the-art DL-based visual trackers were categorized into a comprehensive taxonomy based on network However, many problems are not precisely solved, and also other problems need to be explored in the future. In the following, some of these future directions are presented for more investigation.</p><p>First, the main concentration is to design custom neural networks to provide robustness, accuracy, and efficiency simultaneously. These trackers are primarily developed by integrating efficient network architectures with either classification &amp; regression branches or two-step classification &amp; BB refinement networks. Most recent works do not re-train/fine-tune their backbone networks to exploit generic features and avoid catastrophic forgetting of general patterns. However, diverse machine learning-based techniques to address this issue have been proposed, such as incremental learning <ref type="bibr" target="#b255">[256]</ref>, transfer learning penalties <ref type="bibr" target="#b256">[257]</ref>, batch spectral shrinkage <ref type="bibr" target="#b257">[258]</ref>, or lifelong learning <ref type="bibr" target="#b258">[259]</ref>. Thus, effective training of backbone networks can boost tracking performance.</p><p>Second, generic visual trackers are required to adapt to unseen targets quickly. Hence, efficient online training of neural networks is crucial. Recently, meta-/few-shot learning approaches are mainly used to find an optimal initialization of the base learner to a new target. But, the meta-networks need to be shallow to avoid over-fitting problems. Therefore, exploring effective few-shot learning approaches provides fast convergence of deeper networks.</p><p>Third, tracking from aerial-views introduces additional chal-lenges for visual tracking. For instance, small/tiny object tracking in videos captured from medium/high-altitudes, severe viewpoint changes, and tracking many targets in dense environments should be considered. Furthermore, these scenarios are consistently involved with out-of-view and large occlusions; thus, developing long-term approaches will help more reliable aerial-view trackers. Fourth, long-term trackers are overlooked despite many advances in short-term trackers. In fact, long-term trackers are closer to practical, real-world scenarios when the target may disappear frequently or occlude for a long time. These trackers should have the ability to re-detect the target once a failure occurs and then continue tracking the correct target during video sequences. Thus, compelling detection &amp; verification networks are needed to be designed.</p><p>Finally, existing visual trackers have a deficiency in scene understanding. The state-of-the-art methods cannot interpret dynamic scenes in a meaningful way, immediately recognize global structures, infer existing objects, and perceive basic level categories of different objects or events. Although recent trackers desirably reduce the computational complexity, these trackers can be modified to employ complementary features (e.g., temporal information) and incorporate proposed adversarial learning contributions in this few-data regime task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An overview of visual target tracking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Taxonomy of DL-based visual tracking methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head/><label/><figDesc>introduces a toolkit (collected from OTB2015, TC128, &amp; LaSOT) that consists of just one challenging factor per sequence to evaluate visual trackers. It also provides two challenging O-B &amp; O-R attributes, including occlusion with background clutter &amp; rotation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 N</head><label>1</label><figDesc>hi -N lo N hi Ns=N lo Φ Ns . (B) Long-term Tracking Plots: (i) Precision/ Recall plot: It is used to compare long-term tracking performances and analyze their detection capabilities in terms of various thresholds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Overall experimental comparison of state-of-the-art visual tracking methods on the OTB2013, OTB2015, LaSOT, UAVDT, and VisDrone2019 visual tracking datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Performance comparison of visual tracking methods on VOT2018 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Qualitative comparison of state-of-the-art visual trackers on the BMX, Gymnastics3, and Singer3 video sequences from VOT2018 dataset. The #frame number and annotated attributes are shown on each frame. architecture, network exploitation, training, network objective, network output, the exploitation of correlation filter advantages, aerial-view tracking, long-term tracking, and online tracking. Moreover, the motivations and contributions of these methods were categorized according to the main problems and proposed solutions of DL-based trackers. Furthermore, almost all visual tracking benchmark datasets and evaluation metrics were briefly investigated, and the various state-of-theart trackers were compared on seven visual tracking datasets. Recently, the DL-based visual tracking methods have investigated different exploitation of deep off-she-shelf features, fusion of deep features &amp; hand-crafted features, various architectures &amp; backbone networks, offline &amp; online training of DNNs on large-scale datasets, update schemes, search strategies, contextual information, temporal information, and how to deal with lacking training data.However, many problems are not precisely solved, and also other problems need to be explored in the future. In the following, some of these future directions are presented for more investigation.First, the main concentration is to design custom neural networks to provide robustness, accuracy, and efficiency simultaneously. These trackers are primarily developed by integrating efficient network architectures with either classification &amp; regression branches or two-step classification &amp; BB refinement networks. Most recent works do not re-train/fine-tune their backbone networks to exploit generic features and avoid catastrophic forgetting of general patterns. However, diverse machine learning-based techniques to address this issue have been proposed, such as incremental learning<ref type="bibr" target="#b255">[256]</ref>, transfer learning penalties<ref type="bibr" target="#b256">[257]</ref>, batch spectral shrinkage<ref type="bibr" target="#b257">[258]</ref>, or lifelong learning<ref type="bibr" target="#b258">[259]</ref>. Thus, effective training of backbone networks can boost tracking performance.Second, generic visual trackers are required to adapt to unseen targets quickly. Hence, efficient online training of neural networks is crucial. Recently, meta-/few-shot learning approaches are mainly used to find an optimal initialization of the base learner to a new target. But, the meta-networks need to be shallow to avoid over-fitting problems. Therefore, exploring effective few-shot learning approaches provides fast convergence of deeper networks.Third, tracking from aerial-views introduces additional chal-</figDesc><graphic coords="20,454.06,160.99,98.21,53.55" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head/><label/><figDesc>to Table IV, these DL-based methods usually train a pre-trained network (i.e., backbone network) by offline training, online training, or both.</figDesc><table/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I DEEP</head><label>I</label><figDesc>OFF-THE-SHELF FEATURES FOR VISUAL TRACKING. THE ABBREVIATIONS ARE DENOTED AS: CONFIDENCE MAP (CM), SALIENCY MAP (SM), BOUNDING BOX (BB), VOTES (VT), DEEP APPEARANCE FEATURES (DAF), DEEP MOTION FEATURES (DMF). Visual trackers employ diverse datasets to train their networks. These datasets are generally categorized into general-purpose &amp; tracking datasets (see Table I to Table</figDesc><table><row><cell>Method</cell><cell>Pre-trained models</cell><cell>Exploited layers</cell><cell>Pre-training data</cell><cell>Pre-training dataset(s)</cell><cell>Exploited features</cell><cell>PC (CPU, RAM, Nvidia GPU)</cell><cell>Language</cell><cell>Framework</cell><cell cols="2">Speed (fps) Tracking output</cell></row><row><cell>DeepSRDCF [52]</cell><cell>VGG-M</cell><cell>Conv5</cell><cell>Still images</cell><cell>ImageNet</cell><cell>HOG, DAF</cell><cell>N/A, GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>N/A</cell><cell>CM</cell></row><row><cell>CCOT [56]</cell><cell>VGG-M</cell><cell>Conv1, Conv5</cell><cell>Still images</cell><cell>ImageNet</cell><cell>HOG, CN, DAF</cell><cell>N/A, GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>1</cell><cell>CM</cell></row><row><cell>ECO [87]</cell><cell>VGG-M</cell><cell>Conv1, Conv5</cell><cell>Still images</cell><cell>ImageNet</cell><cell>HOG, CN, DAF</cell><cell>N/A, GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>8</cell><cell>CM</cell></row><row><cell>DeepCSRDCF [88]</cell><cell>VGG-M</cell><cell>N/A</cell><cell>Still images</cell><cell>ImageNet</cell><cell>HOG, CN, DAF</cell><cell>Intel I7 3.4GHz CPU, GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>13</cell><cell>CM</cell></row><row><cell>SASR [211]</cell><cell>VGG-M</cell><cell>Conv4</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>Intel-8700K 3.7GHz CPU, 32GB RAM, Quadro P2000 GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>3.84</cell><cell>CM</cell></row><row><cell>KAOT [208], [209]</cell><cell>VGG-M</cell><cell>Conv3</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>Intel I7-8700K 3.7GHz CPU, 32GB RAM, RTX 2080 GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>14.1</cell><cell>CM</cell></row><row><cell>MLCFT [204]</cell><cell>VGG-M</cell><cell>Conv-1, Conv-3, Conv-5</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>Intel I7 3770K 3.5 CPU, 8GB RAM, GTX 960 GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>16.1</cell><cell>CM</cell></row><row><cell>UPDT [102]</cell><cell>VGG-M/ GoogLeNet/ ResNet-50</cell><cell>N/A</cell><cell>Still images</cell><cell>ImageNet</cell><cell>HOG, CN, DAF</cell><cell>N/A</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>N/A</cell><cell>CM</cell></row><row><cell>WAEF [112]</cell><cell>VGG-M</cell><cell>Conv1, Conv5</cell><cell>Still images</cell><cell>ImageNet</cell><cell>HOG, CN, DAF</cell><cell>Intel Xeon(R) 3.20 GHz CPU, 44GB RAM, GTX 1080Ti</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>0.62</cell><cell>CM</cell></row><row><cell>DeepSTRCF [115]</cell><cell>VGG-M</cell><cell>Conv3</cell><cell>Still images</cell><cell>ImageNet</cell><cell>HOG, CN, DAF</cell><cell>Intel I7-7700 CPU, 32GB RAM, GTX 1070 GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>24.3</cell><cell>CM</cell></row><row><cell>DRT [119]</cell><cell>VGG-M, VGG-16</cell><cell>Conv1, Conv4-3</cell><cell>Still images</cell><cell>ImageNet</cell><cell>HOG, CN, DAF</cell><cell>N/A, GPU</cell><cell>Matlab</cell><cell>Caffe</cell><cell>N/A</cell><cell>CM</cell></row><row><cell>WECO [76]</cell><cell>VGG-M</cell><cell>Conv1, Conv5</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>Intel Xeon(R) 2.60GHz CPU, GTX 1080 GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>4</cell><cell>CM</cell></row><row><cell>VDSR-SRT [124]</cell><cell>VGG-M</cell><cell>Conv1, Conv5</cell><cell>Still images</cell><cell>ImageNet</cell><cell>HOG, DAF</cell><cell>Intel I7-6700k 4.00GHz CPU, 16GB RAM, GTX 1070 GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>13.5</cell><cell>CM</cell></row><row><cell>ASRCF [148]</cell><cell>VGG-M, VGG-16</cell><cell>Norm1, Conv4-3</cell><cell>Still images</cell><cell>ImageNet</cell><cell>HOG, DAF</cell><cell>Intel I7-8700 CPU, 32GB RAM, GTX 1080Ti GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>28</cell><cell>CM</cell></row><row><cell>RPCF [152]</cell><cell>VGG-M</cell><cell>Conv1, Conv5</cell><cell>Still images</cell><cell>ImageNet</cell><cell>HOG, CN, DAF</cell><cell>Intel I7-4790K CPU, GTX 1080 GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>5</cell><cell>CM</cell></row><row><cell>DeepTACF [170]</cell><cell>VGG-M</cell><cell>Conv1</cell><cell>Still images</cell><cell>ImageNet</cell><cell>HOG, DAF</cell><cell>Intel I7-6700 3.40GHz CPU, GTX Titan GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>N/A</cell><cell>CM</cell></row><row><cell>FCNT [53]</cell><cell>VGG-16</cell><cell>Conv4-3, Conv5-3</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>3.4GHz CPU, GTX Titan GPU</cell><cell>Matlab</cell><cell>Caffe</cell><cell>3</cell><cell>CM</cell></row><row><cell>CREST [72]</cell><cell>VGG-16</cell><cell>Conv4-3</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>Intel I7 3.4GHz CPU, GTX Titan Black GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>N/A</cell><cell>CM</cell></row><row><cell>DTO [79]</cell><cell>VGG-16, SSD</cell><cell>Conv3-3, Conv4-3, Conv5-3</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>Intel I7-4770K CPU, 32G RAM, GTX 1070 GPU</cell><cell>Matlab</cell><cell>Caffe</cell><cell>N/A</cell><cell>CM, BB</cell></row><row><cell>VRCPF [84]</cell><cell>VGG-16, Faster R-CNN</cell><cell>N/A</cell><cell>Still images</cell><cell>ImageNet, COCO</cell><cell>DAF</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>BB</cell></row><row><cell>Obli-RaFT [92]</cell><cell>VGG-16</cell><cell>Conv4-3, Conv5-3</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>Intel I7-3770 3.40GHz CPU, 2 GTX Titan X GPUs</cell><cell>Matlab</cell><cell>Caffe</cell><cell>2</cell><cell>VT</cell></row><row><cell>CPT [108]</cell><cell>VGG-16</cell><cell>Conv5-1, Conv5-3</cell><cell>Still images</cell><cell>ImageNet</cell><cell>HOG, CN, DAF</cell><cell>Intel I7-7800X CPU, 16GB RAM, GTX 1080Ti GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>14</cell><cell>CM</cell></row><row><cell>DeepHPFT [136]</cell><cell>VGG-16, VGG-19, and GoogLeNet</cell><cell>Conv5-3, Conv5-4, and icp6-out</cell><cell>Still images</cell><cell>ImageNet</cell><cell>HOG, CN, DAF</cell><cell>Intel Xeon 2.4GHz CPU, 256 GB RAM, GTX Titan XP GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>4</cell><cell>CM</cell></row><row><cell>DeepFWDCF [144]</cell><cell>VGG-16</cell><cell>Conv4-3</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>N/A, GTX 1080Ti GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>2.7</cell><cell>CM</cell></row><row><cell>MMLT [107]</cell><cell>VGGNet, Fully-convolutional Siamese network</cell><cell>Conv5</cell><cell>Still images, Video frames</cell><cell>ImageNet, ILSVRC-VID</cell><cell>DAF</cell><cell>Intel I7-4770 3.40GHz CPU, 11GB RAM, GTX 1080Ti</cell><cell>Matlab</cell><cell>N/A</cell><cell>6.15</cell><cell>CM</cell></row><row><cell>MKCT [210]</cell><cell>VGGNet</cell><cell>Conv3-4</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>Intel I7 3.7GHz CPU, 32GB RAM, Quadro 2000 GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>9.4</cell><cell>CM</cell></row><row><cell>BEVT [206]</cell><cell>VGGNet</cell><cell>Conv3-4, Conv4-4, Conv5-4</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>Intel I7-8700K 3.7GHz CPU, 48GB RAM, Quadro P2000 GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>0.6</cell><cell>CM</cell></row><row><cell>HCFT [51]</cell><cell>VGG-19</cell><cell>Conv3-4, Conv4-4, Conv5-4</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>Intel I7-4770 3.40GHz CPU, 32 GB RAM, GTX Titan GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>10.4</cell><cell>CM</cell></row><row><cell>HDT [61]</cell><cell>VGG-19</cell><cell>Conv4-2, Conv4-3, Conv4-4, Conv5-2, Conv5-3, Conv5-4</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>Intel I7-4790K 4.00GHz CPU, 16GB RAM, GTX 780Ti GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>1</cell><cell>CM</cell></row><row><cell>IBCCF [78]</cell><cell>VGG-19</cell><cell>Conv3-4, Conv4-4, Conv5-4</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>Intel Xeon(R) 3.3GHz CPU, 32GB RAM, GTX 1080 GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>1.25</cell><cell>CM</cell></row><row><cell>DCPF [85]</cell><cell>VGG-19</cell><cell>Conv3-4, Conv4-4, Conv5-4</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>CM</cell></row><row><cell>MCPF [89]</cell><cell>VGG-19</cell><cell>Conv3-4, Conv4-4, Conv5-4</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>Intel 3.10GHz CPU, 256 GB RAM, GTX Titan X GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>0.5</cell><cell>CM</cell></row><row><cell>DeepLMCF [91]</cell><cell>VGG-19</cell><cell>Conv3-4, Conv4-4, Conv5-4</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>Intel 3.60GHz CPU, Tesla K40 GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>10</cell><cell>CM</cell></row><row><cell>STSGS [99]</cell><cell>VGG-19</cell><cell>Conv3-4, Conv4-4, Conv5-4</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF, DMF</cell><cell>Intel I7 3.20GHz CPU, 8 GB RAM</cell><cell>Matlab</cell><cell>Caffe</cell><cell>4 5</cell><cell>CM</cell></row><row><cell>MCCT [122]</cell><cell>VGG-19</cell><cell>Conv4-4, Conv5-4</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>Intel I7-4790K 4.00GHz CPU, 16GB RAM, GTX 1080Ti GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>8</cell><cell>CM</cell></row><row><cell>DCPF2 [123]</cell><cell>VGG-19</cell><cell>Conv3-4, Conv4-4, Conv5-4</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>CM</cell></row><row><cell>HCFTs [133]</cell><cell>VGG-19</cell><cell>Conv3-4, Conv4-4, Conv5-4</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>Intel I7-4770 3.40GHz CPU, 32GB RAM, GTX Titan GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>6.7</cell><cell>CM</cell></row><row><cell>LCTdeep [142]</cell><cell>VGG-19</cell><cell>Conv5-4</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>Intel I7-4770 3.40GHz CPU, 32GB RAM, GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>13.8</cell><cell>CM</cell></row><row><cell>CF-CNN [67]</cell><cell>VGG-19</cell><cell>Conv3-4, Conv4-4, Conv5-4</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>Intel I7-4770 3.40GHz CPU, 32GB RAM, GTX Titan GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>12.3</cell><cell>CM</cell></row><row><cell>ORHF [147]</cell><cell>VGG-19</cell><cell>Conv3-4, Conv4-4, Conv5-4</cell><cell>Still images</cell><cell>ImageNet</cell><cell>HOG, DAF</cell><cell>Intel I7-4770K 3.50GHz CPU, 24GB RAM, N/A</cell><cell>Matlab</cell><cell>N/A</cell><cell>N/A</cell><cell>CM</cell></row><row><cell>IMM-DFT [168]</cell><cell>VGG-19</cell><cell>Conv3-4, Conv4-4, Conv5-4</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>Intel I5-4590 3.30GHz CPU, 16GB RAM, GTX Titan X GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>10</cell><cell>CM</cell></row><row><cell>CNN-SVM [54]</cell><cell>R-CNN</cell><cell>First fully-connected layer</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>N/A</cell><cell>N/A</cell><cell>Caffe</cell><cell>N/A</cell><cell>SM</cell></row><row><cell>RPNT [63]</cell><cell>Object proposal network</cell><cell>N/A</cell><cell>Still images</cell><cell>ImageNet, PASCAL VOC</cell><cell>DAF</cell><cell>N/A</cell><cell>C/C++</cell><cell>N/A</cell><cell>3.8</cell><cell>BB</cell></row><row><cell>CF-FCSiam [145]</cell><cell>Fully-convolutional Siamese network</cell><cell>N/A</cell><cell>Video frames</cell><cell>ILSVRC-VID</cell><cell>HOG, DAF</cell><cell>Intel I7-6700K 4.00GHz CPU, GTX Titan GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>33</cell><cell>CM</cell></row><row><cell>TADT [157]</cell><cell>Siamese matching network</cell><cell>Conv4-1, Conv4-3</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>Intel I7 3.60GHz CPU, 32GB RAM, GTX 1080 GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>33.7</cell><cell>CM</cell></row><row><cell>GFS-DCF [186]</cell><cell>ResNet-50</cell><cell>Res4x</cell><cell>Still images</cell><cell>ImageNet</cell><cell>DAF</cell><cell>Intel Xeon E5-2637v3 CPU, N/A, GTX Titan X GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>8</cell><cell>CM</cell></row><row><cell>DHT [203]</cell><cell>Various networks</cell><cell>Various layers</cell><cell>Still images, Video frames</cell><cell>Various datasets</cell><cell>DAF</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>12</cell><cell>BB</cell></row></table><note><p><p><p>work training, online fine-tuning, computational complexity, dealing with lack of training data, addressing the overfitting problem, and exploiting unlabeled samples by unsupervised training. The network training sections in the previous review papers [47]-[49] consider both FENs and EENs, although the FENs were only pre-trained for other tasks, and there is no training procedure for visual tracking. In this survey, DL-based methods are categorized into only offline pre-training, only online training, and both offline and online training for visual tracking purposes. The training details of these methods are shown in Table</p>II</p>to Table IV. 1) Training Datasets:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II ONLY</head><label>II</label><figDesc>OFFLINE TRAINING FOR VISUAL TRACKING. THE ABBREVIATIONS ARE DENOTED AS: CONFIDENCE MAP (CM), SALIENCY MAP (SM), BOUNDING BOX (BB), OBJECT SCORE (OS), FEATURE MAPS (FM), SEGMENTATION MASK (SGM), ROTATED BOUNDING BOX (RBB), ACTION (AC), DEEP APPEARANCE FEATURES (DAF), DEEP MOTION FEATURES (DMF), DEEO OPTICAL FLOW (DOF).</figDesc><table><row><cell>Method</cell><cell>Backbone network</cell><cell>Offline training dataset(s)</cell><cell>Exploited features</cell><cell>PC (CPU, RAM, Nvidia GPU)</cell><cell>Language</cell><cell>Framework</cell><cell cols="2">Speed (fps) Tracking output</cell></row><row><cell>GOTURN [57]</cell><cell>AlexNet</cell><cell>ILSVRC-DET, ALOV</cell><cell>DAF</cell><cell>N/A, GTX Titan X GPU</cell><cell>C/C++</cell><cell>Caffe</cell><cell>166</cell><cell>BB</cell></row><row><cell>SiamFC [58]</cell><cell>AlexNet</cell><cell>ImageNet, ILSVRC-VID</cell><cell>DAF</cell><cell>Intel I7-4790K 4.00GHz CPU, GTX Titan X GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>58</cell><cell>CM</cell></row><row><cell>SINT [59]</cell><cell>AlexNet, VGG-16</cell><cell>ImageNet, ALOV</cell><cell>DAF</cell><cell>N/A</cell><cell>Matlab</cell><cell>Caffe</cell><cell>N/A</cell><cell>OS</cell></row><row><cell>R-FCSN [81]</cell><cell>AlexNet</cell><cell>ImageNet, ILSVRC-VID</cell><cell>DAF</cell><cell>N/A, GTX Titan X GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>50.25</cell><cell>CM</cell></row><row><cell>LST [83]</cell><cell>AlexNet</cell><cell>ImageNet, ILSVRC-VID</cell><cell>DAF</cell><cell>Intel Xeon 3.50GHz CPU, GTX Titan X GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>24</cell><cell>CM</cell></row><row><cell>CFNet [86]</cell><cell>AlexNet</cell><cell>ImageNet, ILSVRC-VID</cell><cell>DAF</cell><cell>Intel I7 4.00GHz CPU, GTX Titan X GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>75</cell><cell>CM</cell></row><row><cell>DaSiamRPN [104]</cell><cell>AlexNet</cell><cell>ILSVRC, YTBB, Augmented ILSVRC-DET, Augmented MSCOCO-DET</cell><cell>DAF</cell><cell>Intel I7 CPU, 48GB RAM, GTX Titan X GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell>160</cell><cell>CM</cell></row><row><cell>StructSiam [106]</cell><cell>AlexNet</cell><cell>ILSVRC-VID, ALOV</cell><cell>DAF</cell><cell>Intel I7-4790 3.60GHz, GTX 1080 GPU</cell><cell>Python</cell><cell>TensorFlow</cell><cell>45</cell><cell>CM</cell></row><row><cell>Siam-BM [111]</cell><cell>AlexNet</cell><cell>ImageNet, ILSVRC-VID</cell><cell>DAF</cell><cell>Intel Xeon 2.60GHz CPU, Tesla P100 GPU</cell><cell>Python</cell><cell>TensorFlow</cell><cell>48</cell><cell>CM</cell></row><row><cell>SA-Siam [117]</cell><cell>AlexNet</cell><cell>ImageNet, TC128, ILSVRC-VID</cell><cell>DAF</cell><cell>Intel Xeon 2.40GHz CPU, GTX Titan X GPU</cell><cell>Python</cell><cell>TensorFlow</cell><cell>50</cell><cell>CM</cell></row><row><cell>SiamRPN [116]</cell><cell>AlexNet</cell><cell>ILSVRC, YTBB</cell><cell>DAF</cell><cell>Intel I7 CPU, 12GB RAM, GTX 1060 GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell>160</cell><cell>FM</cell></row><row><cell>C-RPN [150]</cell><cell>AlexNet</cell><cell>ImageNet, ILSVRC-VID, YTBB</cell><cell>DAF</cell><cell>N/A, GTX 1080 GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>36</cell><cell>CM</cell></row><row><cell>GCT [151]</cell><cell>AlexNet</cell><cell>ImageNet, ILSVRC-VID</cell><cell>DAF</cell><cell>Intel Xeon 3.00GHz CPU, 256GB RAM, GTX 1080Ti GPU</cell><cell>Python</cell><cell>TensorFlow</cell><cell>49.8</cell><cell>CM</cell></row><row><cell>GradNet [187]</cell><cell>AlexNet</cell><cell>ILSVRC-2014</cell><cell>DAF</cell><cell>Intel I7 3.2GHz CPU, 32GB RAM, GTX 1080Ti GPU</cell><cell>Python</cell><cell>TensorFlow</cell><cell>80</cell><cell>CM</cell></row><row><cell>i-Siam [215]</cell><cell>AlexNet</cell><cell>GOT-10k</cell><cell>DAF</cell><cell>Intel I7-7700K 4.20GHz CPU, Titan Xp GPU</cell><cell>N/A</cell><cell>N/A</cell><cell>43</cell><cell>CM</cell></row><row><cell>UpdateNet [189]</cell><cell>AlexNet</cell><cell>LaSOT</cell><cell>DAF</cell><cell>N/A</cell><cell>Python</cell><cell>PyTorch</cell><cell>N/A</cell><cell>CM</cell></row><row><cell>SPM [153]</cell><cell>AlexNet, SiameseRPN, RelationNet</cell><cell>ImageNet, ILSVRC-VID, YTBB, ILSVRC-DET, MSCOCO, CityPerson, WiderFace</cell><cell>DAF</cell><cell>N/A, P100 GPU</cell><cell>N/A</cell><cell>N/A</cell><cell>120</cell><cell>OS</cell></row><row><cell>FICFNet [141]</cell><cell>AlexNet</cell><cell>ImageNet, ILSVRC-VID</cell><cell>DAF</cell><cell>Intel I7 4.00GHz CPU, GTX Titan X GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>28</cell><cell>CM</cell></row><row><cell>MTHCF [166]</cell><cell>AlexNet</cell><cell>ImageNet, ILSVRC-VID</cell><cell>DAF</cell><cell>Intel 6700 3.40GHz CPU, GTX Titan GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>33</cell><cell>CM</cell></row><row><cell>HP [178]</cell><cell>AlexNet</cell><cell>ImageNet, ILSVRC-VID</cell><cell>DAF</cell><cell>N/A</cell><cell>Python</cell><cell>Keras</cell><cell>69</cell><cell>CM</cell></row><row><cell>EAST [177]</cell><cell>AlexNet</cell><cell>ImageNet, ILSVRC-VID</cell><cell>DAF</cell><cell>Intel I7 4.00GHz CPU, GTX Titan X GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>23.2</cell><cell>AC</cell></row><row><cell>CFCF [137]</cell><cell>VGG-M</cell><cell>ImageNet, ILSVRC-VID, VOT2015</cell><cell>HOG, DAF</cell><cell>Intel Xeon 3.00GHz CPU, Tesla K40 GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>1.7</cell><cell>CM</cell></row><row><cell>CFSRL [138]</cell><cell>VGG-M</cell><cell>ILSVRC-VID</cell><cell>DAF</cell><cell>Intel Xeon 2.40GHz CPU, 32GB RAM, GTX Titan X GPU</cell><cell>Matlab, Python</cell><cell>PyTorch</cell><cell>N/A</cell><cell>CM</cell></row><row><cell>C2FT [174]</cell><cell>VGG-M</cell><cell>ImageNet, N/A</cell><cell>DAF</cell><cell>Intel Xeon 2.60GHz CPU, GTX 1080Ti GPU</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>AC</cell></row><row><cell>DSNet [205]</cell><cell>VGG-M</cell><cell>ILSVRC-2015</cell><cell>DAF</cell><cell>Intel 6700K 4.0GHz CPU, GTX 1080 GPU</cell><cell>N/A</cell><cell>N/A</cell><cell>68.5</cell><cell>CM</cell></row><row><cell>SRT [80]</cell><cell>VGG-16</cell><cell>ImageNet, ALOV, Deform-SOT</cell><cell>DAF</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>BB</cell></row><row><cell>IMLCF [128]</cell><cell>VGG-16</cell><cell>ImageNet, ILSVRC-VID</cell><cell>DAF</cell><cell>Intel 1.40GHz CPU, GTX 1080Ti GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>N/A</cell><cell>CM</cell></row><row><cell>SINT++ [181]</cell><cell>VGG-16</cell><cell>ImageNet, OTB2013, OTB2015, VOT2014</cell><cell>DAF</cell><cell>Intel I7-6700K CPU, 32GB RAM, GTX 1080 GPU</cell><cell>Python</cell><cell>Caffe, Keras</cell><cell>N/A</cell><cell>AC</cell></row><row><cell>MAM [171]</cell><cell>VGG-16, Faster-RCNN</cell><cell>ImageNet, PASCAL VOC 2007, OTB100, TC128</cell><cell>DAF</cell><cell>3.40GHz CPU, Titan GPU</cell><cell>Matlab</cell><cell>Caffe</cell><cell>3</cell><cell>SM</cell></row><row><cell>PTAV [70], [71]</cell><cell>VGGNet</cell><cell>ALOV</cell><cell>HOG, DAF</cell><cell>N/A, GTX Titan Z GPU</cell><cell>C/C++</cell><cell>Caffe</cell><cell>27</cell><cell>CM</cell></row><row><cell>UDT [158]</cell><cell>VGGNet</cell><cell>ILSVRC</cell><cell>DAF</cell><cell>Intel I7-4790K 4.00GHz, GTX 1080Ti GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>55</cell><cell>CM</cell></row><row><cell>DRRL [162]</cell><cell>VGGNet</cell><cell>ImageNet, VOT2016</cell><cell>DAF</cell><cell>N/A, GTX 1060 GPU</cell><cell>Python</cell><cell>TensorFlow</cell><cell>6.3</cell><cell>OS</cell></row><row><cell>FCSFN [125]</cell><cell>VGG-19</cell><cell>ImageNet, ALOV</cell><cell>DAF</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>CM</cell></row><row><cell>Siam-MCF [110]</cell><cell>ResNet-50</cell><cell>ImageNet, ILSVRC-VID</cell><cell>DAF</cell><cell>Intel Xeon E5 CPU, GTX 1080Ti GPU</cell><cell>Python</cell><cell>TensorFlow</cell><cell>20</cell><cell>CM</cell></row><row><cell>SiamMask [155]</cell><cell>ResNet-50</cell><cell>ImageNet, MSCOCO, ILSVRC-VID, YouTube-VOS</cell><cell>DAF</cell><cell>N/A, RTX 2080 GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell>55</cell><cell>SGM, RBB</cell></row><row><cell>SiamRPN++ [156]</cell><cell>ResNet-50</cell><cell>ImageNet, MSCOCO, ILSVRC-DET, ILSVRC-VID, YTBB</cell><cell>DAF</cell><cell>N/A, Titan Xp Pascal GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell>35</cell><cell>OS, BB</cell></row><row><cell>CGACD [190]</cell><cell>ResNet-50</cell><cell>ILSVRC-VID, YTBB, GOT-10k, COCO, ILSVRC-DET</cell><cell>DAF</cell><cell>3.5GHz CPU, RTX 2080Ti GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell>70</cell><cell>BB</cell></row><row><cell>CSA [191]</cell><cell>ResNet-50</cell><cell>GOT-10k</cell><cell>DAF</cell><cell>Intel I9 CPU, 64GB RAM, RTX 2080Ti GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell>100</cell><cell>CM</cell></row><row><cell>SiamAttn [197]</cell><cell>ResNet-50</cell><cell>COCO, YouTube-VOS, LaSOT, TrackingNet</cell><cell>DAF</cell><cell>N/A, RTX 2080Ti GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell>33</cell><cell>BB, SGM</cell></row><row><cell>SiamBAN [198]</cell><cell>ResNet-50</cell><cell>ILSVRC-VID, YTBB, COCO, ILSVRC-DET, GOT-10k, LaSOT</cell><cell>DAF</cell><cell>Intel Xeon 4108 1.8GHz CPU, 64GB RAM, GTX 1080Ti GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell>40</cell><cell>BB</cell></row><row><cell>SiamCAR [199]</cell><cell>ResNet-50</cell><cell>ILSVRC-DET, COCO, ILSVRC-VID, YTBB</cell><cell>DAF</cell><cell>N/A, 4 RTX 2080Ti GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell>52.2</cell><cell>BB</cell></row><row><cell>SiamDW [154]</cell><cell>ResNet, ResNeXt, Inception</cell><cell>ImageNet, ILSVRC-VID, YTBB</cell><cell>DAF</cell><cell>Intel Xeon 2.40GHz CPU, GTX 1080 GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell>13</cell><cell>CM, FM</cell></row><row><cell>FlowTrack [118]</cell><cell>FlowNet</cell><cell>Flying chairs, Middlebur, KITTI, Sintel, ILSVRC-VID</cell><cell>DAF, DMF</cell><cell>Intel I7-6700 CPU, 48GB RAM, GTX Titan X GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>12</cell><cell>CM</cell></row><row><cell>RASNet [121]</cell><cell>Attention networks</cell><cell>ILSVRC-DET</cell><cell>DAF</cell><cell>Intel Xeon 2.20GHz CPU, Titan Xp Pascal GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>83</cell><cell>CM</cell></row><row><cell>ACFN [93]</cell><cell>Attentional correlation filter network</cell><cell>OTB2013, OTB2015, VOT2014, VOT2015</cell><cell>HOG, Color, DAF</cell><cell>Intel I7-6900K 3.20GHz CPU, 32GB RAM, GTX 1070 GPU</cell><cell cols="2">Matlab, Python MatConvNet, TensorFlow</cell><cell>15</cell><cell>OS</cell></row><row><cell>RFL [77]</cell><cell>Convolutional LSTM</cell><cell>ILSVRC-VID</cell><cell>DAF</cell><cell>Intel I7-6700 3.40GHz CPU, GTX 1080 GPU</cell><cell>Python</cell><cell>TensorFlow</cell><cell>15</cell><cell>CM</cell></row><row><cell>DRLT [176]</cell><cell>YOLO</cell><cell>ImageNet, PASCAL VOC</cell><cell>DAF</cell><cell>N/A, GTX 1080 GPU</cell><cell>Python</cell><cell>TensorFlow</cell><cell>45</cell><cell>BB</cell></row><row><cell>TGGAN [129]</cell><cell>-</cell><cell>ALOV, VOT2015</cell><cell>DAF</cell><cell>N/A, GTX Titan X GPU</cell><cell>Python</cell><cell>Keras</cell><cell>3.1</cell><cell>CM</cell></row><row><cell>DCTN [131]</cell><cell>-</cell><cell>TC128, NUS-PRO, MOT2015</cell><cell>DAF, DMF</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>27</cell><cell>CM</cell></row><row><cell>YCNN [135]</cell><cell>-</cell><cell>ImageNet, ALOV300++</cell><cell>DAF</cell><cell>N/A, Tesla K40c GPU</cell><cell>Python</cell><cell>TensorFlow</cell><cell>45</cell><cell>CM</cell></row><row><cell>RDT [180]</cell><cell>-</cell><cell>VOT2015</cell><cell>DAF</cell><cell>Intel I7-4790K 4.00GHz, 24GB RAM, GTX Titan X GPU</cell><cell>Python</cell><cell>TensorFlow</cell><cell>43</cell><cell>CM, OS</cell></row><row><cell>SiamRCNN [200]</cell><cell>Faster R-CNN, ResNet-101-FPN</cell><cell>ILSVRC-VID, COCO, YouTube-VOS, GOT-10k, LaSOT</cell><cell>DAF</cell><cell>N/A, Tesla V100 GPU</cell><cell>Python</cell><cell>TensorFlow</cell><cell>4.7</cell><cell>BB, SGM</cell></row><row><cell>FGTrack [202]</cell><cell>ResNet-18, FlowNet2</cell><cell>ILSVRC-VID, TrackingNet, YouTube-VOS</cell><cell>DAF, DOF</cell><cell>N/A, GTX 1080Ti GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell>19.6</cell><cell>BB</cell></row><row><cell>CRVFL [183]</cell><cell>-</cell><cell>N/A</cell><cell>DAF</cell><cell>Intel I7 CPU</cell><cell>N/A</cell><cell>N/A</cell><cell>1.5-2</cell><cell>OS</cell></row><row><cell>VTCNN [184]</cell><cell>-</cell><cell>N/A</cell><cell>DAF</cell><cell>Intel I7 3.4GHz</cell><cell>Matlab</cell><cell>N/A</cell><cell>5.5</cell><cell>OS</cell></row><row><cell>GlobalTrack [214]</cell><cell>Faster R-CNN, ResNet-50</cell><cell>COCO, GOT-10k, LaSOT</cell><cell>DAF</cell><cell>N/A, Titan X GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell>6</cell><cell>BB, SGM</cell></row><row><cell>SPLT [218]</cell><cell>MobileNet-v1, ResNet-50</cell><cell>ILSVRC-VID, ILSVRC-DET</cell><cell>DAF</cell><cell>Inter I7 CPU, 32GB RAM, GTX 1080Ti GPU</cell><cell>Python</cell><cell>TensorFlow, Keras</cell><cell>25.7</cell><cell>BB, OS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III ONLY</head><label>III</label><figDesc>ONLINE TRAINING FOR VISUAL TRACKING. THE ABBREVIATIONS ARE DENOTED AS: CONFIDENCE MAP (CM), BOUNDING BOX (BB), OBJECT SCORE (OS), DEEP APPEARANCE FEATURES (DAF), ACTION (AC).</figDesc><table><row><cell>Method</cell><cell>Backbone network</cell><cell>Exploited features</cell><cell>Strategy to alleviate the over-fitting problem</cell><cell>PC (CPU, RAM, Nvidia GPU)</cell><cell>Language</cell><cell>Framework</cell><cell cols="2">Speed (fps) Tracking output</cell></row><row><cell>SMART [163]</cell><cell>ZFNet</cell><cell>DAF</cell><cell>Set the learning rates in conv1-conv3 to zero</cell><cell>Intel 3.10GHz CPU, 256 GB RAM, GTX Titan X GPU</cell><cell>Matlab</cell><cell>Caffe</cell><cell>27</cell><cell>CM</cell></row><row><cell>TCNN [68]</cell><cell>VGG-M</cell><cell>DAF</cell><cell>Only update fully-connected layers</cell><cell>Intel I7-5820K 3.30GHz CPU, GTX Titan X GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>1.5</cell><cell>OS</cell></row><row><cell>C2FT [174]</cell><cell>VGG-M</cell><cell>DAF</cell><cell>Coarse-to-fine localization</cell><cell>Intel Xeon E5-2670 2.60GHz, GTX 1080Ti GPU</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>AC</cell></row><row><cell>TSN [75]</cell><cell>VGG-16</cell><cell>DAF</cell><cell>Coarse-to-fine framework</cell><cell>N/A, GTX 980Ti GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>1</cell><cell>CM</cell></row><row><cell>DNT [98]</cell><cell>VGG-16</cell><cell>DAF</cell><cell>Set uniform weight decay in the objective functions</cell><cell>3.40GHz CPU, GTX Titan GPU</cell><cell>Matlab</cell><cell>Caffe</cell><cell>5</cell><cell>CM</cell></row><row><cell>DSLT [101]</cell><cell>VGG-16</cell><cell>DAF</cell><cell>Use seven last frames for model update</cell><cell>Intel I7 4.00GHz CPU, GTX Titan X GPU</cell><cell>Matlab</cell><cell>Caffe</cell><cell>5.7</cell><cell>CM</cell></row><row><cell>LSART [120]</cell><cell>VGG-16</cell><cell>DAF</cell><cell>Two-stream training network to learn network parameters</cell><cell>Intel 4.00GHz CPU, 32GB RAM, GTX Titan X GPU</cell><cell>Matlab</cell><cell>Caffe</cell><cell>1</cell><cell>CM</cell></row><row><cell>adaDDCF [134]</cell><cell>VGG-16</cell><cell>DAF</cell><cell>Regularization item for training of each layer</cell><cell>3.40GHz CPU, Tesla K40 GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>9</cell><cell>CM</cell></row><row><cell>HSTC [143]</cell><cell>VGG-16</cell><cell>DAF</cell><cell>Dropout layer and convolutional with the mask layer</cell><cell>Intel Xeon 2.10GHz CPU, GTX 1080 GPU</cell><cell>Matlab</cell><cell>Caffe</cell><cell>2.1</cell><cell>CM</cell></row><row><cell>P-Track [179]</cell><cell>VGG-16</cell><cell>DAF</cell><cell>Learning policy for update and re-initialization</cell><cell>N/A, Tesla K40 GPU</cell><cell>N/A</cell><cell>N/A</cell><cell>10</cell><cell>CM</cell></row><row><cell>OSAA [193]</cell><cell>ResNet-50 or MobileNet-v2</cell><cell>DAF</cell><cell>-</cell><cell>N/A, Tesla V100 GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell>N/A</cell><cell>BB</cell></row><row><cell>STCT</cell><cell>Custom</cell><cell>DAF</cell><cell>Sequential training method</cell><cell>3.40GHz CPU, GTX Titan GPU</cell><cell>Matlab</cell><cell>Caffe</cell><cell>2.5</cell><cell>CM</cell></row><row><cell>DeepTrack [64]</cell><cell>Custom</cell><cell>DAF</cell><cell>Temporal sampling mechanism for the batch generation in SGD algorithm</cell><cell>Quad-core CPU, GTX 980 GPU</cell><cell>Matlab</cell><cell>N/A</cell><cell>2.5</cell><cell>OS</cell></row><row><cell>CNT [66]</cell><cell>Custom</cell><cell>DAF</cell><cell>Incremental update scheme</cell><cell>Intel I7-3770 3.40GHz CPU, GPU</cell><cell>Matlab</cell><cell>N/A</cell><cell>5</cell><cell>BB</cell></row><row><cell>RDLT [69]</cell><cell>Custom</cell><cell>DAF</cell><cell>Build relationship between the stable factor and iteration number</cell><cell>Intel I7 2.20GHz CPU</cell><cell>Matlab</cell><cell>N/A</cell><cell>5</cell><cell>CM</cell></row><row><cell>P2T [139]</cell><cell>Custom</cell><cell>DAF</cell><cell>Generate large scale of part pairs in each mini-batch</cell><cell>Intel I7-4790 3.60GHz CPU, 32GB RAM, GTX 980 GPU</cell><cell>Matlab</cell><cell>Caffe</cell><cell>2</cell><cell>BB</cell></row><row><cell>AEPCF [167]</cell><cell>Custom</cell><cell>DAF</cell><cell>Select a proper learning rate</cell><cell>Intel I7 3.40GHz, 32GB RAM, GPU</cell><cell>N/A</cell><cell>N/A</cell><cell>4.15</cell><cell>CM</cell></row><row><cell>FRPN2T-Siam [126]</cell><cell>Custom</cell><cell>DAF</cell><cell>Only update fully-connected layers</cell><cell>N/A</cell><cell>Matlab</cell><cell>Caffe</cell><cell>N/A</cell><cell>CM</cell></row><row><cell>RLS [195]</cell><cell>Custom</cell><cell>DAF</cell><cell>Recursive LSE-aided online learning method</cell><cell>N/A</cell><cell>Python</cell><cell>N/A</cell><cell>N/A</cell><cell>OS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV BOTH</head><label>IV</label><figDesc>OFFLINE AND ONLINE TRAINING FOR VISUAL TRACKING. THE ABBREVIATIONS ARE DENOTED AS: CONFIDENCE MAP (CM), BOUNDING BOX (BB), ROTATED BOUNDING BOX (RBB), OBJECT SCORE (OS), VOTING MAP (VM), ACTION (AC), SEGMENTATION MASK (SGM), DEEP APPEARANCE FEATURES (DAF), DEEP MOTION FEATURES (DMF), COMPRESSED DEEP APPEARANCE FEATURES (CDAF).</figDesc><table><row><cell>Method</cell><cell>Backbone network</cell><cell>Offline training(s)</cell><cell cols="2">Online network training Exploited features</cell><cell>PC (CPU, RAM, Nvidia GPU)</cell><cell>Language</cell><cell>Framework</cell><cell>Speed (fps)</cell><cell>Tracking output</cell></row><row><cell>DRN [97]</cell><cell>AlexNet</cell><cell>ImageNet</cell><cell>Yes</cell><cell>DAF</cell><cell>N/A, K20 GPU</cell><cell>Matlab</cell><cell>Caffe</cell><cell>1.3</cell><cell>CM</cell></row><row><cell>DSiam/DSiamM [74]</cell><cell>AlexNet, VGG-19</cell><cell>ImageNet, ILSVRC-VID</cell><cell>Yes</cell><cell>DAF</cell><cell>N/A, GTX Titan X GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell/><cell>CM</cell></row><row><cell>TripletLoss [100]</cell><cell>AlexNet</cell><cell>ImageNet, ILSVRC-VID, ILSVRC</cell><cell>Dependent</cell><cell>DAF</cell><cell>Intel I7-6700 3.40GHz CPU, GTX 1080Ti GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>55 86</cell><cell>CM</cell></row><row><cell>MM [165]</cell><cell>AlexNet</cell><cell>ImageNet, OTB2015, ILSVRC</cell><cell>Yes</cell><cell>DAF</cell><cell>Intel I7-6700 4.00GHz CPU, 16GB RAM, GTX 1060 GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>1.2</cell><cell>OS</cell></row><row><cell>TAAT [169]</cell><cell>AlexNet, VGGNet, ResNet</cell><cell>ImageNet, ALOV, ILSVRC-VID</cell><cell>Yes</cell><cell>DAF</cell><cell>Intel Xeon 1.60GHz CPU, 16GB RAM, GTX Titan X GPU</cell><cell>Matlab</cell><cell>Caffe</cell><cell/><cell>BB</cell></row><row><cell>DPST [55]</cell><cell>VGG-M</cell><cell>ImageNet, ILSVRC-VID, ALOV</cell><cell>Only on the first frame</cell><cell>DAF</cell><cell>Intel I7 3.60GHz CPU, GTX 1080Ti GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell/><cell>OS</cell></row><row><cell>MDNet [60]</cell><cell>VGG-M</cell><cell>ImageNet, OTB2015, ILSVRC-VID</cell><cell>Yes</cell><cell>DAF</cell><cell>Intel Xeon 2.20GHz CPU, Tesla K20m GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell/><cell>OS</cell></row><row><cell>GNet [82]</cell><cell>VGG-M</cell><cell>ImageNet, VOT</cell><cell>Yes</cell><cell>DAF</cell><cell>Intel Xeon 2.66GHz CPU, Tesla K40 GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell/><cell>OS</cell></row><row><cell>BranchOut [90]</cell><cell>VGG-M</cell><cell>ImageNet, OTB2015, ILSVRC</cell><cell>Yes</cell><cell>DAF</cell><cell>N/A</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>N/A</cell><cell>OS</cell></row><row><cell>SANet [94]</cell><cell>VGG-M</cell><cell>ImageNet, OTB2015, ILSVRC</cell><cell>Yes</cell><cell>DAF</cell><cell>Intel I7 3.70GHz CPU, GTX Titan Z GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell/><cell>OS</cell></row><row><cell>RT-MDNet [105]</cell><cell>VGG-M</cell><cell>ImageNet, ILSVRC-VID</cell><cell>Yes</cell><cell>DAF</cell><cell>Intel I7-6850K 3.60GHz, Titan Xp Pascal GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell/><cell>OS</cell></row><row><cell>TRACA [113]</cell><cell>VGG-M</cell><cell>ImageNet, PASCAL VOC</cell><cell>Yes</cell><cell>CDAF</cell><cell>Intel I7-2700K 3.50GHz CPU, 16GB RAM, GTX 1080 GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>101.3</cell><cell>CM</cell></row><row><cell>VITAL [114]</cell><cell>VGG-M</cell><cell>ImageNet, OTB2015, ILSVRC</cell><cell>Yes</cell><cell>DAF</cell><cell>Intel I7 3.60GHz CPU, Tesla K40c GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>1.5</cell><cell>OS</cell></row><row><cell>DAT [130]</cell><cell>VGG-M</cell><cell>ImageNet, OTB2015, ILSVRC</cell><cell>Yes</cell><cell>DAF</cell><cell>Intel I7-3.40GHz CPU, GTX 1080 GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell/><cell>CM</cell></row><row><cell>ACT [103]</cell><cell>VGG-M</cell><cell>ImageNet-Video, ILSVRC</cell><cell>Yes</cell><cell>DAF</cell><cell>3.40GHz CPU, 32GB RAM, GTX Titan GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell/><cell>OS</cell></row><row><cell>MGNet [146]</cell><cell>VGG-M</cell><cell>ImageNet, OTB2015, ILSVRC</cell><cell>Yes</cell><cell>DAF, DMF</cell><cell>Intel I7-5930K 3.50GHz CPU, GTX Titan X GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell/><cell>OS</cell></row><row><cell>DRL-IS [175]</cell><cell>VGG-M</cell><cell>ImageNet, VOT2013 2015</cell><cell>Yes</cell><cell>DAF</cell><cell>Intel I7 3.40GHz CPU, 24GB RAM, GTX 1080Ti GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell>10.2</cell><cell>AC</cell></row><row><cell>ADNet [172], [173]</cell><cell>VGG-M</cell><cell>ImageNet, VOT2013 2015, ALOV</cell><cell>Yes</cell><cell>DAF</cell><cell>Intel I7-4790K, 32GB RAM, GTX Titan X GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell/><cell>AC, OS</cell></row><row><cell>FMFT [127]</cell><cell>VGG-16</cell><cell>ImageNet</cell><cell>Yes</cell><cell>DAF</cell><cell>Intel Xeon 3.50GHz CPU, GTX Titan X GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>N/A</cell><cell>CM</cell></row><row><cell>DET [96]</cell><cell>VGG-16</cell><cell>ImageNet, ALOV, VOT2014, VOT2015</cell><cell>Yes</cell><cell>DAF</cell><cell>Intel I7-4790 3.60GHz CPU, GTX Titan X GPU</cell><cell>Python</cell><cell>Keras</cell><cell>3.4</cell><cell>OS</cell></row><row><cell>DCFNet/DCFNet2 [95]</cell><cell>VGGNet</cell><cell>ImageNet, TC128, UAV123, NUS-PRO</cell><cell>Yes</cell><cell>DAF</cell><cell>Intel Xeon 2.40GHz CPU, GTX 1080 GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell/><cell>CM</cell></row><row><cell>STP [109]</cell><cell>VGGNet</cell><cell>ImageNet</cell><cell>Yes</cell><cell>Votes</cell><cell>N/A, GTX Titan X GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell/><cell>VM</cell></row><row><cell>MRCNN [164]</cell><cell>VGGNet</cell><cell>ImageNet, VOT2015</cell><cell>Yes</cell><cell>DAF</cell><cell>Intel I7 3.50GHz CPU, GTX 1080 GPU</cell><cell>Matlab</cell><cell>MatConvNet</cell><cell>1.2</cell><cell>CM</cell></row><row><cell>CODA [161]</cell><cell>VGG-19, SSD</cell><cell>ImageNet, VOT2013, VOT2014, VOT2015</cell><cell>Yes</cell><cell>DAF</cell><cell>Intel I7-4770K CPU, 32GB RAM, GTX 1070 GPU</cell><cell>Matlab</cell><cell>Caffe</cell><cell>34.8</cell><cell>CM</cell></row><row><cell>ATOM [149]</cell><cell>ResNet-18, IoU-Nets</cell><cell>ImageNet, COCO, LaSOT, TrackingNet</cell><cell>Yes</cell><cell>DAF</cell><cell>N/A, GTX 1080 GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell/><cell>CM</cell></row><row><cell>D3S [192]</cell><cell>ResNet-50</cell><cell>Youtube-VOS</cell><cell>Yes</cell><cell>DAF</cell><cell>N/A, GTX 1080 GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell/><cell>RBB, SGM</cell></row><row><cell>MetaUpdater [217]</cell><cell>ResNet-50</cell><cell>ImageNet, LaSOT</cell><cell>Yes</cell><cell>DAF</cell><cell>Intel I9 CPU, 64GB RAM, GTX 2080Ti GPU</cell><cell>Python</cell><cell>TensorFlow</cell><cell/><cell>CM</cell></row><row><cell>CRAC [207]</cell><cell>ResNet-50</cell><cell>ImageNet, KITTI, VisDrone-2018</cell><cell>Yes</cell><cell>DAF</cell><cell>N/A</cell><cell>Python</cell><cell>PyTorch, MatConvNet</cell><cell/><cell>OS</cell></row><row><cell>COMET [212]</cell><cell>ResNet-50</cell><cell>ImageNet, LaSOT, GOT-10K, NfS, VisDrone-2019</cell><cell>Yes</cell><cell>DAF</cell><cell>N/A, Tesla V100 GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell/><cell>CM</cell></row><row><cell>FGLT [213]</cell><cell>VGG-M, ResNet-50, PWC-Net</cell><cell>ImageNet, ILSVRC-VID, COCO, ILSVRC-DET, YTBB, FlyingChairs, FlyingThings3D</cell><cell>Yes</cell><cell>DAF</cell><cell>Intel Xeon 3.50GHz CPU, GTX 1080Ti GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell>N/A</cell><cell>CM, BB</cell></row><row><cell>LRVN [216]</cell><cell>VGG-M, MobileNet</cell><cell>ImageNet, ILSVRC-VID, ILSVRC-DET</cell><cell>Yes</cell><cell>DAF</cell><cell>Intel I7 CPU, 32GB RAM, GTX Titan X GPU</cell><cell>Python</cell><cell>TensorFlow</cell><cell>2.7</cell><cell>BB, OS</cell></row><row><cell>UCT/UCT-Lite [73]</cell><cell>ResNet101</cell><cell>ImageNet, TC128, UAV123</cell><cell>Only on the first frame</cell><cell>DAF</cell><cell>Intel I7-6700 CPU, 48GB RAM, GTX Titan X GPU</cell><cell>Matlab</cell><cell>Caffe</cell><cell/><cell>CM</cell></row><row><cell>FPRNet [132]</cell><cell>ResNet-101, FlowNet</cell><cell>ImageNet, ILSVRC, SceneFlow</cell><cell>Yes</cell><cell>DAF, DMF</cell><cell>N/A</cell><cell>Matlab</cell><cell>Caffe</cell><cell>N/A</cell><cell>BB</cell></row><row><cell>ADT [160]</cell><cell>-</cell><cell>ImageNet, ALOV300++, UAV123, NUS-PRO</cell><cell>Only on the first frame</cell><cell>DAF</cell><cell>Intel 2.40GHz CPU, GTX TITAN X GPU</cell><cell>Python</cell><cell>TensorFlow</cell><cell/><cell>CM</cell></row><row><cell>DiMP [159]</cell><cell>ResNet-18, ResNet-50</cell><cell>ImageNet, TrackingNet, LaSOT, GOT10k, COCO</cell><cell>Meta-learning</cell><cell>DAF</cell><cell>N/A, GTX 1080 GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell>43 57</cell><cell>OS</cell></row><row><cell>PrDiMP [194]</cell><cell>ResNet-18 or ResNet-50</cell><cell>ImageNet, LaSOT, GOT-10k, TrackingNet, COCO</cell><cell>Meta-learning</cell><cell>DAF</cell><cell>N/A</cell><cell>Python</cell><cell>PyTorch</cell><cell>30 40</cell><cell>CM</cell></row><row><cell>BGBDT [185]</cell><cell>SSD or FasterRCNN</cell><cell>ImageNet, COCO, GOT-10k</cell><cell>Meta-learning</cell><cell>DAF</cell><cell>N/A, GTX 1080 GPU</cell><cell>Python</cell><cell>PyTorch</cell><cell>3 10</cell><cell>BB</cell></row><row><cell>MLT [188]</cell><cell>AlexNet</cell><cell>ImageNet, ILSVRC-2015, ILSVRC-2017</cell><cell>Meta-learning</cell><cell>DAF</cell><cell>Intel I7-4790K 4.0GHz CPU, 32GB RAM, GTX Titan X GPU</cell><cell>Python</cell><cell>TensorFlow</cell><cell>48.1</cell><cell>CM</cell></row><row><cell>ROAM [196]</cell><cell>VGG-16</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row></table><note><p>ImageNet, ILSVRC-VID, ILSVRC-DET, TrackingNet, LaSOT, GOT-10k, COCO Meta-learning DAF Intel I9 3.6GHz CPU, 4 RTX 2080 GPU Python PyTorch CM TMAML [201] ResNet-18 with RetinaNet or FCOS ImageNet, COCO, GOT-10k, TrackingNet, LaSOT Meta-learning DAF N/A, P100 GPU Python N/A BB Meta-Tracker [182] dependent ImageNet, ILSVRC-DET, VOT-2013, VOT-2014, VOT-2015 Meta-learning DAF N/A, GTX Titan X GPU Python PyTorch dependent dependent</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V DATA</head><label>V</label><figDesc>AUGMENTATIONS FOR VISUAL TRACKING METHODS.</figDesc><table/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>OF VISUAL TRACKING DATASETS. THE ABBREVIATIONS ARE DENOTED AS RN: ROW NUMBER, NOV: NUMBER OF VIDEOS, NOF: NUMBER OF FRAMES, NOA: NUMBER OF ATTRIBUTES, OD: OVERLAPPED DATASETS, AL: ABSENT LABELS, NOC: NUMBER OF CLASSES OR CLUSTERS, AD: AVERAGE DURATION (S: SECONDS).</figDesc><table><row><cell>Year</cell><cell>Application</cell><cell>Dataset</cell><cell>Scenario</cell><cell>NoV</cell><cell>NoF</cell><cell>NoA</cell><cell>OD</cell><cell>AL</cell><cell>NoC</cell><cell>AD</cell><cell>Attributes</cell></row><row><cell>2013</cell><cell>Generic</cell><cell>OTB2013</cell><cell>ST</cell><cell>51</cell><cell>29K</cell><cell>11</cell><cell>VOT, OTB2015, TC128</cell><cell>No</cell><cell>10</cell><cell>19.4s</cell><cell>IV, SV, OCC, DEF, MB, FM, IPR, OPR, OV, BC, LR</cell></row><row><cell>2013-2019</cell><cell>Generic</cell><cell>VOT2013-2019</cell><cell>ST</cell><cell>16-60</cell><cell>6K-21K</cell><cell>12</cell><cell>OTB, ALOV++, TC128, UAV123, NUS-PRO</cell><cell>No</cell><cell>12-24</cell><cell>12s</cell><cell>IV, SV, OCC, DEF, MB, BC, ARC, CM, MOC, OCO, SCO, AM</cell></row><row><cell>2014</cell><cell>Generic</cell><cell>ALOV++</cell><cell>ST</cell><cell>314</cell><cell>89K</cell><cell>14</cell><cell>VOT, YouTube</cell><cell>No</cell><cell>64</cell><cell>16.2s</cell><cell>OCC, BC, CM, LI, SC, SP, TR, SH, MS, MCO, CON, LC, ZC, LD</cell></row><row><cell>2015</cell><cell>Generic</cell><cell>OTB2015</cell><cell>ST</cell><cell>100</cell><cell>59K</cell><cell>11</cell><cell>OTB2013, VOT, TC128</cell><cell>No</cell><cell>16</cell><cell>19.8s</cell><cell>IV, SV, OCC, DEF, MB, FM, IPR, OPR, OV, BC, LR</cell></row><row><cell>2015</cell><cell>Generic</cell><cell>TC128</cell><cell>ST</cell><cell>129</cell><cell>55K</cell><cell>11</cell><cell>OTB, VOT</cell><cell>No</cell><cell>27</cell><cell>15.6s</cell><cell>IV, SV, OCC, DEF, MB, FM, IPR, OPR, OV, BC, LR</cell></row><row><cell>2016</cell><cell>UAV</cell><cell>UAV123</cell><cell>ST</cell><cell>123</cell><cell>113K</cell><cell>12</cell><cell>VOT</cell><cell>No</cell><cell>9</cell><cell>30.6s</cell><cell>IV, SV, FM, OV, BC, LR, ARC, CM, FCM, FOC, POC, SIB, VC</cell></row><row><cell>2016</cell><cell>UAV</cell><cell>UAV20L</cell><cell>LT</cell><cell>20</cell><cell>59K</cell><cell>12</cell><cell>VOT</cell><cell>No</cell><cell>5</cell><cell>75s</cell><cell>IV, SV, FM, OV, BC, LR, ARC, CM, FCM, FOC, POC, SIB, VC</cell></row><row><cell>2016</cell><cell>Generic</cell><cell>NUS-PRO</cell><cell>ST</cell><cell>365</cell><cell>135K</cell><cell>12</cell><cell>VOT, YouTube</cell><cell>No</cell><cell>17</cell><cell>12.6s</cell><cell>SV, DEF, BC, FOC, POC, SIB, SHC, FL, DL, CS, ROT, FBC</cell></row><row><cell>2017</cell><cell>Generic</cell><cell>NfS</cell><cell>ST</cell><cell>100</cell><cell>383K</cell><cell>9</cell><cell>YouTube</cell><cell>No</cell><cell>17</cell><cell>15.6s</cell><cell>IV, SV, OCC, DEF, FM, OV, BC, LR, VC</cell></row><row><cell>2017</cell><cell>UAV</cell><cell>DTB</cell><cell>ST</cell><cell>70</cell><cell>15K</cell><cell>11</cell><cell>YouTube</cell><cell>No</cell><cell>15</cell><cell>7.2s</cell><cell>SV, OCC, DEF, MB, IPR, OPR, OV, BC, ARC, FCM, SIB</cell></row><row><cell>2018</cell><cell>Generic</cell><cell>TrackingNet</cell><cell>ST</cell><cell>30643</cell><cell>14.43M</cell><cell>15</cell><cell>YTBB</cell><cell>No</cell><cell>27</cell><cell>16.6s</cell><cell>IV, SV, DEF, MB, FM, IPR, OPR, OV, BC, LR, ARC, CM, FOC, POC, SIB</cell></row><row><cell>2018</cell><cell>Generic</cell><cell>TinyTLP / TLPattr</cell><cell>ST</cell><cell>50</cell><cell>30K</cell><cell>6</cell><cell>YouTube</cell><cell>No</cell><cell>N/A</cell><cell>20s</cell><cell>FM, IV, SV, POC, OV, BC</cell></row><row><cell>2018</cell><cell>Generic</cell><cell>OxUvA</cell><cell>LT</cell><cell>366</cell><cell>1.55M</cell><cell>6</cell><cell>YTBB</cell><cell>Yes</cell><cell>22</cell><cell>144s</cell><cell>SV, OV, SZ, RS, DI, LE</cell></row><row><cell>2018</cell><cell>Generic</cell><cell>TLP</cell><cell>LT</cell><cell>50</cell><cell>676K</cell><cell>0</cell><cell>YouTube</cell><cell>No</cell><cell>N/A</cell><cell>484.8s</cell><cell>-</cell></row><row><cell>2018</cell><cell>UAV</cell><cell>BUAA-PRO</cell><cell>ST</cell><cell>150</cell><cell>8.7K</cell><cell>12</cell><cell>NUS-PRO, YTBB</cell><cell>No</cell><cell>12</cell><cell>2s</cell><cell>SV, DEF, BC, FOC, POC, SIB, SHC, FL, DL, CS, ROT, FBC</cell></row><row><cell>2018</cell><cell>Generic</cell><cell>GOT10k</cell><cell>ST</cell><cell>10000</cell><cell>1.5M</cell><cell>6</cell><cell>VOT, WordNet, ImageNet</cell><cell>Yes</cell><cell>563</cell><cell>16s</cell><cell>IV, SV, OCC, FM, ARC, LO</cell></row><row><cell>2018</cell><cell>UAV</cell><cell>UAVDT</cell><cell>ST</cell><cell>50</cell><cell>80K</cell><cell>9</cell><cell>-</cell><cell>No</cell><cell>3</cell><cell>N/A</cell><cell>BC, CM, OM, SOB, IV, OB, SV, LOC, LT</cell></row><row><cell>2018</cell><cell>Generic</cell><cell>LTB35 / VOT2018-LT</cell><cell>LT</cell><cell>35</cell><cell>147K</cell><cell>10</cell><cell>YouTube, VOT, UAV20L</cell><cell>No</cell><cell>6</cell><cell>N/A</cell><cell>FOC, POC, OV, CM, FM, SC, ARC, VC, SIB, DEF</cell></row><row><cell>2018-2020</cell><cell>UAV</cell><cell>VisDrone2018-2020</cell><cell>ST</cell><cell>132</cell><cell>106.4K</cell><cell>12</cell><cell>-</cell><cell>No</cell><cell>4</cell><cell>N/A</cell><cell>IV, SV, FM, OV, BC, LR, ARC, CM, FOC, POC, SIB, VC</cell></row><row><cell>2019-2020</cell><cell>UAV</cell><cell>VisDrone2019-2020L</cell><cell>LT</cell><cell>25</cell><cell>82.6K</cell><cell>12</cell><cell>-</cell><cell>No</cell><cell>4</cell><cell>N/A</cell><cell>IV, SV, FM, OV, BC, LR, ARC, CM, FOC, POC, SIB, VC</cell></row><row><cell>2019</cell><cell>Generic</cell><cell>LaSOT</cell><cell>LT</cell><cell>1400</cell><cell>3.5M</cell><cell>14</cell><cell>YouTube, ImageNet</cell><cell>Yes</cell><cell>70</cell><cell>84.3s</cell><cell>IV, SV, DEF, MB, FM, OV, BC, LR, ARC, CM, FOC, POC, VC, ROT</cell></row><row><cell>2020</cell><cell>UAV</cell><cell>Small-90</cell><cell>ST</cell><cell>90</cell><cell>N/A</cell><cell>11</cell><cell>UAV123, VOT, OTB, TC128</cell><cell>No</cell><cell>N/A</cell><cell>N/A</cell><cell>IV, SV, OCC, DEF, MB, FM, IPR, OPR, OV, BC, LR</cell></row><row><cell>2020</cell><cell>UAV</cell><cell>Small-112</cell><cell>ST</cell><cell>112</cell><cell>N/A</cell><cell>11</cell><cell>UAV123, VOT, OTB, TC128, VisDrone</cell><cell>No</cell><cell>N/A</cell><cell>N/A</cell><cell>IV, SV, OCC, DEF, MB, FM, IPR, OPR, OV, BC, LR</cell></row><row><cell>2021</cell><cell>Generic</cell><cell>TracKlinic</cell><cell>ST</cell><cell>2390</cell><cell>280K</cell><cell>9</cell><cell>OTB, TC128, LaSOT</cell><cell>No</cell><cell>N/A</cell><cell>N/A</cell><cell>IV, SV, OCC, MB, OV, BC, ROT, O-B, O-R</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII STATE</head><label>VII</label><figDesc>-OF-THE-ART VISUAL TRACKING METHODS FOR EXPERIMENTAL COMPARISONS ON VISUAL TRACKING DATASETS.</figDesc><table><row><cell>Published in</cell><cell>Visual Tracking Method</cell><cell>Exploited Features</cell><cell>Test Datasets</cell></row><row><cell>ICCV 2015</cell><cell>HCFT [51]</cell><cell>DAF</cell><cell>OTB, LaSOT</cell></row><row><cell>ICCV 2015</cell><cell>DeepSRDCF [52]</cell><cell>DAF, HOG</cell><cell>OTB, VOT2018</cell></row><row><cell>ECCV 2016</cell><cell>CCOT [56]</cell><cell>DAF</cell><cell>OTB, VOT2018, UAVDT</cell></row><row><cell>ECCVW 2016</cell><cell>SiamFC [58]</cell><cell>DAF</cell><cell>OTB, LaSOT, UAVDT</cell></row><row><cell>CVPR 2016</cell><cell>SINT [59]</cell><cell>DAF</cell><cell>OTB, LaSOT, UAVDT</cell></row><row><cell>CVPR 2016</cell><cell>MDNet [60]</cell><cell>DAF</cell><cell>OTB, LaSOT, UAVDT</cell></row><row><cell>CVPR 2016</cell><cell>HDT [61]</cell><cell>DAF</cell><cell>OTB, UAVDT</cell></row><row><cell>ICCV 2017, TIP 2019</cell><cell>PTAV [70], [71]</cell><cell>DAF, HOG</cell><cell>OTB, LaSOT, UAVDT</cell></row><row><cell>ICCV 2017</cell><cell>CREST [72]</cell><cell>DAF</cell><cell>OTB, UAVDT</cell></row><row><cell>ICCV 2017</cell><cell>Meta-CREST [72]</cell><cell>DAF</cell><cell>OTB</cell></row><row><cell>ICCV 2017</cell><cell>UCT [73]</cell><cell>DAF</cell><cell>OTB, VOT2018</cell></row><row><cell>ICCV 2017</cell><cell>DSiam [74]</cell><cell>DAF</cell><cell>VOT2018, LaSOT</cell></row><row><cell>CVPR 2017</cell><cell>CFNet [86]</cell><cell>DAF</cell><cell>OTB, VOT2018, LaSOT, UAVDT</cell></row><row><cell>CVPR 2017</cell><cell>ECO [87]</cell><cell>DAF, HOG, CN</cell><cell>OTB, VOT2018, LaSOT, UAV123, UAVDT, VisDrone2019</cell></row><row><cell>CVPR 2017</cell><cell>DeepCSRDCF [88]</cell><cell>DAF, HOG, CN</cell><cell>VOT2018, LaSOT</cell></row><row><cell>CVPR 2017</cell><cell>MCPF [89]</cell><cell>DAF</cell><cell>OTB, VOT2018</cell></row><row><cell>CVPR 2017</cell><cell>ACFN [93]</cell><cell>DAF, HOG, Color</cell><cell>OTB</cell></row><row><cell>arXiv 2017</cell><cell>DCFNet [95]</cell><cell>DAF</cell><cell>OTB</cell></row><row><cell>arXiv 2017</cell><cell>DCFNet2 [95]</cell><cell>DAF</cell><cell>OTB, VOT2018</cell></row><row><cell>ECCV 2018</cell><cell>TripletLoss-CFNet [100]</cell><cell>DAF</cell><cell>OTB</cell></row><row><cell>ECCV 2018</cell><cell>TripletLoss-SiamFC [100]</cell><cell>DAF</cell><cell>OTB</cell></row><row><cell>ECCV 2018</cell><cell>TripletLoss-CFNet2 [100]</cell><cell>DAF</cell><cell>OTB</cell></row><row><cell>ECCV 2018</cell><cell>UPDT [102]</cell><cell>DAF, HOG, CN</cell><cell>VOT2018</cell></row><row><cell>ECCV 2018</cell><cell>DaSiamRPN [104]</cell><cell>DAF</cell><cell>VOT2018, UAV123</cell></row><row><cell>ECCV 2018</cell><cell>StructSiam [106]</cell><cell>DAF</cell><cell>LaSOT</cell></row><row><cell>ECCVW 2018</cell><cell>Siam-MCF [110]</cell><cell>DAF</cell><cell>OTB</cell></row><row><cell>CVPR 2018</cell><cell>TRACA [113]</cell><cell>CDAF</cell><cell>OTB, VOT2018, LaSOT</cell></row><row><cell>CVPR 2018</cell><cell>VITAL [114]</cell><cell>DAF</cell><cell>OTB, LaSOT</cell></row><row><cell>CVPR 2018</cell><cell>DeepSTRCF [115]</cell><cell>DAF, HOG, CN</cell><cell>OTB, VOT2018, LaSOT, UAV123</cell></row><row><cell>CVPR 2018</cell><cell>SiamRPN [116]</cell><cell>DAF</cell><cell>OTB, VOT2018, UAV123</cell></row><row><cell>CVPR 2018</cell><cell>SA-Siam [117]</cell><cell>DAF</cell><cell>OTB, VOT2018</cell></row><row><cell>CVPR 2018</cell><cell>LSART [120]</cell><cell>DAF</cell><cell>VOT2018</cell></row><row><cell>CVPR 2018</cell><cell>DRT [119]</cell><cell>DAF, HOG, CN</cell><cell>VOT2018</cell></row><row><cell>NIPS 2018</cell><cell>DAT [130]</cell><cell>DAF</cell><cell>OTB, VOT2018</cell></row><row><cell>PAMI 2018</cell><cell>HCFTs [133]</cell><cell>DAF</cell><cell>OTB</cell></row><row><cell>IJCV 2018</cell><cell>LCTdeep [142]</cell><cell>DAF</cell><cell>OTB</cell></row><row><cell>TIP 2018</cell><cell>CFCF [137]</cell><cell>DAF, HOG</cell><cell>VOT2018</cell></row><row><cell>CVPR 2019</cell><cell>C-RPN [150]</cell><cell>DAF</cell><cell>OTB, VOT2018, LaSOT</cell></row><row><cell>CVPR 2019</cell><cell>GCT [151]</cell><cell>DAF</cell><cell>OTB, VOT2018, UAV123</cell></row><row><cell>CVPR 2019</cell><cell>SiamMask [155]</cell><cell>DAF</cell><cell>VOT2018, UAVDT, VisDrone2019</cell></row><row><cell>CVPR 2019</cell><cell>SiamRPN++ [156]</cell><cell>DAF</cell><cell>OTB, VOT2018, UAV123, UAVDT, VisDrone2019</cell></row><row><cell>CVPR 2019</cell><cell>TADT [157]</cell><cell>DAF</cell><cell>OTB</cell></row><row><cell>CVPR 2019</cell><cell>ASRCF [148]</cell><cell>DAF, HOG</cell><cell>OTB, LaSOT</cell></row><row><cell>CVPR 2019</cell><cell>SiamDW-SiamRPN [154]</cell><cell>DAF</cell><cell>OTB, VOT2018, UAVDT, VisDrone2019</cell></row><row><cell>CVPR 2019</cell><cell>SiamDW-SiamFC [154]</cell><cell>DAF</cell><cell>OTB, VOT2018</cell></row><row><cell>CVPR 2019</cell><cell>ATOM [149]</cell><cell>DAF</cell><cell>VOT2018, LaSOT, UAV123, UAVDT, VisDrone2019</cell></row><row><cell>ICCV 2019</cell><cell>DiMP50 [159]</cell><cell>DAF</cell><cell>VOT2018, LaSOT, UAV123, UAVDT, VisDrone2019</cell></row><row><cell>CVPR 2020</cell><cell>PrDiMP50 [194]</cell><cell>DAF</cell><cell>LaSOT, UAVDT, VisDrone2019</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII FIVE</head><label>VIII</label><figDesc>MOST CHALLENGING ATTRIBUTES OF BENCHMARK DATASETS. [FIRST TO THIRD CHALLENGING ATTRIBUTES ARE SHOWN BY RED , YELLOW , Comparison of state-of-the-art trackers in terms of the most challenging attributes on the OTB2015, LaSOT, UAV123, UAVDT, and VisDrone2019 datasets, left to right column, respectively.</figDesc><table><row><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell cols="5">AND GREEN COLORS.]</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>Dataset</cell><cell>Metric</cell><cell>IV</cell><cell>DEF</cell><cell>MB</cell><cell>CM</cell><cell>OCC</cell><cell>POC</cell><cell>FOC</cell><cell>ROT</cell><cell>IPR</cell><cell>OPR</cell><cell>BC</cell><cell>VC</cell><cell>SV</cell><cell>FM</cell><cell>OV</cell><cell>LR</cell><cell>ARC</cell><cell>MC</cell><cell>SIB</cell><cell>OM</cell><cell>SOB</cell><cell>OB</cell><cell>LT</cell><cell>LOC</cell></row><row><cell>Fig. 6.</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head/><label/><figDesc>addition to providing a desirable balance between the performance and speed of Siamese or custom network-based trackers, the architectures are modified to integrate with diverse deep backbone networks, searching strategies, and learning schemes but also exploit fully convolutional networks, correlation layers, region proposal networks, video object detection/segmentation modules. The interesting point is that five SNN-based methods including the SiamDW-SiamRPN, SiamRPN++, C-RPN, SiamMask, and DaSiamRPN are based on the fast SiamRPN method<ref type="bibr" target="#b115">[116]</ref>, which is consisted of Siamese subnetwork and region proposal subnetwork; these subnetworks are leveraged for feature extraction and proposal extraction on correlation feature maps to solve the visual tracking problem by one-shot detection task. The main advantages of SiamRPN are the time efficiency and precise estimations with integrating proposal selection and refinement strategies into a Siamese network.Interestingly, the ASRCF, UPDT, DRT, and DeepSTRCF, which exploit deep off-the-shelf features, are among the topperforming visual tracking methods. Moreover, five methods of UPDT, DeepSTRCF, DRT, LSART, and ASRCF take the advantages of the DCF framework. On the other side, the best performing visual trackers, namely PrDiMP50, DiMP50, ATOM, VITAL, MDNet, DAT, SiamDW, SiamRPN++, C-RPN, StructSiam, SiamMask, DaSiamRPN, and LSART exploit specialized deep features for visual tracking purpose.</figDesc><table/></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We wish to thank <rs type="person">Prof. Kamal Nasrollahi</rs> (<rs type="affiliation">Visual Analysis of People Lab (VAP), Aalborg University, Denmark</rs>) for his beneficial comments.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"/>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Manifold Siamese Network: A Novel Visual Tracking ConvNet for Autonomous Vehicles</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gao</surname></persName>
			<idno type="ORCID">0000-0003-4199-4470</idno>
		</author>
		<author>
			<persName><forename type="first">Lisheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuying</forename><surname>Jiang</surname></persName>
			<idno type="ORCID">0000-0002-4036-8024</idno>
		</author>
		<author>
			<persName><forename type="first">Baicang</forename><surname>Guo</surname></persName>
			<idno type="ORCID">0000-0002-1130-4232</idno>
		</author>
		<idno type="DOI">10.1109/tits.2019.2930337</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<title level="j" type="abbrev">IEEE Trans. Intell. Transport. Syst.</title>
		<idno type="ISSN">1524-9050</idno>
		<idno type="ISSNe">1558-0016</idno>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1612" to="1623"/>
			<date type="published" when="2019">2019</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-robot target detection and tracking: taxonomy and survey</title>
		<author>
			<persName><forename type="first">Cyril</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacroix</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10514-015-9491-7</idno>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<title level="j" type="abbrev">Auton Robot</title>
		<idno type="ISSN">0929-5593</idno>
		<idno type="ISSNe">1573-7527</idno>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="729" to="760"/>
			<date type="published" when="2016">2016</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ground-Moving-Platform-Based Human Tracking Using Visual SLAM and Constrained Multiple Kernels</title>
		<author>
			<persName><forename type="first">Kuan-Hui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Okopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pitton</surname></persName>
		</author>
		<idno type="DOI">10.1109/tits.2016.2557763</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<title level="j" type="abbrev">IEEE Trans. Intell. Transport. Syst.</title>
		<idno type="ISSN">1524-9050</idno>
		<idno type="ISSNe">1558-0016</idno>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3602" to="3612"/>
			<date type="published" when="2016-12">2016</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vision-Based Tracking for Mobile Augmented Reality</title>
		<author>
			<persName><forename type="first">Fakhreddine</forename><surname>Ababsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madjid</forename><surname>Maidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Yves</forename><surname>Didier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malik</forename><surname>Mallem</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-78502-6_12</idno>
	</analytic>
	<monogr>
		<title level="m">Studies in Computational Intelligence</title>
		<imprint>
			<publisher>Springer Berlin Heidelberg</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="297" to="326"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Review of Target Tracking Algorithm Based on UAV</title>
		<author>
			<persName><forename type="first">Jingxuan</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yimin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingtian</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/cbs.2018.8612263</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Cyborg and Bionic Systems (CBS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="328" to="333"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey on player tracking in soccer videos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Manafifard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ebadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abrishami Moghaddam</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cviu.2017.02.002</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<title level="j" type="abbrev">Computer Vision and Image Understanding</title>
		<idno type="ISSN">1077-3142</idno>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="19" to="46"/>
			<date type="published" when="2017-06">2017</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vision-based and marker-less surgical tool detection and tracking: a review of the literature</title>
		<author>
			<persName><forename type="first">David</forename><surname>Bouget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danail</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Jannin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2016.09.003</idno>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<title level="j" type="abbrev">Medical Image Analysis</title>
		<idno type="ISSN">1361-8415</idno>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="633" to="654"/>
			<date type="published" when="2017-01">2017</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An objective comparison of cell-tracking algorithms</title>
		<author>
			<persName><forename type="first">Vladimír</forename><surname>Ulman</surname></persName>
			<idno type="ORCID">0000-0002-4270-7982</idno>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Maška</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klas</forename><forename type="middle">E G</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Haubold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Harder</surname></persName>
			<idno type="ORCID">0000-0002-6171-6462</idno>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Matula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Matula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Svoboda</surname></persName>
			<idno type="ORCID">0000-0001-6074-0164</idno>
		</author>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Radojevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ihor</forename><surname>Smal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Rohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Jaldén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><forename type="middle">M</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleh</forename><surname>Dzyubachyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boudewijn</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengdong</forename><surname>Xiao</surname></persName>
			<idno type="ORCID">0000-0001-5178-2274</idno>
		</author>
		<author>
			<persName><forename type="first">Yuexiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siu-Yeung</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><forename type="middle">C</forename><surname>Dufour</surname></persName>
			<idno type="ORCID">0000-0002-9417-7389</idno>
		</author>
		<author>
			<persName><forename type="first">Jean-Christophe</forename><surname>Olivo-Marin</surname></persName>
			<idno type="ORCID">0000-0001-6796-0696</idno>
		</author>
		<author>
			<persName><forename type="first">Constantino</forename><forename type="middle">C</forename><surname>Reyes-Aldasoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">A</forename><surname>Solis-Lemus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bensch</surname></persName>
			<idno type="ORCID">0000-0001-8618-3167</idno>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Stegmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Mikut</surname></persName>
			<idno type="ORCID">0000-0001-9100-5496</idno>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Esteves</surname></persName>
			<idno type="ORCID">0000-0003-3407-6436</idno>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Quelhas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ömer</forename><surname>Demirel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Malmström</surname></persName>
			<idno type="ORCID">0000-0001-9885-9312</idno>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Jug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Tomancak</surname></persName>
			<idno type="ORCID">0000-0002-2222-9370</idno>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Meijering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arrate</forename><surname>Muñoz-Barrutia</surname></persName>
			<idno type="ORCID">0000-0002-1573-1661</idno>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Kozubek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Ortiz-De-Solorzano</surname></persName>
			<idno type="ORCID">0000-0001-8720-0205</idno>
		</author>
		<idno type="DOI">10.1038/nmeth.4473</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<title level="j" type="abbrev">Nat Methods</title>
		<idno type="ISSN">1548-7091</idno>
		<idno type="ISSNe">1548-7105</idno>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1141" to="1152"/>
			<date type="published" when="2017-10-30">2017</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Underwater Acoustic Target Tracking: A Review</title>
		<author>
			<persName><forename type="first">Junhai</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liying</forename><surname>Fan</surname></persName>
		</author>
		<idno type="DOI">10.3390/s18010112</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<title level="j" type="abbrev">Sensors</title>
		<idno type="ISSNe">1424-8220</idno>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">112</biblScope>
			<date type="published" when="2018-01-02">2018</date>
			<publisher>MDPI AG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High-Speed Tracking with Kernelized Correlation Filters</title>
		<author>
			<persName><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Batista</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2014.2345390</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<title level="j" type="abbrev">IEEE Trans. Pattern Anal. Mach. Intell.</title>
		<idno type="ISSN">0162-8828</idno>
		<idno type="ISSNe">2160-9292</idno>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="583" to="596"/>
			<date type="published" when="2015-03-01">2015</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real-Time Scalable Visual Tracking via Quadrangle Kernelized Correlation Filters</title>
		<author>
			<persName><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
			<idno type="ORCID">0000-0003-0137-9975</idno>
		</author>
		<author>
			<persName><forename type="first">Wenshuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
			<idno type="ORCID">0000-0001-5843-6411</idno>
		</author>
		<author>
			<persName><forename type="first">Jungong</forename><surname>Han</surname></persName>
			<idno type="ORCID">0000-0003-4361-956X</idno>
		</author>
		<author>
			<persName><forename type="first">Qiaoyan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/tits.2017.2774778</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<title level="j" type="abbrev">IEEE Trans. Intell. Transport. Syst.</title>
		<idno type="ISSN">1524-9050</idno>
		<idno type="ISSNe">1558-0016</idno>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="140" to="150"/>
			<date type="published" when="2018-01">2018</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rotation-Aware Discriminative Scale Space Tracking</title>
		<author>
			<persName><forename type="first">Seyed</forename><forename type="middle">Mojtaba</forename><surname>Marvasti-Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Ghanei-Yakhdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shohreh</forename><surname>Kasaei</surname></persName>
		</author>
		<idno type="DOI">10.1109/iraniancee.2019.8786548</idno>
	</analytic>
	<monogr>
		<title level="m">2019 27th Iranian Conference on Electrical Engineering (ICEE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-04">2019</date>
			<biblScope unit="page" from="1272" to="1276"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive exploitation of pre-trained deep convolutional neural networks for robust visual tracking</title>
		<author>
			<persName><forename type="first">Seyed</forename><forename type="middle">Mojtaba</forename><surname>Marvasti-Zadeh</surname></persName>
			<idno type="ORCID">0000-0003-0536-0796</idno>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Ghanei-Yakhdan</surname></persName>
			<idno type="ORCID">0000-0003-4575-1062</idno>
		</author>
		<author>
			<persName><forename type="first">Shohreh</forename><surname>Kasaei</surname></persName>
			<idno type="ORCID">0000-0002-3831-0878</idno>
		</author>
		<idno type="DOI">10.1007/s11042-020-10382-x</idno>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<title level="j" type="abbrev">Multimed Tools Appl</title>
		<idno type="ISSN">1380-7501</idno>
		<idno type="ISSNe">1573-7721</idno>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="22027" to="22076"/>
			<date type="published" when="2021-03-23">2021</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive spatio-temporal context learning for visual target tracking</title>
		<author>
			<persName><forename type="first">Seyed</forename><forename type="middle">Mojtaba</forename><surname>Marvasti-Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Ghanei-Yakhdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shohreh</forename><surname>Kasaei</surname></persName>
		</author>
		<idno type="DOI">10.1109/iranianmvip.2017.8342331</idno>
		<ptr target="http://arxiv.org/abs/2004.02932"/>
	</analytic>
	<monogr>
		<title level="m">2017 10th Iranian Conference on Machine Vision and Image Processing (MVIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient scale estimation methods using lightweight deep convolutional neural networks for visual tracking</title>
		<author>
			<persName><forename type="first">Seyed</forename><forename type="middle">Mojtaba</forename><surname>Marvasti-Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Ghanei-Yakhdan</surname></persName>
			<idno type="ORCID">0000-0003-4575-1062</idno>
		</author>
		<author>
			<persName><forename type="first">Shohreh</forename><surname>Kasaei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-020-05586-z</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<title level="j" type="abbrev">Neural Comput &amp; Applic</title>
		<idno type="ISSN">0941-0643</idno>
		<idno type="ISSNe">1433-3058</idno>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="8319" to="8334"/>
			<date type="published" when="2021-01-02">2021</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Effective fusion of deep multitasking representations for robust visual tracking</title>
		<author>
			<persName><forename type="first">Seyed</forename><forename type="middle">Mojtaba</forename><surname>Marvasti-Zadeh</surname></persName>
			<idno type="ORCID">0000-0003-0536-0796</idno>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Ghanei-Yakhdan</surname></persName>
			<idno type="ORCID">0000-0003-4575-1062</idno>
		</author>
		<author>
			<persName><forename type="first">Shohreh</forename><surname>Kasaei</surname></persName>
			<idno type="ORCID">0000-0002-3831-0878</idno>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Nasrollahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00371-021-02304-1</idno>
		<ptr target="http://arxiv.org/abs/2004.01382"/>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<title level="j" type="abbrev">Vis Comput</title>
		<idno type="ISSN">0178-2789</idno>
		<idno type="ISSNe">1432-2315</idno>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4397" to="4417"/>
			<date type="published" when="2020">2020</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient tracking with distinctive target colors and silhouette</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICPR</title>
		<meeting>ICPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2728" to="2733"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An Improvement of Kernel-Based Object Tracking Based on Human Perception</title>
		<author>
			<persName><forename type="first">Vittoria</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domenico</forename><surname>Vitulano</surname></persName>
		</author>
		<idno type="DOI">10.1109/tsmc.2014.2331217</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics: Systems</title>
		<title level="j" type="abbrev">IEEE Trans. Syst. Man Cybern, Syst.</title>
		<idno type="ISSN">2168-2216</idno>
		<idno type="ISSNe">2168-2232</idno>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1474" to="1485"/>
			<date type="published" when="2014-11">2014</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tracking of Moving Objects With Regeneration of Object Feature Points</title>
		<author>
			<persName><forename type="first">Igor</forename><forename type="middle">I</forename><surname>Lychkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">N</forename><surname>Alfimtsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><forename type="middle">A</forename><surname>Sakulin</surname></persName>
		</author>
		<idno type="DOI">10.1109/glosic.2018.8570061</idno>
	</analytic>
	<monogr>
		<title level="m">2018 Global Smart Industry Conference (GloSIC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-11">2018</date>
			<biblScope unit="page" from="1" to="6"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Histograms of Oriented Gradients for Human Detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2005.177</idno>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="886" to="893"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning Color Names from Real-World Images</title>
		<author>
			<persName><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2007.383218</idno>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007-06">2007</date>
			<biblScope unit="page" from="1" to="8"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning Spatially Regularized Correlation Filters for Visual Tracking</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustav</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2015.490</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-12">2015</date>
			<biblScope unit="page" from="4310" to="4318"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive Decontamination of the Training Set: A Unified Formulation for Discriminative Visual Tracking</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustav</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.159</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">2016</date>
			<biblScope unit="page" from="1430" to="1438"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning Background-Aware Correlation Filters for Visual Tracking</title>
		<author>
			<persName><forename type="first">Hamed</forename><forename type="middle">Kiani</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashton</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.129</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10">2017</date>
			<biblScope unit="page" from="1144" to="1152"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">AutoTrack: Towards High-Performance Visual Tracking for UAV With Automatic Spatio-Temporal Regularization</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangqiang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.01194</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning Aberrance Repressed Correlation Filters for Real-Time UAV Tracking</title>
		<author>
			<persName><forename type="first">Ziyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuling</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2019.00298</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10">2019</date>
			<biblScope unit="page" from="2891" to="2900"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Training-Set Distillation for Real-Time UAV Object Tracking</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuling</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/icra40945.2020.9197252</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-05">2020</date>
			<biblScope unit="page" from="1" to="7"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1145/3065386</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<title level="j" type="abbrev">Commun. ACM</title>
		<idno type="ISSN">0001-0782</idno>
		<idno type="ISSNe">1557-7317</idno>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90"/>
			<date type="published" when="2012">2012</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Return of the Devil in the Details: Delving Deep into Convolutional Nets</title>
		<author>
			<persName><forename type="first">Ken</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.5244/c.28.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference 2014</title>
		<meeting>the British Machine Vision Conference 2014</meeting>
		<imprint>
			<publisher>British Machine Vision Association</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="11"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="14"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><surname>Wei Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Yangqing Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298594</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-06">2015</date>
			<biblScope unit="page" from="1" to="9"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">2016</date>
			<biblScope unit="page" from="770" to="778"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<title level="j" type="abbrev">Int J Comput Vis</title>
		<idno type="ISSN">0920-5691</idno>
		<idno type="ISSNe">1573-1405</idno>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252"/>
			<date type="published" when="2015-04-11">2015</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The visual object tracking VOT2013 challenge results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="98" to="111"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The Visual Object Tracking VOT2014 Challenge Results</title>
		<author>
			<persName><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleš</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luka</forename><surname>Čehovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Vojíř</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Lukežič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Dimitriev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfredo</forename><surname>Petrosino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cherkeng</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Pangeršič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustav</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franci</forename><surname>Oven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horst</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jijia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Woo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Lebeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristoffer</forename><surname>Öfjäll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwang</forename><forename type="middle">Moo</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><forename type="middle">Edoardo</forename><surname>Maresca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samantha</forename><forename type="middle">Yueying</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Duffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mauthner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuankai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><forename type="middle">Heng</forename><surname>Niu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-16181-5_14</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision - ECCV 2014 Workshops</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="191" to="217"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The Visual Object Tracking VOT2015 Challenge Results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccvw.2015.79</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-12">2015</date>
			<biblScope unit="page" from="564" to="586"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The visual object tracking VOT2016 challenge results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCVW</title>
		<meeting>ECCVW</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="777" to="823"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The visual object tracking VOT2017 challenge results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Zajc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCVW</title>
		<meeting>IEEE ICCVW</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1949" to="1972"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The sixth visual object tracking VOT2018 challenge results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCVW</title>
		<meeting>ECCVW</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3" to="53"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The Visual Object Tracking VOT2015 Challenge Results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccvw.2015.79</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Object tracking</title>
		<author>
			<persName><forename type="first">Alper</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.1145/1177352.1177355</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<title level="j" type="abbrev">ACM Comput. Surv.</title>
		<idno type="ISSN">0360-0300</idno>
		<idno type="ISSNe">1557-7341</idno>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2006-12-25">Dec. 2006</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visual Tracking: An Experimental Survey</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2013.230</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<title level="j" type="abbrev">IEEE Trans. Pattern Anal. Mach. Intell.</title>
		<idno type="ISSN">0162-8828</idno>
		<idno type="ISSNe">2160-9292</idno>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1442" to="1468"/>
			<date type="published" when="2014-07">2014</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Recent advances and trends in visual tracking: A review</title>
		<author>
			<persName><forename type="first">Hanxuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2011.07.024</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<title level="j" type="abbrev">Neurocomputing</title>
		<idno type="ISSN">0925-2312</idno>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="3823" to="3831"/>
			<date type="published" when="2011-11">2011</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A survey of appearance models in visual object tracking</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><forename type="middle">Van Den</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="DOI">10.1145/2508037.2508039</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<title level="j" type="abbrev">ACM Trans. Intell. Syst. Technol.</title>
		<idno type="ISSN">2157-6904</idno>
		<idno type="ISSNe">2157-6912</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="48"/>
			<date type="published" when="2013-09">2013</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Boundary Effect-Aware Visual Tracking for UAV with Online Enhanced Background Learning and Multi-Frame Consensus Verification</title>
		<author>
			<persName><forename type="first">Changhong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/iros40897.2019.8967674</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11">2019</date>
			<biblScope unit="page" from="4415" to="4422"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Keyfilter-Aware Real-Time UAV Object Tracking</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1109/icra40945.2020.9196943</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-05">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep Siamese Networks toward Robust Visual Tracking</title>
		<author>
			<persName><forename type="first">Mustansar</forename><surname>Fiaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arif</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soon</forename><surname>Ki Jung</surname></persName>
		</author>
		<idno type="DOI">10.5772/intechopen.86235</idno>
		<ptr target="http://arxiv.org/abs/1802.03098"/>
	</analytic>
	<monogr>
		<title level="m">Visual Object Tracking with Deep Neural Networks</title>
		<imprint>
			<publisher>IntechOpen</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Handcrafted and Deep Trackers</title>
		<author>
			<persName><forename type="first">Mustansar</forename><surname>Fiaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arif</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sajid</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soon</forename><forename type="middle">Ki</forename><surname>Jung</surname></persName>
		</author>
		<idno type="DOI">10.1145/3309665</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<title level="j" type="abbrev">ACM Comput. Surv.</title>
		<idno type="ISSN">0360-0300</idno>
		<idno type="ISSNe">1557-7341</idno>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="44"/>
			<date type="published" when="2019-04-30">2019</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep visual tracking: Review and experimental comparison</title>
		<author>
			<persName><forename type="first">Peixia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2017.11.007</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<title level="j" type="abbrev">Pattern Recognition</title>
		<idno type="ISSN">0031-3203</idno>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="323" to="338"/>
			<date type="published" when="2018-04">2018</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">An in-depth analysis of visual tracking with Siamese neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1707.00569"/>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Hierarchical Convolutional Features for Visual Tracking</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2015.352</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-12">2015</date>
			<biblScope unit="page" from="3074" to="3082"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Convolutional Features for Correlation Filter Based Visual Tracking</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustav</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccvw.2015.84</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="621" to="629"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Visual Tracking with Fully Convolutional Networks</title>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2015.357</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-12">2015</date>
			<biblScope unit="page" from="3119" to="3127"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Online tracking by learning discriminative saliency map with convolutional neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="597" to="606"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep Position-Sensitive Tracking</title>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Zha</surname></persName>
			<idno type="ORCID">0000-0001-5013-2501</idno>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Ku</surname></persName>
			<idno type="ORCID">0000-0002-6250-0639</idno>
		</author>
		<author>
			<persName><forename type="first">Yunqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
			<idno type="ORCID">0000-0001-9690-7026</idno>
		</author>
		<idno type="DOI">10.1109/tmm.2019.2922125</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<title level="j" type="abbrev">IEEE Trans. Multimedia</title>
		<idno type="ISSN">1520-9210</idno>
		<idno type="ISSNe">1941-0077</idno>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="96" to="107"/>
			<date type="published" when="2019">2019</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46454-1_29</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2016</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">9909. 2016</date>
			<biblScope unit="page" from="472" to="488"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning to Track at 100 FPS with Deep Regression Networks</title>
		<author>
			<persName><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46448-0_45</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2016</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="749" to="765"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fully-Convolutional Siamese Networks for Object Tracking</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-48881-3_56</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="850" to="865"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Siamese Instance Search for Tracking</title>
		<author>
			<persName><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.158</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">2016</date>
			<biblScope unit="page" from="1420" to="1429"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning Multi-domain Convolutional Neural Networks for Visual Tracking</title>
		<author>
			<persName><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.465</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">2016</date>
			<biblScope unit="page" from="4293" to="4302"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Hedged Deep Tracking</title>
		<author>
			<persName><forename type="first">Yuankai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.466</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">2016</date>
			<biblScope unit="page" from="4303" to="4311"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">STCT: Sequentially Training Convolutional Networks for Visual Tracking</title>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.153</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">2016</date>
			<biblScope unit="page" from="1373" to="1381"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Robust Visual Tracking with Deep Convolutional Neural Network Based Object Proposals on PETS</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvprw.2016.160</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">2016</date>
			<biblScope unit="page" from="1265" to="1272"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">DeepTrack: Learning Discriminative Feature Representations Online for Robust Visual Tracking</title>
		<author>
			<persName><forename type="first">Hanxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<idno type="DOI">10.1109/tip.2015.2510583</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<title level="j" type="abbrev">IEEE Trans. on Image Process.</title>
		<idno type="ISSN">1057-7149</idno>
		<idno type="ISSNe">1941-0042</idno>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1834" to="1848"/>
			<date type="published" when="2016-04">2016</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">DeepTrack: Learning Discriminative Feature Representations by Convolutional Neural Networks for Visual Tracking</title>
		<author>
			<persName><forename type="first">Hanxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<idno type="DOI">10.5244/c.28.56</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference 2014</title>
		<meeting>the British Machine Vision Conference 2014</meeting>
		<imprint>
			<publisher>British Machine Vision Association</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Robust Visual Tracking via Convolutional Networks without Training</title>
		<author>
			<persName><forename type="first">Kaihua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/tip.2016.2531283</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<title level="j" type="abbrev">IEEE Trans. on Image Process.</title>
		<idno type="ISSN">1057-7149</idno>
		<idno type="ISSNe">1941-0042</idno>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="1"/>
			<date type="published" when="2016">2016</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">When Correlation Filters Meet Convolutional Neural Networks for Visual Tracking</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/lsp.2016.2601691</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<title level="j" type="abbrev">IEEE Signal Process. Lett.</title>
		<idno type="ISSN">1070-9908</idno>
		<idno type="ISSNe">1558-2361</idno>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1454" to="1458"/>
			<date type="published" when="2016-10">2016</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning Multi-domain Convolutional Neural Networks for Visual Tracking</title>
		<author>
			<persName><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.465</idno>
		<ptr target="http://arxiv.org/abs/1608.07242"/>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Regional deep learning model for visual tracking</title>
		<author>
			<persName><forename type="first">Guoxing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangwei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunxia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayin</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2015.10.064</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<title level="j" type="abbrev">Neurocomputing</title>
		<idno type="ISSN">0925-2312</idno>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="issue">PartA</biblScope>
			<biblScope unit="page" from="310" to="323"/>
			<date type="published" when="2015">2015</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Parallel Tracking and Verifying: A Framework for Real-Time and High Accuracy Visual Tracking</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.585</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10">2017</date>
			<biblScope unit="page" from="5487" to="5495"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Parallel Tracking and Verifying</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Fan</surname></persName>
			<idno type="ORCID">0000-0002-3308-7873</idno>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
			<idno type="ORCID">0000-0003-4094-8413</idno>
		</author>
		<idno type="DOI">10.1109/tip.2019.2904789</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<title level="j" type="abbrev">IEEE Trans. on Image Process.</title>
		<idno type="ISSN">1057-7149</idno>
		<idno type="ISSNe">1941-0042</idno>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4130" to="4144"/>
			<date type="published" when="2019-08">2019</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">CREST: Convolutional Residual Learning for Visual Tracking</title>
		<author>
			<persName><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rynson</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.279</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10">2017</date>
			<biblScope unit="page" from="2574" to="2583"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">UCT: Learning Unified Convolutional Networks for Real-Time Visual Tracking</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccvw.2017.231</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1973" to="1982"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning Dynamic Siamese Network for Visual Object Tracking</title>
		<author>
			<persName><forename type="first">Qing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.196</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10">2017</date>
			<biblScope unit="page" from="1781" to="1789"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Robust Object Tracking Based on Temporal and Spatial Deep Networks</title>
		<author>
			<persName><forename type="first">Zhu</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congyan</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhe</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.130</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10">2017</date>
			<biblScope unit="page" from="1153" to="1162"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Correlation Filters with Weighted Convolution Responses</title>
		<author>
			<persName><forename type="first">Zhiqun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingruo</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfei</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongliang</forename><surname>Bai</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccvw.2017.233</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1992" to="2000"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Recurrent Filter Learning for Visual Tracking</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccvw.2017.235</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2010" to="2019"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Integrating Boundary and Center Correlation Filters for Visual Tracking with Aspect Ratio Variation</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingjie</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccvw.2017.234</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2001" to="2009"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Deep tracking with objectness</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingwen</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/icip.2017.8296363</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="660" to="664"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Siamese recurrent architecture for visual tracking</title>
		<author>
			<persName><forename type="first">Xiaqing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/icip.2017.8296462</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1152" to="1156"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Region-based fully convolutional siamese networks for robust real-time visual tracking</title>
		<author>
			<persName><forename type="first">Longchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/icip.2017.8296746</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-09">2017</date>
			<biblScope unit="page" from="2567" to="2571"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Gate connected convolutional neural network for object tracking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kokul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">A J</forename><surname>Pinidiyaarachchi</surname></persName>
		</author>
		<idno type="DOI">10.1109/icip.2017.8296753</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-09">2017</date>
			<biblScope unit="page" from="2602" to="2606"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Long-term object tracking based on siamese network</title>
		<author>
			<persName><forename type="first">Kaiheng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuehuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyun</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1109/icip.2017.8296961</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-09">2017</date>
			<biblScope unit="page" from="3640" to="3644"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Robust object tracking by interleaving variable rate color particle filtering and deep learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Akok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gurkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gunsel</surname></persName>
		</author>
		<idno type="DOI">10.1109/icip.2017.8296966</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-09">2017</date>
			<biblScope unit="page" from="3665" to="3669"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Deep convolutional particle filter for visual tracking</title>
		<author>
			<persName><forename type="first">Reza</forename><forename type="middle">Jalil</forename><surname>Mozhdehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Medeiros</surname></persName>
		</author>
		<idno type="DOI">10.1109/icip.2017.8296963</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-09">2017</date>
			<biblScope unit="page" from="3650" to="3654"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">End-to-End Representation Learning for Correlation Filter Based Tracking</title>
		<author>
			<persName><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.531</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">2017</date>
			<biblScope unit="page" from="5000" to="5008"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">ECO: Efficient Convolution Operators for Tracking</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.733</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">2017</date>
			<biblScope unit="page" from="6931" to="6939"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Discriminative Correlation Filter Tracker with Channel and Spatial Reliability</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Lukežič</surname></persName>
			<idno type="ORCID">0000-0002-6316-2707</idno>
		</author>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Vojíř</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luka</forename><surname>Čehovin zajc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiří</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-017-1061-3</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<title level="j" type="abbrev">Int J Comput Vis</title>
		<idno type="ISSN">0920-5691</idno>
		<idno type="ISSNe">1573-1405</idno>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="671" to="688"/>
			<date type="published" when="2018-01-08">2018</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Multi-task Correlation Particle Filter for Robust Object Tracking</title>
		<author>
			<persName><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.512</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">2017</date>
			<biblScope unit="page" from="4819" to="4827"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">BranchOut: Regularization for Online Ensemble Tracking with Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.63</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">2017</date>
			<biblScope unit="page" from="521" to="530"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Large Margin Object Tracking with Circulant Feature Maps</title>
		<author>
			<persName><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.510</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">2017</date>
			<biblScope unit="page" from="4800" to="4808"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Robust Visual Tracking Using Oblique Random Forests</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jagannadan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ponnuthurai</forename><forename type="middle">Nagaratnam</forename><surname>Suganthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Moulin</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.617</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">2017</date>
			<biblScope unit="page" from="5825" to="5834"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Attentional Correlation Filter Network for Adaptive Visual Tracking</title>
		<author>
			<persName><forename type="first">Jongwon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Jin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiannis</forename><surname>Demiris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.513</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">2017</date>
			<biblScope unit="page" from="4828" to="4837"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">SANet: Structure-Aware Network for Visual Tracking</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvprw.2017.275</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">2017</date>
			<biblScope unit="page" from="2217" to="2224"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">DCFNet: Discriminant correlation filters network for visual tracking</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1704.04057"/>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Deep Ensemble Tracking</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingfa</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1109/lsp.2017.2749458</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<title level="j" type="abbrev">IEEE Signal Process. Lett.</title>
		<idno type="ISSN">1070-9908</idno>
		<idno type="ISSNe">1558-2361</idno>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1562" to="1566"/>
			<date type="published" when="2017-10">2017</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Deep Relative Tracking</title>
		<author>
			<persName><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
			<idno type="ORCID">0000-0001-8343-9665</idno>
		</author>
		<idno type="DOI">10.1109/tip.2017.2656628</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<title level="j" type="abbrev">IEEE Trans. on Image Process.</title>
		<idno type="ISSN">1057-7149</idno>
		<idno type="ISSNe">1941-0042</idno>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1845" to="1858"/>
			<date type="published" when="2017-04">2017</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Dual Deep Network for Visual Tracking</title>
		<author>
			<persName><forename type="first">Zhizhen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Li</surname></persName>
			<idno type="ORCID">0000-0001-9110-5534</idno>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
			<idno type="ORCID">0000-0002-6668-9758</idno>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/tip.2017.2669880</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<title level="j" type="abbrev">IEEE Trans. on Image Process.</title>
		<idno type="ISSN">1057-7149</idno>
		<idno type="ISSNe">1941-0042</idno>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2005" to="2015"/>
			<date type="published" when="2017-04">2017</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Online object tracking based on CNN with spatial-temporal saliency guided sampling</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2016.10.073</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<title level="j" type="abbrev">Neurocomputing</title>
		<idno type="ISSN">0925-2312</idno>
		<imprint>
			<biblScope unit="volume">257</biblScope>
			<biblScope unit="page" from="115" to="127"/>
			<date type="published" when="2017-09">2017</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Triplet Loss in Siamese Network for Object Tracking</title>
		<author>
			<persName><forename type="first">Xingping</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01261-8_28</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11217</biblScope>
			<biblScope unit="page" from="472" to="488"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Deep Regression Tracking with Shrinkage Loss</title>
		<author>
			<persName><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01264-9_22</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="369" to="386"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Unveiling the Power of Deep Tracking</title>
		<author>
			<persName><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01216-8_30</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="493" to="509"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Real-Time ‘Actor-Critic’ Tracking</title>
		<author>
			<persName><forename type="first">Boyu</forename><surname>Chen</surname></persName>
			<idno type="ORCID">0000-0003-2397-7669</idno>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
			<idno type="ORCID">0000-0002-6976-4004</idno>
		</author>
		<author>
			<persName><forename type="first">Peixia</forename><surname>Li</surname></persName>
			<idno type="ORCID">0000-0001-6167-5309</idno>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Wang</surname></persName>
			<idno type="ORCID">0000-0002-6462-6040</idno>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
			<idno type="ORCID">0000-0002-6668-9758</idno>
		</author>
		<idno type="DOI">10.1007/978-3-030-01234-2_20</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="328" to="345"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Distractor-Aware Siamese Networks for Visual Object Tracking</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
			<idno type="ORCID">0000-0002-4435-1692</idno>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01240-3_7</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11213</biblScope>
			<biblScope unit="page" from="103" to="119"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Real-Time MDNet</title>
		<author>
			<persName><forename type="first">Ilchae</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeany</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mooyeol</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01225-0_6</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="89" to="104"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Structured Siamese Network for Real-Time Visual Tracking</title>
		<author>
			<persName><forename type="first">Yunhua</forename><surname>Zhang</surname></persName>
			<idno type="ORCID">0000-0003-3567-215X</idno>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Wang</surname></persName>
			<idno type="ORCID">0000-0003-2538-8358</idno>
		</author>
		<author>
			<persName><forename type="first">Jinqing</forename><surname>Qi</surname></persName>
			<idno type="ORCID">0000-0002-3777-2405</idno>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
			<idno type="ORCID">0000-0002-6976-4004</idno>
		</author>
		<author>
			<persName><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
			<idno type="ORCID">0000-0002-7112-4655</idno>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
			<idno type="ORCID">0000-0002-6668-9758</idno>
		</author>
		<idno type="DOI">10.1007/978-3-030-01240-3_22</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="355" to="370"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">A Memory Model Based on the Siamese Network for Long-Term Tracking</title>
		<author>
			<persName><forename type="first">Hankyeol</forename><surname>Lee</surname></persName>
			<idno type="ORCID">0000-0003-1559-5791</idno>
		</author>
		<author>
			<persName><forename type="first">Seokeon</forename><surname>Choi</surname></persName>
			<idno type="ORCID">0000-0002-1695-5894</idno>
		</author>
		<author>
			<persName><forename type="first">Changick</forename><surname>Kim</surname></persName>
			<idno type="ORCID">0000-0001-9323-8488</idno>
		</author>
		<idno type="DOI">10.1007/978-3-030-11009-3_5</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="100" to="115"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Channel Pruning for Visual Tracking</title>
		<author>
			<persName><forename type="first">Manqiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changzhen</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-11009-3_3</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="70" to="82"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Learning a Robust Society of Tracking Parts Using Co-occurrence Constraints</title>
		<author>
			<persName><forename type="first">Elena</forename><surname>Burceanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Leordeanu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-11009-3_9</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="162" to="178"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Multiple Context Features in Siamese Networks for Visual Object Tracking</title>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Morimitsu</surname></persName>
			<idno type="ORCID">0000-0001-9455-8571</idno>
		</author>
		<idno type="DOI">10.1007/978-3-030-11009-3_6</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="116" to="131"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Towards a Better Match in Siamese Network Based Visual Object Tracker</title>
		<author>
			<persName><forename type="first">Anfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-11009-3_7</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="132" to="147"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">WAEF: Weighted Aggregation with Enhancement Filter for Visual Object Tracking</title>
		<author>
			<persName><forename type="first">Litu</forename><surname>Rout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><forename type="middle">Krishna Sai Subrahmanyam</forename><surname>Gorthi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-11009-3_4</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="83" to="99"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Context-Aware Deep Feature Compression for High-Speed Visual Tracking</title>
		<author>
			<persName><forename type="first">Jongwon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Jin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyuewang</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyeoup</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiannis</forename><surname>Demiris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00057</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">2018</date>
			<biblScope unit="page" from="479" to="488"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">VITAL: VIsual Tracking via Adversarial Learning</title>
		<author>
			<persName><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rynson</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00937</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">2018</date>
			<biblScope unit="page" from="8990" to="8999"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00515</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">2018</date>
			<biblScope unit="page" from="4904" to="4913"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">High Performance Visual Tracking with Siamese Region Proposal Network</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00935</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">2018</date>
			<biblScope unit="page" from="8971" to="8980"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">A Twofold Siamese Network for Real-Time Object Tracking</title>
		<author>
			<persName><forename type="first">Anfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00508</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">2018</date>
			<biblScope unit="page" from="4834" to="4843"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">End-to-End Flow Correlation Tracking with Spatial-Temporal Attention</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00064</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">2018</date>
			<biblScope unit="page" from="548" to="557"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Correlation Tracking via Joint Discrimination and Reliability Learning</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00058</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">2018</date>
			<biblScope unit="page" from="489" to="497"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Learning Spatial-Aware Regressions for Visual Tracking</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00934</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">2018</date>
			<biblScope unit="page" from="8962" to="8970"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Learning Attentions: Residual Attentional Siamese Network for High Performance Online Visual Tracking</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhu</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Maybank</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00510</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">2018</date>
			<biblScope unit="page" from="4854" to="4863"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Multi-cue Correlation Filters for Robust Visual Tracking</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00509</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">2018</date>
			<biblScope unit="page" from="4844" to="4853"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Deep Convolutional Particle Filter with Adaptive Correlation Maps for Visual Tracking</title>
		<author>
			<persName><forename type="first">Reza</forename><forename type="middle">Jalil</forename><surname>Mozhdehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yevgeniy</forename><surname>Reznichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abubakar</forename><surname>Siddique</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Medeiros</surname></persName>
		</author>
		<idno type="DOI">10.1109/icip.2018.8451069</idno>
	</analytic>
	<monogr>
		<title level="m">2018 25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-10">2018</date>
			<biblScope unit="page" from="798" to="802"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Robust Visual Tracking in Low-Resolution Sequence</title>
		<author>
			<persName><forename type="first">Zhiguan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="DOI">10.1109/icip.2018.8451826</idno>
	</analytic>
	<monogr>
		<title level="m">2018 25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-10">2018</date>
			<biblScope unit="page" from="4103" to="4107"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Fully Convolutional Siamese Fusion Networks for Object Tracking</title>
		<author>
			<persName><forename type="first">Miaobin</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheolkon</forename><surname>Jung</surname></persName>
		</author>
		<idno type="DOI">10.1109/icip.2018.8451102</idno>
	</analytic>
	<monogr>
		<title level="m">2018 25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-10">2018</date>
			<biblScope unit="page" from="3718" to="3722"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Flow Guided Siamese Network for Visual Tracking</title>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/icip.2018.8451324</idno>
	</analytic>
	<monogr>
		<title level="m">2018 25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-10">2018</date>
			<biblScope unit="page" from="231" to="235"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Fusion of Template Matching and Foreground Detection for Robust Visual Tracking</title>
		<author>
			<persName><forename type="first">Kaiheng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuehuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Huo</surname></persName>
		</author>
		<idno type="DOI">10.1109/icip.2018.8451332</idno>
	</analytic>
	<monogr>
		<title level="m">2018 25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-10">2018</date>
			<biblScope unit="page" from="2720" to="2724"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Integrating Multi-Level Convolutional Features for Correlation Filter Tracking</title>
		<author>
			<persName><forename type="first">Guangen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guizhong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/icip.2018.8451425</idno>
	</analytic>
	<monogr>
		<title level="m">2018 25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-10">2018</date>
			<biblScope unit="page" from="3029" to="3033"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Generating Reliable Online Adaptive Templates for Visual Tracking</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingfa</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenwang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1109/icip.2018.8451440</idno>
	</analytic>
	<monogr>
		<title level="m">2018 25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-10">2018</date>
			<biblScope unit="page" from="226" to="230"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Learning Recurrent Memory Activation Networks for Visual Tracking</title>
		<author>
			<persName><forename type="first">Shi</forename><surname>Pu</surname></persName>
			<idno type="ORCID">0000-0002-8748-8971</idno>
		</author>
		<author>
			<persName><forename type="first">Yibing</forename><surname>Song</surname></persName>
			<idno type="ORCID">0000-0003-3667-531X</idno>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
			<idno type="ORCID">0000-0002-8459-2845</idno>
		</author>
		<author>
			<persName><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
			<idno type="ORCID">0000-0001-8287-6783</idno>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
			<idno type="ORCID">0000-0003-4848-2304</idno>
		</author>
		<idno type="DOI">10.1109/tip.2020.3038356</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<title level="j" type="abbrev">IEEE Trans. on Image Process.</title>
		<idno type="ISSN">1057-7149</idno>
		<idno type="ISSNe">1941-0042</idno>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="725" to="738"/>
			<date type="published" when="2018">2018</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Model-Free Tracking With Deep Appearance and Motion Features Integration</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiantong</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianbin</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1109/wacv.2019.00018</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">87</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">High Speed Recurrent Regression Network for Visual Tracking</title>
		<author>
			<persName><forename type="first">Ding</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangqian</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/icme.2019.00122</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">242</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Robust Visual Tracking via Hierarchical Convolutional Features</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
			<idno type="ORCID">0000-0002-0536-3658</idno>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
			<idno type="ORCID">0000-0003-4848-2304</idno>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2018.2865311</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<title level="j" type="abbrev">IEEE Trans. Pattern Anal. Mach. Intell.</title>
		<idno type="ISSN">0162-8828</idno>
		<idno type="ISSNe">1939-3539</idno>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2709" to="2723"/>
			<date type="published" when="2018">2018</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Adaptive Discriminative Deep Correlation Filter for Visual Object Tracking</title>
		<author>
			<persName><forename type="first">Zhenjun</forename><surname>Han</surname></persName>
			<idno type="ORCID">0000-0002-9970-5152</idno>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
			<idno type="ORCID">0000-0003-1215-6259</idno>
		</author>
		<idno type="DOI">10.1109/tcsvt.2018.2888492</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<title level="j" type="abbrev">IEEE Trans. Circuits Syst. Video Technol.</title>
		<idno type="ISSN">1051-8215</idno>
		<idno type="ISSNe">1558-2205</idno>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="155" to="166"/>
			<date type="published" when="2018">2018</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Once for All: A Two-Flow Convolutional Neural Network for Visual Tracking</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
			<idno type="ORCID">0000-0001-8436-6533</idno>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Tao</surname></persName>
			<idno type="ORCID">0000-0003-3284-864X</idno>
		</author>
		<idno type="DOI">10.1109/tcsvt.2017.2757061</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<title level="j" type="abbrev">IEEE Trans. Circuits Syst. Video Technol.</title>
		<idno type="ISSN">1051-8215</idno>
		<idno type="ISSNe">1558-2205</idno>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3377" to="3386"/>
			<date type="published" when="2018-12">2018</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Robust Visual Tracking via Hierarchical Particle Filter and Ensemble Deep Features</title>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Li</surname></persName>
			<idno type="ORCID">0000-0001-7250-9840</idno>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhao</surname></persName>
			<idno type="ORCID">0000-0002-5217-004X</idno>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Cheng</surname></persName>
			<idno type="ORCID">0000-0003-2160-2839</idno>
		</author>
		<author>
			<persName><forename type="first">Erhu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/tcsvt.2018.2889457</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<title level="j" type="abbrev">IEEE Trans. Circuits Syst. Video Technol.</title>
		<idno type="ISSN">1051-8215</idno>
		<idno type="ISSNe">1558-2205</idno>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="179" to="191"/>
			<date type="published" when="2018">2018</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Good Features to Correlate for Visual Tracking</title>
		<author>
			<persName><forename type="first">Erhan</forename><surname>Gundogdu</surname></persName>
			<idno type="ORCID">0000-0003-3416-5842</idno>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Aydin</forename><surname>Alatan</surname></persName>
		</author>
		<idno type="DOI">10.1109/tip.2018.2806280</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<title level="j" type="abbrev">IEEE Trans. on Image Process.</title>
		<idno type="ISSN">1057-7149</idno>
		<idno type="ISSNe">1941-0042</idno>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2526" to="2540"/>
			<date type="published" when="2018-05">2018</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Correlation Filter Selection for Visual Tracking Using Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Yanchun</forename><surname>Xie</surname></persName>
			<idno type="ORCID">0000-0002-1274-2103</idno>
		</author>
		<author>
			<persName><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
			<idno type="ORCID">0000-0002-9416-2486</idno>
		</author>
		<author>
			<persName><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
			<idno type="ORCID">0000-0002-3034-9639</idno>
		</author>
		<author>
			<persName><forename type="first">Jeyarajan</forename><surname>Thiyagalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
			<idno type="ORCID">0000-0002-8581-9554</idno>
		</author>
		<idno type="DOI">10.1109/tcsvt.2018.2889488</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<title level="j" type="abbrev">IEEE Trans. Circuits Syst. Video Technol.</title>
		<idno type="ISSN">1051-8215</idno>
		<idno type="ISSNe">1558-2205</idno>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="192" to="204"/>
			<date type="published" when="2018">2018</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">P2T: Part-to-Target Tracking via Deep Regression Learning</title>
		<author>
			<persName><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
			<idno type="ORCID">0000-0001-8343-9665</idno>
		</author>
		<idno type="DOI">10.1109/tip.2018.2813166</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<title level="j" type="abbrev">IEEE Trans. on Image Process.</title>
		<idno type="ISSN">1057-7149</idno>
		<idno type="ISSNe">1941-0042</idno>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3074" to="3086"/>
			<date type="published" when="2018-06">2018</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Densely Connected Discriminative Correlation Filters for Visual Tracking</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Peng</surname></persName>
			<idno type="ORCID">0000-0002-8378-6971</idno>
		</author>
		<author>
			<persName><forename type="first">Fanghui</forename><surname>Liu</surname></persName>
			<idno type="ORCID">0000-0003-4133-7921</idno>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
			<idno type="ORCID">0000-0003-4801-7162</idno>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Kasabov</surname></persName>
			<idno type="ORCID">0000-0003-4433-7521</idno>
		</author>
		<idno type="DOI">10.1109/lsp.2018.2836360</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<title level="j" type="abbrev">IEEE Signal Process. Lett.</title>
		<idno type="ISSN">1070-9908</idno>
		<idno type="ISSNe">1558-2361</idno>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1019" to="1023"/>
			<date type="published" when="2018-07">2018</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">End-to-End Feature Integration for Correlation Filter Tracking With Channel Attention</title>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Li</surname></persName>
			<idno type="ORCID">0000-0001-6004-0408</idno>
		</author>
		<author>
			<persName><forename type="first">Gongjian</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangliu</forename><surname>Kuai</surname></persName>
			<idno type="ORCID">0000-0001-9357-8482</idno>
		</author>
		<author>
			<persName><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<idno type="DOI">10.1109/lsp.2018.2877008</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<title level="j" type="abbrev">IEEE Signal Process. Lett.</title>
		<idno type="ISSN">1070-9908</idno>
		<idno type="ISSNe">1558-2361</idno>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1815" to="1819"/>
			<date type="published" when="2018-12">2018</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Adaptive Correlation Filters with Long-Term and Short-Term Memory for Object Tracking</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
			<idno type="ORCID">0000-0003-4848-2304</idno>
		</author>
		<idno type="DOI">10.1007/s11263-018-1076-4</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<title level="j" type="abbrev">Int J Comput Vis</title>
		<idno type="ISSN">0920-5691</idno>
		<idno type="ISSNe">1573-1405</idno>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="771" to="796"/>
			<date type="published" when="2018-03-16">2018</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal context via hierarchical features for visual tracking</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xue</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.image.2018.04.010</idno>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<title level="j" type="abbrev">Signal Processing: Image Communication</title>
		<idno type="ISSN">0923-5965</idno>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="50" to="65"/>
			<date type="published" when="2018-08">2018</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Spatial–temporal adaptive feature weighted correlation filter for visual tracking</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianglong</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.image.2018.05.013</idno>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<title level="j" type="abbrev">Signal Processing: Image Communication</title>
		<idno type="ISSN">0923-5965</idno>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="58" to="70"/>
			<date type="published" when="2018-09">2018</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">When correlation filters meet fully-convolutional Siamese networks for distractor-aware tracking</title>
		<author>
			<persName><forename type="first">Yangliu</forename><surname>Kuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gongjian</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.image.2018.03.002</idno>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<title level="j" type="abbrev">Signal Processing: Image Communication</title>
		<idno type="ISSN">0923-5965</idno>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="107" to="117"/>
			<date type="published" when="2018-05">2018</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Online object tracking via motion-guided convolutional neural network (MGNet)</title>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Gan</surname></persName>
			<idno type="ORCID">0000-0002-4076-6452</idno>
		</author>
		<author>
			<persName><forename type="first">Ming-Sui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Kuo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jvcir.2018.03.016</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<title level="j" type="abbrev">Journal of Visual Communication and Image Representation</title>
		<idno type="ISSN">1047-3203</idno>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="180" to="191"/>
			<date type="published" when="2018-05">2018</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Occlusion‐robust object tracking based on the confidence of online selected hierarchical features</title>
		<author>
			<persName><forename type="first">Mingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng‐bin</forename><surname>Jin</surname></persName>
			<idno type="ORCID">0000-0001-8486-5738</idno>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuenan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakil</forename><surname>Kim</surname></persName>
			<idno type="ORCID">0000-0003-4232-3804</idno>
		</author>
		<idno type="DOI">10.1049/iet-ipr.2018.5454</idno>
	</analytic>
	<monogr>
		<title level="j">IET Image Processing</title>
		<title level="j" type="abbrev">IET Image Processing</title>
		<idno type="ISSN">1751-9667</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2023" to="2029"/>
			<date type="published" when="2018-11">2018</date>
			<publisher>Institution of Engineering and Technology (IET)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Visual Tracking via Adaptive Spatially-Regularized Correlation Filters</title>
		<author>
			<persName><forename type="first">Kenan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00480</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06">2019</date>
			<biblScope unit="page" from="4670" to="4679"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">ATOM: Accurate Tracking by Overlap Maximization</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00479</idno>
		<ptr target="http://arxiv.org/abs/1811.07628"/>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Siamese Cascaded Region Proposal Networks for Real-Time Visual Tracking</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00814</idno>
		<ptr target="http://arxiv.org/abs/1812.06148"/>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Graph Convolutional Tracking</title>
		<author>
			<persName><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00478</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06">2019</date>
			<biblScope unit="page" from="4649" to="4659"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">ROI Pooled Correlation Filters for Visual Tracking</title>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">You</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00593</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06">2019</date>
			<biblScope unit="page" from="5783" to="5791"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">SPM-Tracker: Series-Parallel Matching for Real-Time Visual Object Tracking</title>
		<author>
			<persName><forename type="first">Guangting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00376</idno>
		<ptr target="http://arxiv.org/abs/1904.04452"/>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Deeper and Wider Siamese Networks for Real-Time Visual Tracking</title>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00472</idno>
		<ptr target="http://arxiv.org/abs/1901.01660"/>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Fast Online Object Tracking and Segmentation: A Unifying Approach</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00142</idno>
		<ptr target="http://arxiv.org/abs/1812.05050"/>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">SiamRPN++: Evolution of Siamese Visual Tracking With Very Deep Networks</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00441</idno>
		<ptr target="http://arxiv.org/abs/1812.11703"/>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Target-Aware Deep Tracking</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00146</idno>
		<ptr target="http://arxiv.org/abs/1904.01772"/>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Unsupervised Deep Tracking</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00140</idno>
		<ptr target="http://arxiv.org/abs/1904.01828"/>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Learning Discriminative Model Prediction for Tracking</title>
		<author>
			<persName><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2019.00628</idno>
		<ptr target="http://arxiv.org/abs/1904.07220"/>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Adversarial Deep Tracking</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Zhao</surname></persName>
			<idno type="ORCID">0000-0002-8198-3787</idno>
		</author>
		<author>
			<persName><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
			<idno type="ORCID">0000-0002-9118-2780</idno>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Tang</surname></persName>
			<idno type="ORCID">0000-0003-4976-3095</idno>
		</author>
		<idno type="DOI">10.1109/tcsvt.2018.2856540</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<title level="j" type="abbrev">IEEE Trans. Circuits Syst. Video Technol.</title>
		<idno type="ISSN">1051-8215</idno>
		<idno type="ISSNe">1558-2205</idno>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1998" to="2011"/>
			<date type="published" when="2019-07">2019</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Real-Time Deep Tracking via Corrective Domain Adaptation</title>
		<author>
			<persName><forename type="first">Hanxi</forename><surname>Li</surname></persName>
			<idno type="ORCID">0000-0002-9433-1833</idno>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
			<idno type="ORCID">0000-0001-9082-094X</idno>
		</author>
		<author>
			<persName><forename type="first">Fumin</forename><surname>Shen</surname></persName>
			<idno type="ORCID">0000-0001-7303-3231</idno>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
			<idno type="ORCID">0000-0002-1520-4466</idno>
		</author>
		<author>
			<persName><forename type="first">Mingwen</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/tcsvt.2019.2923639</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<title level="j" type="abbrev">IEEE Trans. Circuits Syst. Video Technol.</title>
		<idno type="ISSN">1051-8215</idno>
		<idno type="ISSNe">1558-2205</idno>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2600" to="2612"/>
			<date type="published" when="2019-09">2019</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Hierarchical Tracking by Reinforcement Learning-Based Searching and Coarse-to-Fine Verifying</title>
		<author>
			<persName><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
			<idno type="ORCID">0000-0003-3423-1539</idno>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Li</surname></persName>
			<idno type="ORCID">0000-0003-3716-671X</idno>
		</author>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
			<idno type="ORCID">0000-0002-2288-5079</idno>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<idno type="DOI">10.1109/tip.2018.2885238</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<title level="j" type="abbrev">IEEE Trans. on Image Process.</title>
		<idno type="ISSN">1057-7149</idno>
		<idno type="ISSNe">1941-0042</idno>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2331" to="2341"/>
			<date type="published" when="2019-05">2019</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">SMART: Joint Sampling and Regression for Visual Tracking</title>
		<author>
			<persName><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
			<idno type="ORCID">0000-0003-1856-9564</idno>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
			<idno type="ORCID">0000-0001-8343-9665</idno>
		</author>
		<idno type="DOI">10.1109/tip.2019.2904434</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<title level="j" type="abbrev">IEEE Trans. on Image Process.</title>
		<idno type="ISSN">1057-7149</idno>
		<idno type="ISSNe">1941-0042</idno>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3923" to="3935"/>
			<date type="published" when="2019-08">2019</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Robust Object Tracking Using Manifold Regularized Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
			<idno type="ORCID">0000-0003-2656-3082</idno>
		</author>
		<author>
			<persName><forename type="first">Hanqiu</forename><surname>Sun</surname></persName>
			<idno type="ORCID">0000-0002-3244-4111</idno>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
			<idno type="ORCID">0000-0002-8264-6117</idno>
		</author>
		<author>
			<persName><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
			<idno type="ORCID">0000-0002-1520-4466</idno>
		</author>
		<idno type="DOI">10.1109/tmm.2018.2859831</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<title level="j" type="abbrev">IEEE Trans. Multimedia</title>
		<idno type="ISSN">1520-9210</idno>
		<idno type="ISSNe">1941-0077</idno>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="510" to="521"/>
			<date type="published" when="2019-02">2019</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Memory Mechanisms for Discriminative Visual Tracking Algorithms With Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Lituan</forename><surname>Wang</surname></persName>
			<idno type="ORCID">0000-0002-5920-787X</idno>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
			<idno type="ORCID">0000-0002-5867-9322</idno>
		</author>
		<author>
			<persName><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Yi</surname></persName>
		</author>
		<idno type="DOI">10.1109/tcds.2019.2900506</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive and Developmental Systems</title>
		<title level="j" type="abbrev">IEEE Trans. Cogn. Dev. Syst.</title>
		<idno type="ISSN">2379-8920</idno>
		<idno type="ISSNe">2379-8939</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="108"/>
			<date type="published" when="2019">2019</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Multi-Task Hierarchical Feature Learning for Real-Time Visual Tracking</title>
		<author>
			<persName><forename type="first">Yangliu</forename><surname>Kuai</surname></persName>
			<idno type="ORCID">0000-0001-9357-8482</idno>
		</author>
		<author>
			<persName><forename type="first">Gongjian</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Li</surname></persName>
			<idno type="ORCID">0000-0001-6004-0408</idno>
		</author>
		<idno type="DOI">10.1109/jsen.2018.2883593</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors Journal</title>
		<title level="j" type="abbrev">IEEE Sensors J.</title>
		<idno type="ISSN">1530-437X</idno>
		<idno type="ISSNe">2379-9153</idno>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1961" to="1968"/>
			<date type="published" when="2019-03-01">2019</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Visual Tracking via Auto-Encoder Pair Correlation Filter</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Cheng</surname></persName>
			<idno type="ORCID">0000-0003-2355-9010</idno>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1109/tie.2019.2913815</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Electronics</title>
		<title level="j" type="abbrev">IEEE Trans. Ind. Electron.</title>
		<idno type="ISSN">0278-0046</idno>
		<idno type="ISSNe">1557-9948</idno>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3288" to="3297"/>
			<date type="published" when="2019">2019</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Deep feature tracking based on interactive multiple model</title>
		<author>
			<persName><forename type="first">Fuhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
			<idno type="ORCID">0000-0002-9543-6960</idno>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqiang</forename><surname>Hu</surname></persName>
			<idno type="ORCID">0000-0001-5710-9304</idno>
		</author>
		<author>
			<persName><forename type="first">Huanlong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2018.12.035</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<title level="j" type="abbrev">Neurocomputing</title>
		<idno type="ISSN">0925-2312</idno>
		<imprint>
			<biblScope unit="volume">333</biblScope>
			<biblScope unit="page" from="29" to="40"/>
			<date type="published" when="2019-03">2019</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Learning transform-aware attentive network for object tracking</title>
		<author>
			<persName><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
			<idno type="ORCID">0000-0002-9543-6960</idno>
		</author>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2019.02.021</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<title level="j" type="abbrev">Neurocomputing</title>
		<idno type="ISSN">0925-2312</idno>
		<imprint>
			<biblScope unit="volume">349</biblScope>
			<biblScope unit="page" from="133" to="144"/>
			<date type="published" when="2019-07">2019</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Learning target-aware correlation filters for visual tracking</title>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Li</surname></persName>
			<idno type="ORCID">0000-0001-6004-0408</idno>
		</author>
		<author>
			<persName><forename type="first">Gongjian</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangliu</forename><surname>Kuai</surname></persName>
			<idno type="ORCID">0000-0001-9357-8482</idno>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xiao</surname></persName>
			<idno type="ORCID">0000-0001-5020-638X</idno>
		</author>
		<author>
			<persName><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jvcir.2018.11.036</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<title level="j" type="abbrev">Journal of Visual Communication and Image Representation</title>
		<idno type="ISSN">1047-3203</idno>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="149" to="159"/>
			<date type="published" when="2019-01">2019</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Multi attention module for visual tracking</title>
		<author>
			<persName><forename type="first">Boyu</forename><surname>Chen</surname></persName>
			<idno type="ORCID">0000-0003-2397-7669</idno>
		</author>
		<author>
			<persName><forename type="first">Peixia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
			<idno type="ORCID">0000-0002-6668-9758</idno>
		</author>
		<idno type="DOI">10.1016/j.patcog.2018.10.005</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<title level="j" type="abbrev">Pattern Recognition</title>
		<idno type="ISSN">0031-3203</idno>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="80" to="93"/>
			<date type="published" when="2019-03">2019</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Action-Decision Networks for Visual Tracking with Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongwon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimin</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.148</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2" to="6"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Action-Driven Visual Object Tracking With Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
			<idno type="ORCID">0000-0002-0417-8450</idno>
		</author>
		<author>
			<persName><forename type="first">Jongwon</forename><surname>Choi</surname></persName>
			<idno type="ORCID">0000-0001-9753-8760</idno>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimin</forename><surname>Yun</surname></persName>
			<idno type="ORCID">0000-0002-4493-9437</idno>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
			<idno type="ORCID">0000-0003-3891-5815</idno>
		</author>
		<idno type="DOI">10.1109/tnnls.2018.2801826</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<title level="j" type="abbrev">IEEE Trans. Neural Netw. Learning Syst.</title>
		<idno type="ISSN">2162-237X</idno>
		<idno type="ISSNe">2162-2388</idno>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2239" to="2252"/>
			<date type="published" when="2018-06">2018</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Coarse-to-Fine UAV Target Tracking With Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
			<idno type="ORCID">0000-0001-5556-3896</idno>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Song</surname></persName>
			<idno type="ORCID">0000-0002-1949-9986</idno>
		</author>
		<author>
			<persName><forename type="first">Xuewen</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibin</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/tase.2018.2877499</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automation Science and Engineering</title>
		<title level="j" type="abbrev">IEEE Trans. Automat. Sci. Eng.</title>
		<idno type="ISSN">1545-5955</idno>
		<idno type="ISSNe">1558-3783</idno>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1522" to="1530"/>
			<date type="published" when="2018">2018</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Deep Reinforcement Learning with Iterative Shift for Visual Tracking</title>
		<author>
			<persName><forename type="first">Liangliang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01240-3_42</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="697" to="713"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning for visual object tracking in videos</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Maei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1701.08936"/>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Learning Policies for Adaptive Tracking with Deep Feature Cascades</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.21</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10">2017</date>
			<biblScope unit="page" from="105" to="114"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Hyperparameter Optimization for Tracking with Continuous Deep Q-Learning</title>
		<author>
			<persName><forename type="first">Xingping</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00061</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">2018</date>
			<biblScope unit="page" from="518" to="527"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Tracking as Online Decision-Making: Learning a Policy from Streaming Videos with Reinforcement Learning</title>
		<author>
			<persName><forename type="first">James</forename><surname>Supancic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.43</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10">2017</date>
			<biblScope unit="page" from="322" to="331"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Real-time visual tracking by deep reinforced decision making</title>
		<author>
			<persName><forename type="first">Janghoon</forename><surname>Choi</surname></persName>
			<idno type="ORCID">0000-0002-0932-6213</idno>
		</author>
		<author>
			<persName><forename type="first">Junseok</forename><surname>Kwon</surname></persName>
			<idno type="ORCID">0000-0001-9526-7549</idno>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
			<idno type="ORCID">0000-0001-7210-1036</idno>
		</author>
		<idno type="DOI">10.1016/j.cviu.2018.05.009</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<title level="j" type="abbrev">Computer Vision and Image Understanding</title>
		<idno type="ISSN">1077-3142</idno>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page" from="10" to="19"/>
			<date type="published" when="2018-06">2018</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">SINT++: Robust Visual Tracking via Adversarial Positive Instance Generation</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenglong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00511</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">2018</date>
			<biblScope unit="page" from="4864" to="4873"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Meta-tracker: Fast and Robust Online Adaptation for Visual Object Trackers</title>
		<author>
			<persName><forename type="first">Eunbyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01219-9_35</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="587" to="604"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Visual Tracking With Convolutional Random Vector Functional Link Network</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ponnuthurai</forename><forename type="middle">Nagaratnam</forename><surname>Suganthan</surname></persName>
			<idno type="ORCID">0000-0003-0901-5105</idno>
		</author>
		<idno type="DOI">10.1109/tcyb.2016.2588526</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<title level="j" type="abbrev">IEEE Trans. Cybern.</title>
		<idno type="ISSN">2168-2267</idno>
		<idno type="ISSNe">2168-2275</idno>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3243" to="3253"/>
			<date type="published" when="2017-10">2017</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Visual Tracking with Convolutional Neural Network</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ponnuthurai</forename><forename type="middle">Nagaratnam</forename><surname>Suganthan</surname></persName>
		</author>
		<idno type="DOI">10.1109/smc.2015.362</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Systems, Man, and Cybernetics</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-10">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Bridging the Gap Between Detection and Tracking: A Unified Approach</title>
		<author>
			<persName><forename type="first">Lianghua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2019.00410</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Joint Group Feature Selection and Discriminative Filter Learning for Robust Visual Object Tracking</title>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen-Hua</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2019.00804</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">GradNet: Gradient-Guided Network for Visual Object Tracking</title>
		<author>
			<persName><forename type="first">Peixia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2019.00626</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Deep Meta Learning for Real-Time Target-Aware Visual Tracking</title>
		<author>
			<persName><forename type="first">Janghoon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junseok</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2019.00100</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Learning the Model Update for Siamese Trackers</title>
		<author>
			<persName><forename type="first">Lichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abel</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joost</forename><forename type="middle">Van De</forename><surname>Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2019.00411</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Correlation-Guided Attention for Corner Detection Based Visual Tracking</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianglong</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00687</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Cooling-Shrinking Attack: Blinding the Tracker With Imperceptible Noises</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00107</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">D3S – A Discriminative Single Shot Segmentation Tracker</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00716</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">One-Shot Adversarial Attacks on Visual Tracking With Dual Attention</title>
		<author>
			<persName><forename type="first">Xuesong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.01019</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Probabilistic Regression for Visual Tracking</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00721</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Recursive Least-Squares Estimator-Aided Online Learning for Visual Tracking</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00741</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">ROAM: Recurrently Optimizing Tracking Model</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runbo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00675</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Deformable Siamese Attention Networks for Visual Object Tracking</title>
		<author>
			<persName><forename type="first">Yuechen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00676</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Siamese Box Adaptive Network for Visual Tracking</title>
		<author>
			<persName><forename type="first">Zedu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guorong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00670</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">SiamCAR: Siamese Fully Convolutional Classification and Regression for Visual Tracking</title>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00630</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Siam R-CNN: Visual Tracking by Re-Detection</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00661</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Tracking by Instance Detection: A Meta-Learning Approach</title>
		<author>
			<persName><forename type="first">Guangting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00632</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Fine-Grained Motion Representation For Template-Free Visual Tracking</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1109/wacv45572.2020.9093517</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-03">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Adaptive Aggregation of Arbitrary Online Trackers with a Regret Bound</title>
		<author>
			<persName><forename type="first">Heon</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daiki</forename><surname>Suehiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seiichi</forename><surname>Uchida</surname></persName>
		</author>
		<idno type="DOI">10.1109/wacv45572.2020.9093613</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-03">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Efficient Multi-level Correlating for Visual Tracking</title>
		<author>
			<persName><forename type="first">Yipeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-20873-8_29</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ACCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="452" to="465"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Dsnet: Deep and shallow feature learning for efficient visual tracking</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wuyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liangyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Boundary Effect-Aware Visual Tracking for UAV with Online Enhanced Background Learning and Multi-Frame Consensus Verification</title>
		<author>
			<persName><forename type="first">Changhong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/iros40897.2019.8967674</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Cross-View Contextual Relation Transferred Network for Unsupervised Vehicle Tracking in Drone Videos</title>
		<author>
			<persName><forename type="first">Wenfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aimin</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinping</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Qin</surname></persName>
		</author>
		<idno type="DOI">10.1109/wacv45572.2020.9093382</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-03">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Keyfilter-Aware Real-Time UAV Object Tracking</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1109/icra40945.2020.9196943</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-05">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Intermittent Contextual Learning for Keyfilter-Aware UAV Object Tracking Using Deep Convolutional Feature</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Li</surname></persName>
			<idno type="ORCID">0000-0002-0157-6218</idno>
		</author>
		<author>
			<persName><forename type="first">Changhong</forename><surname>Fu</surname></persName>
			<idno type="ORCID">0000-0002-9897-6022</idno>
		</author>
		<author>
			<persName><forename type="first">Ziyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinqiang</forename><surname>Zhang</surname></persName>
			<idno type="ORCID">0000-0002-5423-1774</idno>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1109/tmm.2020.2990064</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<title level="j" type="abbrev">IEEE Trans. Multimedia</title>
		<idno type="ISSN">1520-9210</idno>
		<idno type="ISSNe">1941-0077</idno>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="810" to="822"/>
			<date type="published" when="2020">2020</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Robust multi-kernelized correlators for UAV tracking with adaptive context analysis and dynamic weighted filters</title>
		<author>
			<persName><forename type="first">Changhong</forename><surname>Fu</surname></persName>
			<idno type="ORCID">0000-0002-9897-6022</idno>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuling</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijiang</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-020-04716-x</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<title level="j" type="abbrev">Neural Comput &amp; Applic</title>
		<idno type="ISSN">0941-0643</idno>
		<idno type="ISSNe">1433-3058</idno>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="12591" to="12607"/>
			<date type="published" when="2020-01-31">2020</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Surrounding-aware correlation filter for UAV tracking with selective spatial regularization</title>
		<author>
			<persName><forename type="first">Changhong</forename><surname>Fu</surname></persName>
			<idno type="ORCID">0000-0002-9897-6022</idno>
		</author>
		<author>
			<persName><forename type="first">Weijiang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuling</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Yue</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.sigpro.2019.107324</idno>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<title level="j" type="abbrev">Signal Processing</title>
		<idno type="ISSN">0165-1684</idno>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="page">107324</biblScope>
			<date type="published" when="2020-02">2020</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">COMET: Context-Aware IoU-Guided Network for Small Object Tracking</title>
		<author>
			<persName><forename type="first">Seyed</forename><forename type="middle">Mojtaba</forename><surname>Marvasti-Zadeh</surname></persName>
			<idno type="ORCID">0000-0003-0536-0796</idno>
		</author>
		<author>
			<persName><forename type="first">Javad</forename><surname>Khaghani</surname></persName>
			<idno type="ORCID">0000-0002-0753-1950</idno>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Ghanei-Yakhdan</surname></persName>
			<idno type="ORCID">0000-0003-4575-1062</idno>
		</author>
		<author>
			<persName><forename type="first">Shohreh</forename><surname>Kasaei</surname></persName>
			<idno type="ORCID">0000-0002-3831-0878</idno>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Cheng</surname></persName>
			<idno type="ORCID">0000-0003-3261-3533</idno>
		</author>
		<idno type="DOI">10.1007/978-3-030-69532-3_36</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ACCV 2020</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="594" to="611"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Flow Guided Short-Term Trackers with Cascade Detection for Long-Term Tracking</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guizhong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccvw.2019.00026</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">GlobalTrack: A Simple and Strong Baseline for Long-Term Tracking</title>
		<author>
			<persName><forename type="first">Lianghua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i07.6758</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<title level="j" type="abbrev">AAAI</title>
		<idno type="ISSN">2159-5399</idno>
		<idno type="ISSNe">2374-3468</idno>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">07</biblScope>
			<biblScope unit="page" from="11037" to="11044"/>
			<date type="published" when="2020-04-03">2020</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">i-siam: Improving siamese tracker with distractors suppression and long-term strategies</title>
		<author>
			<persName><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE. ICCVW</title>
		<meeting>IEEE. ICCVW</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">Learning Regression and Verification Networks for Robust Long-term Tracking</title>
		<author>
			<persName><forename type="first">Yunhua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinqing</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
			<idno type="ORCID">0000-0002-6668-9758</idno>
		</author>
		<idno type="DOI">10.1007/s11263-021-01487-3</idno>
		<ptr target="http://arxiv.org/abs/1809.04320"/>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<title level="j" type="abbrev">Int J Comput Vis</title>
		<idno type="ISSN">0920-5691</idno>
		<idno type="ISSNe">1573-1405</idno>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2536" to="2547"/>
			<date type="published" when="2018">2018</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">High-Performance Long-Term Tracking With Meta-Updater</title>
		<author>
			<persName><forename type="first">Kenan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00633</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">‘Skimming-Perusal’ Tracking: A Framework for Real-Time and Robust Long-Term Tracking</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haojie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2019.00247</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Online Object Tracking: A Benchmark</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2013.312</idno>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-06">2013</date>
			<biblScope unit="page" from="2411" to="2418"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Object Tracking Benchmark</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wu</surname></persName>
			<idno type="ORCID">0000-0003-4848-2304</idno>
		</author>
		<author>
			<persName><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2014.2388226</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<title level="j" type="abbrev">IEEE Trans. Pattern Anal. Mach. Intell.</title>
		<idno type="ISSN">0162-8828</idno>
		<idno type="ISSNe">2160-9292</idno>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1834" to="1848"/>
			<date type="published" when="2015-09-01">2015</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">LaSOT: A High-Quality Benchmark for Large-Scale Single Object Tracking</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liting</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hexin</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00552</idno>
		<ptr target="http://arxiv.org/abs/1809.07845"/>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">A Benchmark and Simulator for UAV Tracking</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46448-0_27</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2016</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="445" to="461"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">The Unmanned Aerial Vehicle Benchmark: Object Detection and Tracking</title>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Du</surname></persName>
			<idno type="ORCID">0000-0001-9404-524X</idno>
		</author>
		<author>
			<persName><forename type="first">Yuankai</forename><surname>Qi</surname></persName>
			<idno type="ORCID">0000-0003-4312-5682</idno>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Yu</surname></persName>
			<idno type="ORCID">0000-0003-0036-531X</idno>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Yang</surname></persName>
			<idno type="ORCID">0000-0003-1455-2001</idno>
		</author>
		<author>
			<persName><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
			<idno type="ORCID">0000-0002-8663-7429</idno>
		</author>
		<author>
			<persName><forename type="first">Guorong</forename><surname>Li</surname></persName>
			<idno type="ORCID">0000-0003-3954-2387</idno>
		</author>
		<author>
			<persName><forename type="first">Weigang</forename><surname>Zhang</surname></persName>
			<idno type="ORCID">0000-0003-0042-7074</idno>
		</author>
		<author>
			<persName><forename type="first">Qingming</forename><surname>Huang</surname></persName>
			<idno type="ORCID">0000-0001-7542-296X</idno>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
			<idno type="ORCID">0000-0002-7252-5047</idno>
		</author>
		<idno type="DOI">10.1007/978-3-030-01249-6_23</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="375" to="391"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">VisDrone-SOT2019: The Vision Meets Drone Single Object Tracking Challenge Results</title>
		<author>
			<persName><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCVW</title>
		<meeting>ICCVW</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">Encoding Color Information for Visual Tracking: Algorithms and Benchmark</title>
		<author>
			<persName><forename type="first">Pengpeng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Blasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<idno type="DOI">10.1109/tip.2015.2482905</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<title level="j" type="abbrev">IEEE Trans. on Image Process.</title>
		<idno type="ISSN">1057-7149</idno>
		<idno type="ISSNe">1941-0042</idno>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5630" to="5644"/>
			<date type="published" when="2015-12">2015</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">NUS-PRO: A New Visual Tracking Challenge</title>
		<author>
			<persName><forename type="first">Annan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2015.2417577</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<title level="j" type="abbrev">IEEE Trans. Pattern Anal. Mach. Intell.</title>
		<idno type="ISSN">0162-8828</idno>
		<idno type="ISSNe">2160-9292</idno>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="335" to="349"/>
			<date type="published" when="2016-02-01">2016</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">Need for Speed: A Benchmark for Higher Frame Rate Object Tracking</title>
		<author>
			<persName><forename type="first">Hamed</forename><forename type="middle">Kiani</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashton</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.128</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10">2017</date>
			<biblScope unit="page" from="1134" to="1143"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">Visual Object Tracking for Unmanned Aerial Vehicles: A Benchmark and New Motion Models</title>
		<author>
			<persName><forename type="first">Siyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v31i1.11205</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<title level="j" type="abbrev">AAAI</title>
		<idno type="ISSN">2159-5399</idno>
		<idno type="ISSNe">2374-3468</idno>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4140" to="4146"/>
			<date type="published" when="2017-02-12">2017</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">TrackingNet: A Large-Scale Dataset and Benchmark for Object Tracking in the Wild</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adel</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Alsubaihi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01246-5_19</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="310" to="327"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">Long-Term Tracking in the Wild: A Benchmark</title>
		<author>
			<persName><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01219-9_41</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11207</biblScope>
			<biblScope unit="page" from="692" to="707"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">BUAA-PRO: A tracking dataset with pixel-level annotation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://bmvc2018.org/contents/papers/0851.pdf"/>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">GOT-10k: A Large High-Diversity Benchmark for Generic Object Tracking in the Wild</title>
		<author>
			<persName><forename type="first">Lianghua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2019.2957464</idno>
		<ptr target="http://arxiv.org/abs/1810.11981"/>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<title level="j" type="abbrev">IEEE Trans. Pattern Anal. Mach. Intell.</title>
		<idno type="ISSN">0162-8828</idno>
		<idno type="ISSNe">1939-3539</idno>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1562" to="1577"/>
			<date type="published" when="2018">2018</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">Long-Term Visual Object Tracking Benchmark</title>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Moudgil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Gandhi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-20890-5_40</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ACCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="629" to="645"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main">TracKlinic: Diagnosis of Challenge Factors in Visual Tracking</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuewei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<idno type="DOI">10.1109/wacv48630.2021.00101</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-01">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">Performance Evaluation Methodology for Long-Term Single-Object Tracking</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Lukezic</surname></persName>
			<idno type="ORCID">0000-0002-6316-2707</idno>
		</author>
		<author>
			<persName><forename type="first">Luka</forename><forename type="middle">Cehovin</forename><surname>Zajc</surname></persName>
			<idno type="ORCID">0000-0003-2823-272X</idno>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiri</forename><surname>Matas</surname></persName>
			<idno type="ORCID">0000-0003-0863-4844</idno>
		</author>
		<author>
			<persName><forename type="first">Matej</forename><surname>Kristan</surname></persName>
			<idno type="ORCID">0000-0002-4252-4342</idno>
		</author>
		<idno type="DOI">10.1109/tcyb.2020.2980618</idno>
		<ptr target="http://arxiv.org/abs/1804.07056"/>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<title level="j" type="abbrev">IEEE Trans. Cybern.</title>
		<idno type="ISSN">2168-2267</idno>
		<idno type="ISSNe">2168-2275</idno>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6305" to="6318"/>
			<date type="published" when="2018">2018</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main">VisDrone-VDT2018: The Vision Meets Drone Video Detection and Tracking Challenge Results</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinqin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenfeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenya</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arne</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Luna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanouil</forename><surname>Michail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Bochinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filiz</forename><surname>Bunyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gege</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guna</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guorong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Kompatsiaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><forename type="middle">M</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">C</forename><surname>San Miguel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kannappan</forename><surname>Palaniappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Avgerinakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengkun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noor</forename><forename type="middle">M</forename><surname>Al-Shakarji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Acatay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panagiotis</forename><surname>Giannakeris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Vrochidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Sikora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Senst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yidong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoliang</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiming</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-11021-5_29</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="496" to="518"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">Aggregation Signature for Small Object Tracking</title>
		<author>
			<persName><forename type="first">Chunlei</forename><surname>Liu</surname></persName>
			<idno type="ORCID">0000-0002-1138-7488</idno>
		</author>
		<author>
			<persName><forename type="first">Wenrui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
			<idno type="ORCID">0000-0002-8645-2328</idno>
		</author>
		<author>
			<persName><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
			<idno type="ORCID">0000-0001-7396-6218</idno>
		</author>
		<author>
			<persName><forename type="first">Jungong</forename><surname>Han</surname></persName>
			<idno type="ORCID">0000-0003-4361-956X</idno>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1109/tip.2019.2940477</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<title level="j" type="abbrev">IEEE Trans. on Image Process.</title>
		<idno type="ISSN">1057-7149</idno>
		<idno type="ISSNe">1941-0042</idno>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1738" to="1747"/>
			<date type="published" when="2020">2020</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">SSD: Single Shot MultiBox Detector</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46448-0_2</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2016</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="37"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML Deep Learning Workshop</title>
		<meeting>ICML Deep Learning Workshop</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main">RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation</title>
		<author>
			<persName><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.549</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">2017</date>
			<biblScope unit="page" from="5168" to="5177"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.106</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">2017</date>
			<biblScope unit="page" from="936" to="944"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main">Deep motion features for visual tracking</title>
		<author>
			<persName><forename type="first">Susanna</forename><surname>Gladh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/icpr.2016.7899807</idno>
	</analytic>
	<monogr>
		<title level="m">2016 23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-12">2016</date>
			<biblScope unit="page" from="1243" to="1248"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<analytic>
		<title level="a" type="main">YouTube-VOS: Sequence-to-Sequence Video Object Segmentation</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01228-1_36</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="603" to="619"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<title level="a" type="main">YouTube-BoundingBoxes: A Large High-Precision Human-Annotated Data Set for Object Detection in Video</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Mazzocchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.789</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">2017</date>
			<biblScope unit="page" from="7464" to="7473"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The KITTI dataset</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="DOI">10.1177/0278364913491297</idno>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<title level="j" type="abbrev">The International Journal of Robotics Research</title>
		<idno type="ISSN">0278-3649</idno>
		<idno type="ISSNe">1741-3176</idno>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237"/>
			<date type="published" when="2013-08-23">2013</date>
			<publisher>SAGE Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main">Meta-Learning in Neural Networks: A Survey</title>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2021.3079209</idno>
		<ptr target="http://arxiv.org/abs/2004.05439"/>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<title level="j" type="abbrev">IEEE Trans. Pattern Anal. Mach. Intell.</title>
		<idno type="ISSN">0162-8828</idno>
		<idno type="ISSNe">1939-3539</idno>
		<imprint>
			<biblScope unit="page" from="1" to="1"/>
			<date type="published" when="2020">2020</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b246">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main">JOTS: Joint Online Tracking and Segmentation</title>
		<author>
			<persName><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><surname>Dawei Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298835</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-06">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b248">
	<analytic>
		<title level="a" type="main">Tracking-by-Segmentation with Online Gradient Boosting Decision Tree</title>
		<author>
			<persName><forename type="first">Jeany</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilchae</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kayoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2015.350</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-12">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b249">
	<analytic>
		<title level="a" type="main">Superpixel-Based Tracking-by-Segmentation Using Markov Chains</title>
		<author>
			<persName><forename type="first">Donghun</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeany</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joon</forename><forename type="middle">Hee</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.62</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b250">
	<analytic>
		<title level="a" type="main">PReMVOS: Proposal-Generation, Refinement and Merging for Video Object Segmentation</title>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-20870-7_35</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ACCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="565" to="580"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b251">
	<analytic>
		<title level="a" type="main">Box2Seg: Attention Weighted Loss and Discriminative Feature Learning for Weakly Supervised Segmentation</title>
		<author>
			<persName><forename type="first">Viveka</forename><surname>Kulharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58583-9_18</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2020</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="290" to="308"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b252">
	<analytic>
		<title level="a" type="main">TraX: The visual Tracking eXchange protocol and library</title>
		<author>
			<persName><forename type="first">Luka</forename><surname>Čehovin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2017.02.036</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<title level="j" type="abbrev">Neurocomputing</title>
		<idno type="ISSN">0925-2312</idno>
		<imprint>
			<biblScope unit="volume">260</biblScope>
			<biblScope unit="page" from="5" to="8"/>
			<date type="published" when="2017-10">2017</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b253">
	<analytic>
		<title level="a" type="main">MatConvNet</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<idno type="DOI">10.1145/2733373.2807412</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015-10-13">2015</date>
			<biblScope unit="page" from="689" to="692"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b254">
	<analytic>
		<title level="a" type="main">Acquisition of Localization Confidence for Accurate Object Detection</title>
		<author>
			<persName><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruixuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01264-9_48</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="816" to="832"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b255">
	<analytic>
		<title level="a" type="main">Continual lifelong learning in neural systems</title>
		<author>
			<persName><forename type="first">Owen</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.33612/diss.625549871</idno>
	</analytic>
	<monogr>
		<title level="m">ser. Proc. ICML</title>
		<imprint>
			<publisher>University of Groningen Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4548" to="4557"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b256">
	<analytic>
		<title level="a" type="main">A baseline regularization scheme for transfer learning with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Xuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Davoine</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2019.107049</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<title level="j" type="abbrev">Pattern Recognition</title>
		<idno type="ISSN">0031-3203</idno>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">107049</biblScope>
			<date type="published" when="2018">2018</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b257">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting meets negative transfer: Batch spectral shrinkage for safe transfer learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1908" to="1918"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main">Lifelong learning with dynamically expandable networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b259">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Mojtaba</surname></persName>
			<affiliation>
				<orgName type="collaboration">Student Member</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Marvasti-Zadeh</forename></persName>
			<affiliation>
				<orgName type="collaboration">Student Member</orgName>
			</affiliation>
		</author>
		<imprint/>
		<respStmt>
			<orgName>Electrical Engineering, Yazd University (YU), Iran ; University of Alberta, Canada</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">is currently a Ph.D. student in</note>
	<note>researcher student of the Department of Electrical Engineering, YU, in 2015 and 2017. He is also a member of Vision and Learning Lab. , where he was a visiting researcher from Dec. 2019 to Sep. 2020. His research interest includes computer vision and machine learning</note>
</biblStruct>

<biblStruct xml:id="b260">
	<monogr>
		<title level="m" type="main">degree in computer science from the University of Alberta</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Cheng</surname></persName>
			<affiliation>
				<orgName type="collaboration">Senior Member</orgName>
			</affiliation>
		</author>
		<imprint>
			<publisher>Singapore, TTIChicago, USA, and NICTA, Australia</publisher>
			<pubPlace>Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>professor with the Department of Electrical and Computer Engineering, University of Alberta, Canada</orgName>
		</respStmt>
	</monogr>
	<note>received the Ph.D.. He is also the Director of the Vision and Learning Lab. Prior to coming back to University of Alberta, he has worked at A*STAR. His research expertise is mainly on computer vision and machine learning</note>
</biblStruct>

<biblStruct xml:id="b261">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Ghanei-Yakhdan Received The</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989. 2009</date>
			<pubPlace>Since</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Electrical Engineering from Isfahan University of Technology, Iran ; Toosi University of Technology, Iran, ; Ferdowsi University of Mashhad, Iran ; Professor with the Department of Electrical Engineering, Yazd University, Iran</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Sc. degree</note>
	<note>the M.Sc. degree in 1993 from K. N.. and the Ph.D. degree in 2009 from. he has been an Assistant. His research interests are in digital video and image processing, error concealment and error-resilient video coding, and object tracking</note>
</biblStruct>

<biblStruct xml:id="b262">
	<monogr>
		<title level="m" type="main">received the Ph.D. degree from the Signal Processing Research Center</title>
		<author>
			<persName><forename type="first">Shohreh</forename><surname>Kasaei</surname></persName>
			<affiliation>
				<orgName type="collaboration">Senior Member</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>School of Electrical Engineering and Computer Science, Queensland University of Technology, Australia</orgName>
		</respStmt>
	</monogr>
	<note>She was awarded as a distinguished researcher of Sharif University of Technology (SUT), in 2002 and 2010, where she is currently a Full Professor. Since 1999, she has been the Director of the Image Processing Laboratory (IPL). Her research interests include machine learning, computer vision, and image/video processing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>